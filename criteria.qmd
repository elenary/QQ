---
format:
  html:
    theme: cosmo
    self-contained: true
---

# Statistical criteria

```{r, eval=TRUE, echo = FALSE, message = FALSE}
library(tidyverse)
library(kableExtra)
studens_mat <- read_csv("student-mat.csv") %>% 
  rename_with(., ~ paste0(., "_mat"), .cols = c(absences, paid, G1, G2, G3)) -> studens_mat 
studens_por <- read_csv("student-por.csv") %>% 
  rename_with(., ~ paste0(., "_por"), .cols = c(absences, paid, G1, G2, G3)) -> studens_por
studens_mat %>% 
  full_join(studens_por, by = c("school","sex","age","address","famsize","Pstatus","Medu","Fedu",
                             "Mjob","Fjob","reason", "guardian", "traveltime","studytime", "failures", "schoolsup", "famsup",
                             "activities", "nursery", "higher", "internet", "romantic", "famrel", "freetime", "goout", 
                             "Dalc", "Walc", "health")) -> students 

students %>% 
  mutate("student" = paste0("id", row_number()), .before = "school")  %>% 
  drop_na() %>% 
  mutate(G_mat = rowMeans(dplyr::select(., c(G1_mat, G2_mat, G3_mat))),
         G_por = rowMeans(dplyr::select(., c(G1_por, G2_por, G3_por)))) %>% 
  mutate(absences_mat_groups = ifelse(absences_mat <=5, "less", ifelse(absences_mat <=15, "middle", "more"))) %>% 
  mutate(absences_por_groups = ifelse(absences_por <=5, "less", ifelse(absences_por <=15, "middle", "more"))) -> students
```

A statistical criterion is a rule by which we will try to reject the null hypothesis. Different criteria are appropriate for different hypotheses and data.

The statistical criterion includes:

-   **Theoretical distribution** , on the probability density graph of which we will place the compared averages: the type of curve and the mathematical formula of this probability law (T-distribution, F-distribution, \(\chi^2\) -distribution and others)

-   **The formula** by which we will calculate the desired **value of the criterion** (as we already calculated $Z_{Mrural}$)) based on our data on the sample for the Z-score)

-   The correspondence of each value calculated using this formula (for example, a Z-value or a T-value) to the percentage of data located behind this value on the probability density graph of this distribution ( *we will not use such correspondence tables, soulless machines do everything for us* )

-   **Formula for calculating the number of degrees of freedom** for this criterion

## Degrees of Freedom

The number of degrees of freedom (df) is the number of directions for changing a feature. The formula for calculating the degrees of freedom is specific to each statistical criterion (we do not need to calculate it ourselves) and depends, in fact, on the type of statistical criterion, the design of our study (how many comparisons we conduct) and the number of observations in the sample.

For the t-criterion, the number of degrees of freedom is calculated very simply: $df = n_1 -1 + n_2 - 1 = n_1 + n_2 - 2$

## Key distributions and criteria statistics (Z, T, F)

**Z-distribution**

Last time we looked at an example where we compared the average time parents spend with their children in large and small cities, and compared these values ​​using the \[Z-distribution\] {stat_test_example\] example: we got the Z-statistics and found the percentage of data for the point found that corresponded to the Z-score distribution.

```{r eval=TRUE, echo = FALSE, message = FALSE, fig.align = 'center', out.width="100%"}
knitr::include_graphics("images/z-dist_2.png")
```

$Z = \frac{M - \mu}{\frac{\sigma}{\sqrt{n}}} = \frac{M - \mu}{\frac{sd}{\sqrt{n}}}$


This is the simplest version of the criterion for testing statistical hypotheses – but not the only one. In fact, there may be others as a distribution on which we place the points (averages) and compare them with each other, depending on the nature of our data. Let's consider the main ones.

<https://gallery.shinyapps.io/dist_calc/>

**Student's t-distribution**

```{r eval=TRUE, echo = FALSE, message = FALSE, fig.align = 'center', out.width="70%"}
knitr::include_graphics("images/t-dist.png")
```

As we can see, it is very similar to the Z-distribution, but it has slightly higher tails.

The formula that defines the probability law of Student's T-distribution:

```{r eval=TRUE, echo = FALSE, message = FALSE, fig.align = 'center', out.width="50%"}
knitr::include_graphics("images/t-dist-pdf.png")
```

It is tacitly assumed that with a sample size of n=30, the t-distribution is considered close to the Z-distribution, which is normal, and we can use it. But in reality, this is not quite true.

The formula for calculating the T-value based on our data is:

<p align="center"> 
$t = \frac{M_1 - M_2}{\frac{sd_{1;2}}{\sqrt{n_{1;2}}}} = \frac{M_1 - M_2}{\sqrt{\frac{sd_1^2}{n_1} + \frac{sd_2^2}{n_2}}}$
</p>

**F-distribution**

```{r eval=TRUE, echo = FALSE, message = FALSE, fig.align = 'center', out.width="100%"}
knitr::include_graphics("images/F-dist.png")
```

$\chi^2$**-distribution**

```{r eval=TRUE, echo = FALSE, message = FALSE, fig.align = 'center', out.width="100%"}
knitr::include_graphics("images/chisq_dist.png")
```

## Selection of statistical criterion

**Questions that influence the choice of statistical test:**

1.  Dependent variable: quantitative (interval or ratio scale) or categorical (nominative or ordinal scale)?

2.  If the DV is quantitative, is it described by a parametric (usually normal) distribution? *(usually determined “by eye” using a probability density graph or QQ graphs, normality tests will almost always give a negative result due to sensitivity and therefore are rarely used in modern data analysis*

3.  How many independent variables are there?

4.  Are the IV quantitative (interval and ratio scale) or categorical (nominative and ordinal)?

5. If the IV are categorical and we compare groups, how many groups do we compare?

6.  If the IV are categorical and we compare groups, are the data in the groups dependent or not? If not, how much do the data in the groups differ, can we say that the variance of the DV is approximately the same in the groups or not? ( *the equality of variances is called Homogeneity of Variance, it is tested using Levene's test* )

There are a lot of schemes, but they are usually very overloaded and difficult to use, more confusing than helpful.

But for some reason I still decided to try to draw my own, and here is the result: <https://miro.com/app/board/uXjVOxmKhr8=/?share_link_id=245423331470> (will be updated)


### Parametric and nonparametric tests

In the list of questions that influence the choice of statistical criterion, the second item is the question of parametrics. What does this mean?

By **parametric** distribution we mean any distribution that can be described by a law. Let's remember: the distribution law is a formula by which we can give any number the probability of encountering such a value in nature (we discussed this in the topic about distributions). This formula has parameters - unknown variables that we are trying to find out. For a normal distribution, this is the mean of the general population (the mathematical expectation) and the standard deviation of the general population. Let's look again at the normal distribution formula and find them: 
$P(x) = \frac{e^{-(x - \mu)^{2}/(2\sigma^{2}) }} {\sigma\sqrt{2\pi}}$

Let us also remember that the features that we study in psychological research are mainly random variables , and with a large volume of data, random variables, according to the central limit theorem, are normally distributed. Therefore, in the study of psychological features, the parametric distribution usually means one specific distribution - **normal** .

If the variable that expresses the studied feature is normally distributed, then we can use parametric statistical criteria for its study, which assume normal data distribution: t-test, ANOVA. If the variable is not normally distributed, then we need to use their nonparametric analogues: Mann-Whitney or Wilcoxon test, Kruskal-Wallis test.

Testing for normality is a fairly well-established routine in the hypothesis testing algorithm. But there is one nuance.

1.  When the amount of data is small, it is difficult to understand whether the data is distributed normally or not.

2.  With a large amount of data, any deviation from the normal distribution, even the slightest one, will be significant.

Therefore, in the modern research culture, normality testing is no longer given such a sacred meaning. It is more important, for example, to operate with the sample size: if it is large enough (we can return to the empirical rule of n \>= 30), then it is enough to look at the data distribution and QQ-plot, and if they do not raise suspicions, feel free to use parametric methods. If the sample is small or if the data with a large sample clearly differ greatly from the bell-shaped Gaussian, then it is better to use a) nonparametric analogues b) generalized linear models (we will talk about them later).

So, I would not recommend doing a thorough normality test like the Kolmogorov-Smirnov or Shapiro-Ulik tests (although you can count them, it is not forbidden), but to assess the belonging to the parametric (in this case, normal) distribution as follows:

1.  Probability density plot or histogram: symmetry and kurtosis which were discussed in descriptives 

2.  QQ-plot

```{r eval=TRUE, echo = FALSE, message = FALSE, fig.align = 'center', out.width="70%"}
knitr::include_graphics("images/parametric_assumptions.png")
```
Graphs from Andy Field's book “Discovering Statistics Using R”

The top row of graphs shows what graphs might look like in the case of a distribution close to normal: the histogram or probability density plot of the dependent variable looks like a bell-shaped Gaussian, on the QQ plot the theoretical quartiles almost correspond to the actual ones in the data - the more this graph looks like a straight line y = x, the better.

The bottom line of the graphs shows the opposite story, what visualizations of the salary distribution might look like, which we will NOT consider normally distributed: the density and probability graph and the histogram are strongly skewed to the left, the QQ plot no longer resembles a straight line.

### Dependent and independent samples

Another point when choosing a statistical criterion is whether our observations are dependent or not. We discussed dependent and independent samples at the beginning. Now this knowledge will be useful to us in order to correctly select a statistical method.
