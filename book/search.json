[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Qualitative and Quantitative Research Methods in Psychology",
    "section": "",
    "text": "1 Sample\nWe looked at two examples - about the effect of caffeine on attention and the effect of a communication work tool on productivity. In these examples, we assumed that we conducted a study - that is, we recruited a certain number of people and drew a conclusion about how it works in general, not only for these people, but for people in general. Why does this happen? Why can we extend these conclusions to all people who can theoretically drink coffee and use this communication tool? And why do we need this at all?\nImage https://towardsdatascience.com/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sample</span>"
    ]
  },
  {
    "objectID": "index.html#sample-estimates",
    "href": "index.html#sample-estimates",
    "title": "Qualitative and Quantitative Research Methods in Psychology",
    "section": "1.1 Sample estimates",
    "text": "1.1 Sample estimates\nHow can we make a generalization about all people? What do we do if we want to know the average height of people on the entire planet? Do we measure every person out of the nearly 8 billion on the planet and take the arithmetic mean?\n\n\n\n\n\n\n\n\n\n\nhttps://ourworldindata.org/human-height\n\nThe general population is the set (or totality) of all objects under study within the framework of a specific research question.\nA sample or sample population is a portion of a general population selected to study a specific research question.\nWhen we talk about a general population, we talk about its parameters , for example, the average height of people, we denote it by X. We cannot calculate it directly, so we have to resort to sample estimates , which are often denoted by the same letter as the parameter of the general population, only “with a cap” or “with a line” X̅ is the sample mean, the average height of people in a specific sample.\nSo, when we want to make a conclusion about all people from the general population, we conduct a study on a small sample (depending on the research method, the required sample size can be from 10 to 1000 people) and estimate the studied parameter of the general population based on sample estimates.\nTo avoid confusion about what we are talking about, key population and sample estimates are referred to differently:\n\n\n\n\n\n\n\n\n\nGeneral population\nSample\n\n\n\n\nMean (mathematical expectation)\n\\(\\mu\\)\nM, \\(\\overline X\\)\n\n\nStandard deviation\n\\(\\sigma\\)\ns, sd",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sample</span>"
    ]
  },
  {
    "objectID": "index.html#representativeness-of-the-sample",
    "href": "index.html#representativeness-of-the-sample",
    "title": "Qualitative and Quantitative Research Methods in Psychology",
    "section": "1.2 Representativeness of the sample",
    "text": "1.2 Representativeness of the sample\nAn important concept is the representativeness of the sample – the ability of the sample to reflect the studied parameter of the general population.\nThe same sample may be representative for one research question and not representative for another!\nRepresentativeness is achieved:\n\nBy random selection of people (we don’t take specific ones that we like more, but randomly)\nSample size (it has been proven that with a large sample size it is sufficiently representative of the general population to provide an answer about the parameter being studied)\n\nMethods for forming representative samples\n\nSimple random sample\nSystematic sample\nStratified sample\nCluster sample\n\nA simple random sample is drawn from a large population at random - all elements of the population have an equal chance of being included in the sample.\nSystematic sampling imitates random selection and is commonly used by sociologists in field research when a certain step in selecting people is set: for example, every 5th or 10th person.\nStratified sampling is needed if the feature we are studying can be affected by some parameter that is distributed unevenly in the general population: for example, we are studying the level of life satisfaction, but we know that it is affected by the level of income - and we need to ensure an equivalent distribution of income levels in the sample. Then, from these income strata, subjects are selected randomly.\nCluster sampling includes entire individual clusters, without random selection within them: for example, we are studying schoolchildren, and instead of selecting them randomly, we include several specific schools (clusters) in the sample.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sample</span>"
    ]
  },
  {
    "objectID": "index.html#dependent_samples",
    "href": "index.html#dependent_samples",
    "title": "Qualitative and Quantitative Research Methods in Psychology",
    "section": "1.3 Dependent and independent samples",
    "text": "1.3 Dependent and independent samples\nAnother important concept regarding samples is that they can be dependent or independent.\nIndependent samples are used when we compare groups of observations that belong to different people (schools/institutes/etc.). For example, in our example of the effect of caffeine on attention, we can recruit a group of people who will drink 3 cups of coffee a day, and another group who will drink decaffeinated coffee. These are different people, not related to each other in any way.\nBut if we took the same group of people and measured their attention after 3 cups of coffee in one week, and then tested the effects of a decaffeinated drink on these same people , these would be dependent samples , since these are still the same people.\n\n\n\n\n\n\n\n\n\n\n\nCase\n\n\nResearcher Nikita studied the relationship between age, teaching experience and professional burnout of university teachers. For this, he selected several universities: Moscow State University, the Higher School of Economics and RANEPA. Nikita found 10 teachers from the psychology departments of each university on the VKontakte social network and asked them to fill out the Maslach Burnout Inventory (MBI). Based on the processed data, Nikita made the following conclusions: 1) in Russia, younger teachers burn out more often; 2) teachers with less teaching experience burn out more often; 3) MSU teachers generally burn out less than HSE teachers. What sampling methods did Nikita use? Are the conclusions correct?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sample</span>"
    ]
  },
  {
    "objectID": "scales.html",
    "href": "scales.html",
    "title": "2  Data and types of scales",
    "section": "",
    "text": "2.1 Measurement and variable\nWhat does it mean to measure something? It means to bring some value on a scale into conformity with the studied feature .\nWhat could be an example of a trait? Anything that we need to measure in a study: number of cups of coffee per day, concentration level, number of errors, reaction time, degree of burnout, number of tasks completed, level of neuroticism, student rating, number of children in the family, temperature, etc.\nThe feature we are studying is also called a variable . We will encounter this concept constantly in data analysis. In fact, if we look at the observation table, any column with measurements is a variable. The rows contain observations, for example, each new person from our sample. The value in a certain column is the value of the variable of this observation.\nLet’s return to the case with Nikita, who studies burnout of university employees. Let’s make a table of data that Nikita could measure.\nteacher_number &lt;- seq(1,30,1)\nage &lt;-  sample(22:60, size = 30, replace = T)\nexp_years &lt;- sample(1:8, size = 30, replace = T)\nexp_scaled &lt;- ifelse(exp_years &gt;= 1 & exp_years &lt;= 2, \"от 1 до 2\",\n                     ifelse(exp_years &gt; 2 & exp_years &lt;= 5, \"от 3 до 5\",\n                            ifelse(exp_years &gt; 5, \"больше 5\", exp_years)))\nburnout_MBI &lt;- sample(19:70, size = 30, replace = T)\nuniver &lt;-  rep(c(\"MSU\", \"HSE\", \"MSU\", \"RANEPA\", \"HSE\", \"RANEPA\"),5)\nburnout &lt;- tibble(teacher_number, age, exp_years, exp_scaled, burnout_MBI, univer)\nkable(burnout[1:10,])\n\n\n\n\nteacher_number\nage\nexp_years\nexp_scaled\nburnout_MBI\nuniver\n\n\n\n\n1\n28\n2\nот 1 до 2\n60\nMSU\n\n\n2\n38\n6\nбольше 5\n45\nHSE\n\n\n3\n47\n8\nбольше 5\n68\nMSU\n\n\n4\n51\n7\nбольше 5\n54\nRANEPA\n\n\n5\n25\n4\nот 3 до 5\n22\nHSE\n\n\n6\n25\n6\nбольше 5\n61\nRANEPA\n\n\n7\n58\n1\nот 1 до 2\n64\nMSU\n\n\n8\n35\n2\nот 1 до 2\n22\nHSE\n\n\n9\n53\n6\nбольше 5\n33\nMSU\n\n\n10\n26\n1\nот 1 до 2\n60\nRANEPA\nWhat will be the variables here?\nIn the definition of measurement, in addition to the feature, there is a second important concept - the scale.\nA scale is a system of measurement. So that we can all use the same units of measurement and not go crazy, we, the people of planet Earth, use uniform scales. There are 4 of them, and they are metric and non-metric - that is, whether we can attach a measuring ruler to them or not. Here, a measuring ruler is any conventional device that has a division value (centimeter, gram, second, piece).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and types of scales</span>"
    ]
  },
  {
    "objectID": "scales.html#measurement-and-variable",
    "href": "scales.html#measurement-and-variable",
    "title": "2  self-contained: true",
    "section": "3.1 Measurement and variable",
    "text": "3.1 Measurement and variable\nWhat does it mean to measure something? It means to bring some value on a scale into conformity with the studied feature .\nWhat could be an example of a trait? Anything that we need to measure in a study: number of cups of coffee per day, concentration level, number of errors, reaction time, degree of burnout, number of tasks completed, level of neuroticism, student rating, number of children in the family, temperature, etc.\nThe feature we are studying is also called a variable . We will encounter this concept constantly in data analysis. In fact, if we look at the observation table, any column with measurements is a variable. The rows contain observations, for example, each new person from our sample. The value in a certain column is the value of the variable of this observation.\nLet’s return to the case with Nikita, who studies burnout of university employees. Let’s make a table of data that Nikita could measure.\n\nteacher_number &lt;- seq(1,30,1)\nage &lt;-  sample(22:60, size = 30, replace = T)\nexp_years &lt;- sample(1:8, size = 30, replace = T)\nexp_scaled &lt;- ifelse(exp_years &gt;= 1 & exp_years &lt;= 2, \"от 1 до 2\",\n                     ifelse(exp_years &gt; 2 & exp_years &lt;= 5, \"от 3 до 5\",\n                            ifelse(exp_years &gt; 5, \"больше 5\", exp_years)))\nburnout_MBI &lt;- sample(19:70, size = 30, replace = T)\nuniver &lt;-  rep(c(\"MSU\", \"HSE\", \"MSU\", \"RANEPA\", \"HSE\", \"RANEPA\"),5)\nburnout &lt;- tibble(teacher_number, age, exp_years, exp_scaled, burnout_MBI, univer)\nkable(burnout[1:10,])\n\n\n\n\nteacher_number\nage\nexp_years\nexp_scaled\nburnout_MBI\nuniver\n\n\n\n\n1\n48\n1\nот 1 до 2\n25\nMSU\n\n\n2\n39\n3\nот 3 до 5\n52\nHSE\n\n\n3\n45\n4\nот 3 до 5\n30\nMSU\n\n\n4\n33\n2\nот 1 до 2\n33\nRANEPA\n\n\n5\n50\n6\nбольше 5\n22\nHSE\n\n\n6\n56\n8\nбольше 5\n26\nRANEPA\n\n\n7\n44\n1\nот 1 до 2\n54\nMSU\n\n\n8\n48\n6\nбольше 5\n58\nHSE\n\n\n9\n28\n2\nот 1 до 2\n39\nMSU\n\n\n10\n38\n6\nбольше 5\n20\nRANEPA\n\n\n\n\n\n\n\nWhat will be the variables here?\nIn the definition of measurement, in addition to the feature, there is a second important concept - the scale.\nA scale is a system of measurement. So that we can all use the same units of measurement and not go crazy, we, the people of planet Earth, use uniform scales. There are 4 of them, and they are metric and non-metric - that is, whether we can attach a measuring ruler to them or not. Here, a measuring ruler is any conventional device that has a division value (centimeter, gram, second, piece).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>self-contained: true</span>"
    ]
  },
  {
    "objectID": "scales.html#quantitative-and-non-quantitative-data",
    "href": "scales.html#quantitative-and-non-quantitative-data",
    "title": "2  Data and types of scales",
    "section": "2.2 3.2 Quantitative and non-quantitative data",
    "text": "2.2 3.2 Quantitative and non-quantitative data\nData measured by metric scales are quantitative data (e.g. height, weight, number of cases, temperature). What cannot be measured by metric scales (e.g. eye color, well-being, level of neuroticism, level of education) are non-quantitative data, which can have different names: categorical , sometimes qualitative .\nSometimes data is called qualitative as opposed to quantitative, but this is not entirely correct: the distinction between quantitative and qualitative is usually applied to types of research, where qualitative research is, for example, interviews or analysis of blocks of text. But as a result of this analysis, we may well end up with quantitative variables, for example, the number of times a particular word was used, so I recommend not using the word “qualitative” in relation to data, rather than types of research.\nWith quantitative data it’s simpler – this is everything that can be measured with a metric scale, a conventional ruler.\nAmong non-quantitative data, there are two types: categorical and rank (ordinal)\nWe will examine this data in detail on the scales.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and types of scales</span>"
    ]
  },
  {
    "objectID": "scales.html#types-of-scales",
    "href": "scales.html#types-of-scales",
    "title": "2  Data and types of scales",
    "section": "2.3 3.3 Types of scales",
    "text": "2.3 3.3 Types of scales\nAs we have already understood, different data refer to different scales.\nDifferent scales have different measuring power – the precision with which we measure a feature. The same feature can be measured with different precision: for example, depending on the research question, height can be expressed quantitatively in centimeters on the interval {0; ∞}, or it can be coded as {“less than 150 cm”; 150 cm and more} if we are only interested in prevalence over a certain feature.\nThere are 4 scales in total, if you arrange them from bottom to top according to measuring power: names, ordinal, interval, ratios.\n\n\n\n\n\n\n\n\n\nScale\nDescription\nPossible operations\nExamples\n\n\n\n\nRation\nQuantitative, there is absolute zero, you can calculate how much more or less, and how many times\n=, ≠≠, &gt;, &lt;, +, -, ×, ÷\nHeight, weight, number of cases\n\n\nInterval\nQuantitative, but there is no absolute zero, you can calculate how much more or less, but you can’t calculate how many times\n=,≠≠, &gt;, &lt;, +, -\nTemperature in degrees Celsius, timekeeping according to different calendars\n\n\nOrdinal (rank)\nCategorical (qualitative), you can set “more” or “less”, but you can’t calculate quantitatively how much more or less\n=, ≠≠, &gt;, &lt;\nEducation level, neuroticism level, sports rating\n\n\nNominal\nCategorical (qualitative), cannot be set to “more” or “less”\n=,≠≠\nGender, color, place of residence, name of university",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and types of scales</span>"
    ]
  },
  {
    "objectID": "scales.html#continuous-and-discrete-data",
    "href": "scales.html#continuous-and-discrete-data",
    "title": "2  Data and types of scales",
    "section": "2.4 3.4 Continuous and discrete data",
    "text": "2.4 3.4 Continuous and discrete data\nWe see that our richest possibilities for measurement lie in quantitative scales – on the ratio scale (it is the steepest) and the interval scale (it is worse and, in fact, is rarely encountered in research in our field).\nQuantitative data can be discrete, when the variable takes strictly defined values, and continuous, when it can take any values, to infinity or over a given interval.\nFor example, in our example with the burnout study, the variable age can take any value: teachers can be 25 years old, 27.5 years old, or 31,666.. years old - these are all values ​​from the range of acceptable values ​​for this variable . But if we consider the number of people infected with coronavirus, there is no way it can be 27.5 or 31,666.. - the number of people infected is not expressed as fractional shares of one person.\nAn important concept here is the range of admissible values . For a continuous variable, this is always an interval, for example {0;+∞}, for discrete variables, it is strictly defined values, which, nevertheless, can also tend to infinity, for example, {0;1;2;3;4;5;6;7;8…}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and types of scales</span>"
    ]
  },
  {
    "objectID": "scales.html#how-can-i-determine-what-scale-the-data-is-on",
    "href": "scales.html#how-can-i-determine-what-scale-the-data-is-on",
    "title": "2  Data and types of scales",
    "section": "2.5 3.5 How can I determine what scale the data is on?",
    "text": "2.5 3.5 How can I determine what scale the data is on?\nThis is quite a difficult question, and at first it is very difficult to answer it ( and this is normal ). The most difficult thing is to distinguish an ordinal scale from an interval scale, and from the point of view of measurement theory this establishment is not an easy task at all. We rarely encounter a classical interval scale like temperature in degrees Celsius or a scale of chronology in our studies, but the data of the sum scores of questionnaires, most often, also belong to the interval scale, although they have more ordinal properties (it is difficult to say that between 20 and 30 points on the Beck anxiety scale there is exactly the same anxiety interval as between 60 and 70 points). We would rather classify it as an ordinal scale due to the fact that there are a lot of divisions on this scale (more than when we evaluate something on a scale from 1 to 5), and we do not want to limit ourselves to statistical methods suitable for working only with an ordinal scale and not higher. In other cases, defining the scale is a little easier. I suggest using the following simplified algorithm for determining the scale for the first time:\n\nAre there any letters in the variable values? If so, it is either a nominative scale or an ordinal scale.\nCan we arrange these letter values ​​in a certain order on the x-scale? Will anything change if we swap adjacent values? If yes, it is an ordinal scale, if no, it is a nominative scale (for example, I can arrange the names of universities in any order, but the places taken in competitions, only in the order 1-2-3, the change 1-3-2 ruins the meaning)\nIf the answer to (1) is no, and there are no letter values, can the numbers be replaced with letters without changing anything? If yes, then this is also a nominative scale (e.g., subject ID)\nIf there are no letters and the numbers in the data cannot be replaced by letters, it can be either an ordinal scale, an interval scale, or a ratio scale.\nAre the intervals on the scale equal at different places? Will the interval, for example, 20 to 30 include exactly the same number of values ​​as the interval from 50 to 60? If not, it is an ordinal scale; if yes, it is either an interval or a ratio scale.\nIs there an absolute zero on the scale? Doesn’t it sound absurd to say that “value 1” is so many times greater or less than “value 2”? If so, it’s a ratio scale; if not, it’s an interval scale. Done!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and types of scales</span>"
    ]
  },
  {
    "objectID": "descriptives.html",
    "href": "descriptives.html",
    "title": "4  Descriptive statistics",
    "section": "",
    "text": "4.1 Descriptive Visualizations\nWe now turn to a discussion of the first of the two types of statistics, descriptive statistics . When we talk about descriptive statistics, we are always talking about empirically obtained data, not about the population. To say anything about the population, we need to deal with inferential statistics, but for now we will stick with empirically obtained data and sample distributions.\nWhy do we need descriptive statistics? We discussed that descriptive statistics:\nIf it so happens that our collected data represents the entire general population, then we do not even need to engage in inferential statistics - we can draw some conclusions about this data from empirically collected data (for example, compare the performance in class A and B of just one school)\nWe have already discussed two important graphs – the histogram and the probability density plot – when we talked about distributions. These graphs are worth making whenever we want to explore empirically obtained data using descriptive statistics. There seems to be no official term for this, but I also call these graphs descriptive.\nFor example, we built them for data on emotional burnout:\nTo visualize the ordinal variable Teaching Experience exp_scaled\nburnout %&gt;% \n  ggplot(aes(x=exp_scaled)) +\n  geom_bar() +\n  theme_minimal()\nTo visualize the quantitative variable Age (ratio scale)\nburnout %&gt;% \n  ggplot(aes(x=age)) +\n  geom_density() +\n  theme_minimal()\nDistribution graphs, in fact, already give us a lot - at least an understanding of what family of distributions a variable may belong to, and what the distribution properties of these data are. When talking about the properties of graphs, there are usually two.\nThe asymmetry coefficient, or “skewness”, is an indicator of the symmetry of the graph: if the graph is symmetrical6, this indicator is equal to 0; if not, it has a value different from zero in the direction in which the graph is skewed.\nExcess, “stretching” (kurtosis) is an indicator of how much the graph is stretched upward or “flattened” along the abscissa axis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>descriptive</span>"
    ]
  },
  {
    "objectID": "descriptives.html#descriptive-visualizations",
    "href": "descriptives.html#descriptive-visualizations",
    "title": "4  Descriptive statistics",
    "section": "",
    "text": "If the asymmetry coefficient is positive,S&gt;0S&gt;0– they talk about right-sided assimilation (the right side is more elongated than the left).\nIf the asymmetry coefficient is negative,S&lt;0S&lt;0– they talk about left-sided assimilation (the left side is more elongated than the right).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>descriptive</span>"
    ]
  },
  {
    "objectID": "descriptives.html#ctm",
    "href": "descriptives.html#ctm",
    "title": "4  Descriptive statistics",
    "section": "4.2 Measures of Central Tendency",
    "text": "4.2 Measures of Central Tendency\nMeasures of central tendency are descriptive statistics that tell you something about the “center of mass” of a distribution: where is its center, where is the most data in this distribution? This can be very useful in applied questions.\n\nLet’s imagine that we are interested in the question: what is the well-being of Russians? Has it changed from 2008 to 2022, and if so, how? How can we answer this question? ( let’s imagine that we have access to real data of all Russians collected on this issue, and there is no need to make a statistical conclusion on a small sample )\n\nLet’s say we evaluate well-being by annual per capita income. The first thing that comes to mind is to calculate the average income in 2008 and in 2022 and see how they differ. Will this be the right calculation? Most likely, not.\nThe point is that per capita income is a very uneven indicator (variable). https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%94%D0%B6%D0%B8%D0%BD%D0%B8\n\n4.2.1 Mean, median, mode\nThe arithmetic mean (mean) is the sum of all values divided by the number of observations. It is calculated using the formula\n\\(\\bar X = \\dfrac{\\sum_{i=1}^{n}x_i}{n}\\)\nFor example, we have a data set:\n\n\n [1] 49 37  1 25 10 36 18 24  7 45 47\n\n\nIts average\n\nmean(sample_data1)\n\n[1] 27.18182\n\n\nProperties of the average\nIt is useful to remember the properties of the average:\n\nIf you add (or subtract) the same number to each sample value, the average will also increase (or decrease) by the same number:¯\\(\\bar X_(x_i+c) = \\bar X + c\\)\nIf each sample value is multiplied (or divided) by the same number, then the average will also increase (or decrease) by the same number of times.¯\\(\\bar X_(x_i \\times c) = \\bar X \\times c\\)\nThe sum of all deviations from the mean is zero. \\(\\sum_{i=1}^{n}(\\bar X - x_i) = 0\\)\n\nThe median is the boundary that divides an ordered set of data into two. To calculate the median, we need to: 1) arrange all the available values in ascending order; 2) find the middle: this will be either the value corresponding to the place \\(\\frac{n}{2}+1\\), if n is odd, or the average of two central values \\((\\frac{1}{2}(X_{n/2}+X_{n+1})/2\\), if n is even.\n\nsort(sample_data1)\n\n [1]  1  7 10 18 24 25 36 37 45 47 49\n\nmedian(sample_data1)\n\n[1] 25\n\n\n\n\n [1] 37 46 20 26  3 41 25 27 36 50\n\n\n\nsort(sample_data2)\n\n [1]  3 20 25 26 27 36 37 41 46 50\n\nmedian(sample_data2)\n\n[1] 31.5\n\n\nIn practice, we rarely encounter data where each value is presented only once – we have already discussed at length that in statistics we work with probabilities and frequencies. Therefore, in reality, we must remember that we are not halving the values ​​themselves, as in the case of the average – but the distribution of values. In a histogram or probability density graph, the median is a line that divides the graph into two equal parts: the same number of data (values ​​taken with their frequencies) should remain on the left and right.\n\nburnout %&gt;% \n  ggplot(aes(x=exp_years)) +\n  geom_histogram(binwidth = 1) +\n  geom_vline(xintercept = median(exp_years)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nMode is the value of a feature that occurs more often than others.\nOn the histogram, the mode will always be on the highest bar, and on the probability density graph – at its peak (or very, very close to it – remember that the probability density graph is a trick, where we draw each point as if “taking” its neighborhood with us, therefore, depending on the size of the neighborhood, the median may drift a little from the peak, but this is a tiny deviation)\nWeighted average.\nSquare mean.\n\n\n4.2.2 Features of use\nIn the welfare valuation example, we found that the mean is a poor choice for describing these data using measures of central tendency. So which measures of central tendency should we choose?\n\n\n\n\n\n\n\n\nMeasure of CT\nData and scale\nFeatures of use\n\n\n\n\nAverage\nQuantitative only: ratio or interval scale\na fairly large sample, the distribution is symmetrical, there are no noticeable outliers\n\n\nMedian\nQuantitative or ordinal: ratio scale, interval or ordinal (rank) scale\nwe can use it when the distribution is not symmetrical, there are outliers, we cannot use it for the nominative scale\n\n\nFashion\nAny scale, Nominal, Ordinal, Quantitative (ratio or interval)\nmost often used where we cannot perform metric operations, but cannot accurately calculate the mode for continuous quantities\n\n\n\nWhere is the mean, median and mode on the graphs?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have come close to the concepts that can be (and we will be) directly applied to the data. To make these concepts close to reality, we will take a real dataset (collected data) on which we will work further. As an example, we will take a dataset from kaggle https://www.kaggle.com/datasets/uciml/student-alcohol-consumption . This is data from two Portuguese schools (more likely colleges) with detailed socio-demographic information about students, including how they study in mathematics and Portuguese and how often they drink alcohol. I took this dataset, because it contains variables of different types of data on different scales (both a relationship scale, such as age, and an ordinal scale, such as ratings of mother’s or father’s education or support in the family).\n\nkable(students[1:10,]) %&gt;% scroll_box(width = \"100%\") \n\n\n\n\n\nstudent\nschool\nsex\nage\naddress\nfamsize\nPstatus\nMedu\nFedu\nMjob\nFjob\nreason\nguardian\ntraveltime\nstudytime\nfailures\nschoolsup\nfamsup\npaid_mat\nactivities\nnursery\nhigher\ninternet\nromantic\nfamrel\nfreetime\ngoout\nDalc\nWalc\nhealth\nabsences_mat\nG1_mat\nG2_mat\nG3_mat\npaid_por\nabsences_por\nG1_por\nG2_por\nG3_por\nG_mat\nG_por\nansences_mat_groups\nansences_por_groups\n\n\n\n\nid1\nGP\nF\n18\nU\nGT3\nA\n4\n4\nat_home\nteacher\ncourse\nmother\n2\n2\n0\nyes\nno\nno\nno\nyes\nyes\nno\nno\n4\n3\n4\n1\n1\n3\n6\n5\n6\n6\nno\n4\n0\n11\n11\n5.666667\n7.333333\nmiddle\nless\n\n\nid2\nGP\nF\n17\nU\nGT3\nT\n1\n1\nat_home\nother\ncourse\nfather\n1\n2\n0\nno\nyes\nno\nno\nno\nyes\nyes\nno\n5\n3\n3\n1\n1\n3\n4\n5\n5\n6\nno\n2\n9\n11\n11\n5.333333\n10.333333\nless\nless\n\n\nid4\nGP\nF\n15\nU\nGT3\nT\n4\n2\nhealth\nservices\nhome\nmother\n1\n3\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nyes\n3\n2\n2\n1\n1\n5\n2\n15\n14\n15\nno\n0\n14\n14\n14\n14.666667\n14.000000\nless\nless\n\n\nid5\nGP\nF\n16\nU\nGT3\nT\n3\n3\nother\nother\nhome\nfather\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nno\nno\n4\n3\n2\n1\n2\n5\n4\n6\n10\n10\nno\n0\n11\n13\n13\n8.666667\n12.333333\nless\nless\n\n\nid6\nGP\nM\n16\nU\nLE3\nT\n4\n3\nservices\nother\nreputation\nmother\n1\n2\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nno\n5\n4\n2\n1\n2\n5\n10\n15\n15\n15\nno\n6\n12\n12\n13\n15.000000\n12.333333\nmiddle\nmiddle\n\n\nid7\nGP\nM\n16\nU\nLE3\nT\n2\n2\nother\nother\nhome\nmother\n1\n2\n0\nno\nno\nno\nno\nyes\nyes\nyes\nno\n4\n4\n4\n1\n1\n3\n0\n12\n12\n11\nno\n0\n13\n12\n13\n11.666667\n12.666667\nless\nless\n\n\nid8\nGP\nF\n17\nU\nGT3\nA\n4\n4\nother\nteacher\nhome\nmother\n2\n2\n0\nyes\nyes\nno\nno\nyes\nyes\nno\nno\n4\n1\n4\n1\n1\n1\n6\n6\n5\n6\nno\n2\n10\n13\n13\n5.666667\n12.000000\nmiddle\nless\n\n\nid9\nGP\nM\n15\nU\nLE3\nA\n3\n2\nservices\nother\nhome\nmother\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nyes\nno\n4\n2\n2\n1\n1\n1\n0\n16\n18\n19\nno\n0\n15\n16\n17\n17.666667\n16.000000\nless\nless\n\n\nid10\nGP\nM\n15\nU\nGT3\nT\n3\n4\nother\nother\nhome\nmother\n1\n2\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nno\n5\n5\n1\n1\n1\n5\n0\n14\n15\n15\nno\n0\n12\n12\n13\n14.666667\n12.333333\nless\nless\n\n\nid11\nGP\nF\n15\nU\nGT3\nT\n4\n4\nteacher\nhealth\nreputation\nmother\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nyes\nno\n3\n3\n3\n1\n2\n2\n0\n10\n8\n9\nno\n2\n14\n14\n14\n9.000000\n14.000000\nless\nless\n\n\n\n\n\n\n\n\nstudents %&gt;% \n  ggplot(aes(x=age)) +\n  geom_histogram(binwidth = 1) +\n  geom_density(aes(x = age)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ntable(students$age)\n\n\n15 16 17 18 19 22 \n71 91 87 64  6  1 \n\nprop.table(table(students$age))\n\n\n      15       16       17       18       19       22 \n0.221875 0.284375 0.271875 0.200000 0.018750 0.003125 \n\n# round(prop.table(table(students$age, students$Walc), 1), 2)\n\n\nmean(students$age)\n\n[1] 16.525\n\n\n\nmedian(students$age)\n\n[1] 16\n\n\n\nmode(students$age)\n\n[1] 16\n\n\n\nstudents %&gt;% \n  ggplot(aes(x=G_mat)) +\n  geom_density() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nmean(students$G_mat)\n\n[1] 11.16979\n\n\n\nmedian(students$G_mat)\n\n[1] 11\n\n\n\ntable(students$Mjob)\n\n\n at_home   health    other services  teacher \n      44       30      116       75       55 \n\nprop.table(table(students$Mjob))\n\n\n at_home   health    other services  teacher \n0.137500 0.093750 0.362500 0.234375 0.171875 \n\nmode(students$Mjob)\n\n[1] \"other\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>descriptive</span>"
    ]
  },
  {
    "objectID": "descriptives.html#quantiles-quartiles-and-percentiles",
    "href": "descriptives.html#quantiles-quartiles-and-percentiles",
    "title": "4  Descriptive statistics",
    "section": "4.3 Quantiles, quartiles and percentiles",
    "text": "4.3 Quantiles, quartiles and percentiles\nWe have found out that the median divides the distribution in half. This is convenient. For example, to estimate below / above what value the first or second half of the data is.\nThese questions come up quite often, and not just about half of the data, but about specific chunks of data. Quantiles are a general term for points that divide a distribution into equal chunks of data—say, ten percent chunks.\nDeciles are special cases of quantiles, 9 points that divide the entire distribution into 10 equal parts. Thus, each part contains 10% of the data.\nPercentiles are special cases of quantiles, 99 points that divide the entire distribution into 100 equal parts, so that each part contains 1% of the data.\nQuartiles are special cases of quantiles, they divide the entire distribution into four parts, 25% of the data each. That is, there are 4 quartiles in total, and they are designated by the letter Q: Q1, Q2, Q3, Q4.\nIt turns out that:\n\nto the left of the first (lower) quartileQ1Q125% of observations are\nto the left of the second (middle) quartile (median)Q2Q250% of observations lie\nto the left of the third (upper) quartileQ3Q375% of observations are\n\nknitr::include_graphics(\"docs/images/quartiles-2.png\")\n\nknitr::include_graphics(\"images/quartiles-2.png\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>descriptive</span>"
    ]
  },
  {
    "objectID": "descriptives.html#measures-of-variability-variability",
    "href": "descriptives.html#measures-of-variability-variability",
    "title": "4  Descriptive statistics",
    "section": "4.4 Measures of variability (variability)",
    "text": "4.4 Measures of variability (variability)\nIn addition to measures of central tendency, there are measures of variability, also known as measures of changeability or dispersion. Variability is the variability of values, how much the data differs from observation to observation within the sample.\n\nLet’s say you find that your red blood cell count is 3.8*10, while the norm is 4*10. How big of a deviation is this?\n\nWhy do we need them? Isn’t it enough to use measures of central tendency to describe a distribution? No, it’s not enough. While measures of central tendency show where our data tends to be, where their “center of mass” is, that is, what single number can characterize our data in general - measures of variability show how much spread there is in our data, how close each of our observations lies to the mean?\nMeasures of central tendency may coincide although measures of variability differ significantly, for example\n\n\n\n\n\n\n\n\n\nImages from Anton Angelhardt’s textbook\nThese distributions are very different, butthey are together after allif you look at their averages, they will be the same\n\n\n\n\n\n\n\n\n\n\n4.4.1 Range, standard deviation and variance\nRange is the difference between the maximum and minimum values in a sample \\(x_{max}-x_{min}\\)\n\nsample_data1\n\n [1] 49 37  1 25 10 36 18 24  7 45 47\n\nrange(sample_data1)\n\n[1]  1 49\n\n\nHere are the minimum and maximum values, to calculate the range, you need to subtract the minimum from the maximum. And as you can see, the range is very sensitive to outliers - if you add the value “3000” to this sample, the range will change dramatically.\nWe have already heard about standard deviation (at least, it appears in the formula of normal distribution). Now let’s get to know it better and try to derive it.\nSince if we calculate the deviation of each observation from the mean and try to average them, we will get zero, we need a different approach to estimating the deviation. If we want to get rid of the problem of getting zero, we usually have two options: take the values ​​by module or square them. To obtain an estimate of the deviation, we used the second option and introduced another important measure of variability for - variance .\nVariance is a measure of variability, which is the square of the standard deviation and is calculated using the formula \\(D = \\dfrac{\\sum_{i=1}^{n}(x_i - \\bar x)^2}{n}\\)\nThe greater the variability in the data, the greater (proportional to the square of the deviations) the dispersion.\nAn important point is the formula for the dispersion for the general population. For sample dispersion, the number of degrees of freedom is reduced by one, so the formula will take the following form \\(D = \\dfrac{\\sum_{i=1}^{n}(x_i - \\bar x)^2}{n-1}\\)\nStandard deviation, also known as root mean square deviation , is the average deviation of observations from their mean. It is calculated as the square root of the variance: \\(sd = \\sqrt{\\dfrac{\\sum_{i=1}^{n}(x_i - \\bar x)^2}{n-1}}\\)\nProperties of variance and standard deviation\nJust like the mean, there are important properties for the variance/standard deviation\n\nIf the same number (constant) is added (or subtracted) from each sample value \\(c\\), then the dispersion and standard deviation will not change: \\(D_(x_i+c) = D\\), \\(sd_(x_i+c) = sd\\)\nIf each sample value is multiplied (or divided) by the same numbercc, then the dispersion will increase (decrease) in \\(c^2\\) times \\(D_(x_i \\times c) = D \\times c^2\\), and the average will increase (or decrease) in \\(c\\) times: \\(sd_(x_i \\times c) = sd \\times c\\)\nIf the values ​​of two samples do not differ from each other, then they have equal variance and standard deviation\n\nUsing student data as an example:\n\nrange(students$age)\n\n[1] 15 22\n\nrange(students$G_mat)\n\n[1]  2.00000 19.33333\n\nrange(students$Walc)\n\n[1] 1 5\n\nrange(students$Mjob)\n\n[1] \"at_home\" \"teacher\"\n\n\n\nsd(students$age)\n\n[1] 1.141687\n\nsd(students$G_mat)\n\n[1] 3.559905\n\nsd(students$Walc)\n\n[1] 1.264167\n\n\nInterquartile range",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>descriptive</span>"
    ]
  },
  {
    "objectID": "descriptives.html#boxplot-and-violinplot",
    "href": "descriptives.html#boxplot-and-violinplot",
    "title": "4  Descriptive statistics",
    "section": "4.5 Boxplot and Violinplot",
    "text": "4.5 Boxplot and Violinplot\n\n\n# knitr::include_graphics(\"docs/images/Boxplot_vs_PDF.png\")\n\n\n\n\n# knitr::include_graphics(\"docs/images/boxplots_hist.png\")\n\n\n\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nstudents %&gt;% \n  ggplot(aes(x=ansences_mat_groups, y = G_mat)) +\n  geom_violin(aes(fill = ansences_mat_groups)) +\n  geom_boxplot(aes(fill = ansences_mat_groups), width=.1) +\n  scale_fill_viridis(discrete=TRUE) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>descriptive</span>"
    ]
  },
  {
    "objectID": "distributions.html",
    "href": "distributions.html",
    "title": "3  Distributions",
    "section": "",
    "text": "3.1 5.1 Types of distributions\nWhat is distribution? We use this word all the time.\nWhen speaking about distributions, we mean the law of distribution of a random variable – the correspondence between the possible values ​​of this variable from the range of its admissible values ​​and: the probabilities of these values ​​– for a discrete variable, or the probability density – for a continuous variable.\nThe laws of probability distributions can not be derived for all quantities! But it so happened that we can describe some patterns, probability distributions, with formulas, as in physics: the force of gravity (the force with which the Earth attracts all bodies) is directly proportional to the mass of the object, taken with the coefficient of acceleration of free fall \\(F = m*g\\).. We noticed this in relation to the surrounding world and derived a law (not we personally, but representatives of the planet Earth in general). The same is with distributions: we noticed that the probabilities of the distribution of some random variables obey certain laws, and wrote them down. For example, it has been proven that continuous random variables, which are affected by many random factors (for example, height, weight, etc.), are distributed in accordance with the Gaussian distribution , also known as the normal distribution . Its formula is:\n\\(P(x) = \\frac{e^{-(x - \\mu)^{2}/(2\\sigma^{2}) }} {\\sigma\\sqrt{2\\pi}}\\)\nplot(dnorm(1:1000, mean = 500, sd = 100))\nOr, for example, the exponential distribution:\n\\(P(x)= \\lambda \\times e^{-\\lambda x}\\)\nplot(dexp(x = 1:100, rate = 0.1))\n\\(P(x)= \\lambda \\times e^{-\\lambda x}\\)\nplot(dchisq(1:1000, df=100))\nOr, for example, the binomial distribution:\n\\(P(k | n, p) = \\frac{n!}{k!(n-k)!} \\times p^k \\times (1-p)^{n-k} =  {n \\choose k} \\times p^k \\times (1-p)^{n-k}\\)\nplot(dbinom(1:100, size=100, prob=0.5))\nYou can look and be horrified here http://www.math.wm.edu/~leemis/chart/UDR/UDR.html . Fortunately, we won’t need any of this.\nLess scary version https://www.johndcook.com/blog/distribution_chart/#normal",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>distributions</span>"
    ]
  },
  {
    "objectID": "distributions.html#types-of-distributions",
    "href": "distributions.html#types-of-distributions",
    "title": "3  distributions",
    "section": "4.1 5.1 Types of distributions",
    "text": "4.1 5.1 Types of distributions\nThe laws of probability distributions can not be derived for all quantities! But it so happened that we can describe some patterns, probability distributions, with formulas, as in physics: the force of gravity (the force with which the Earth attracts all bodies) is directly proportional to the mass of the object, taken with the coefficient of acceleration of free fall \\(F = m*g\\).. We noticed this in relation to the surrounding world and derived a law (not we personally, but representatives of the planet Earth in general). The same is with distributions: we noticed that the probabilities of the distribution of some random variables obey certain laws, and wrote them down. For example, it has been proven that continuous random variables, which are affected by many random factors (for example, height, weight, etc.), are distributed in accordance with the Gaussian distribution , also known as the normal distribution . Its formula is:\n\\(P(x) = \\frac{e^{-(x - \\mu)^{2}/(2\\sigma^{2}) }} {\\sigma\\sqrt{2\\pi}}\\)\n\nplot(dnorm(1:1000, mean = 500, sd = 100))\n\n\n\n\n\n\n\n\nOr, for example, the exponential distribution:\n\\(P(x)= \\lambda \\times e^{-\\lambda x}\\)\n\nplot(dexp(x = 1:100, rate = 0.1))\n\n\n\n\n\n\n\n\n\\(P(x)= \\lambda \\times e^{-\\lambda x}\\)\n\nplot(dchisq(1:1000, df=100))\n\n\n\n\n\n\n\n\nOr, for example, the binomial distribution:\n\\(P(k | n, p) = \\frac{n!}{k!(n-k)!} \\times p^k \\times (1-p)^{n-k} =  {n \\choose k} \\times p^k \\times (1-p)^{n-k}\\)\n\nplot(dbinom(1:100, size=100, prob=0.5))\n\n\n\n\n\n\n\n\nYou can look and be horrified here http://www.math.wm.edu/~leemis/chart/UDR/UDR.html . Fortunately, we won’t need any of this.\nLess scary version https://www.johndcook.com/blog/distribution_chart/#normal",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>distributions</span>"
    ]
  },
  {
    "objectID": "distributions.html#distribution-functions",
    "href": "distributions.html#distribution-functions",
    "title": "3  Distributions",
    "section": "3.2 5.2 Distribution functions",
    "text": "3.2 5.2 Distribution functions\nAbove we got acquainted with probability density, let’s record all the functions applicable to probability that are useful:\n\nprobability density function for continuous variables (e.g. height, weight) and probability mass function for discrete variables (e.g. number of cases) – the simplest basic function, often denoted by the letterd*\ncumulative distribution function / probability density function for continuous variables, often denoted by the letterp*\nquantile function , also known as the inverse cumulative distribution density function (more on that later), is often denoted by the letter q* probability density function (cumulative distribution function; cdf)\n\nWhy are they needed?\nLet’s look at the example of IQ tests.\nProbability density function\n\niq &lt;- seq(50,150, 0.1)\nplot(iq, dnorm(iq, mean = 100, sd = 15))\n\n\n\n\n\n\n\n\nCumulative distribution function (cdf)\n\nplot(iq, pnorm(iq, mean = 100, sd = 15))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>distributions</span>"
    ]
  },
  {
    "objectID": "r_1.html",
    "href": "r_1.html",
    "title": "6  Introduction to Data Analysis in R",
    "section": "",
    "text": "6.1 14.1 Data and variables\nThis book is written using the R programming and data processing language and the Bookdown package in the RStudio data science environment.\nTo start working in it, you need to download and install the R language itself https://cran.r-project.org/ and download and install RStudio, the environment for work https://posit.co/downloads/ .\nData is information presented in a form suitable for storage and processing by humans or information systems (ISO/IEC/IEEE 24765-2010). If data is presented in a form suitable for processing by information systems, it is formalized.\nA variable is a shell that we define to store data in it and perform operations on it. A variable has a name and the data that it stores.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "r_1.html#data-and-variables",
    "href": "r_1.html#data-and-variables",
    "title": "6  Introduction to Data Analysis in R",
    "section": "",
    "text": "6.1.1 14.1.1 Basic data types\n\nNumeric (whole numbers – integer or numeric , real numbers – real , floating point numbers (fractional) – float )\nText ( character , if one character, or string – many characters)\nLogical ( logical or boolean – accepts only True / False values)\nNA – missing values ​​(not available)\nNaN – not a number, the result of performing an impossible numerical operation (for example, division by 0)\n\n\n\n6.1.2 14.1.2 Basic data structures\n\nVector – a one-dimensional array of one data type\nAn array is a multidimensional array of one data type, consisting of vectors of the same length.\nMatrix - a two-dimensional array of only numeric data\nA list is essentially a multidimensional array, but can consist of vectors of different lengths and have data of different types.\nData, dataframe - essentially a list, but all vectors are the same length\n\n\n\n\n\n\n\n\n\n\n\nhttps://practicum.yandex.ru/blog/10-osnovnyh-struktur-dannyh/\n\nhttps://practicum.yandex.ru/blog/10-osnovnyh-struktur-dannyh/",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "r_1.html#operations-with-variables-and-functions",
    "href": "r_1.html#operations-with-variables-and-functions",
    "title": "6  Introduction to Data Analysis in R",
    "section": "6.2 14.2 Operations with variables and functions",
    "text": "6.2 14.2 Operations with variables and functions\nDepending on what type of variables we are working with, we can perform different operations on them.\nAn operation is the execution of some action on data. What performs this action is called an operator or a function . The difference between them is that an operator performs atomic (single and simple actions), for example, an operator can be a sign of addition, division, greater than or less than, etc. A function does more complex actions: for example, create a vector using the function c(), read data using the function read_csv(), filter data using the function select(). Note that when calling a function, its name is always followed by parentheses.\n\n\n\n\n\n\n\nData type\nPossible atomic operations\n\n\n\n\nNumerical\n= (assignment), +, -, /, *, %\n\n\nText\n= (assignment),+ (concatenation), search for a specific character\n\n\nLogical\n= (assignment),&gt;, &lt;, == (equal), != (not equal)\n\n\n\n\n# vectors  -------------------------------------------------------\nc(1, 2, 3, 4, 5) # c() -- function for vector creation\n\n[1] 1 2 3 4 5\n\n# numeric vectors  -------------------------------------------------------\nc(1, 2, 3, 4, 5, 7, 21, 143)\n\n[1]   1   2   3   4   5   7  21 143\n\nvector1 &lt;- c(1, 2, 3, 4, 5) # vector1 -- variable name, 1, 2, 3, 4, 5 -- arguments of function с()\n\nage &lt;- c(18, 22, 25, 20, 21)\n\n1:10 # function : creates vector from 1 to 10 with step 1\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n10:1\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\nseq(2, 10, 2) # seq(): creates numeric vector from 2 tp 10 and step 2 \n\n[1]  2  4  6  8 10\n\nseq(1, 10, 2) \n\n[1] 1 3 5 7 9\n\n# Character vector -------------------------------------------------------\n\nanswers &lt;- c(\"no\", \"yes\", \"yes\", \"yes\", \"no\")\n\n#Transformation of data types -- TRUE и FALSE became 1 and 0\nc(1, 2, 3, 4, 5, TRUE, FALSE)\n\n[1] 1 2 3 4 5 1 0\n\nc(1, 2, 3, 4, 5, \"1\", \"0\")\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"1\" \"0\"\n\n# Logical vectors ------------------------------------------------------\n\ncondition &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE) #Logical vector\n\nc(1, 2, 3, 4, TRUE, FALSE)\n\n[1] 1 2 3 4 1 0\n\nc(1, 2, 3, 4, 5, 6)\n\n[1] 1 2 3 4 5 6\n\nc(1, 10, 3, -4, \"green\", \"brown\")\n\n[1] \"1\"     \"10\"    \"3\"     \"-4\"    \"green\" \"brown\"\n\n# Logical expressions ------------------------------------------------------\n\na &lt;- 5\nb &lt;- 10\n\na &gt; b\n\n[1] FALSE\n\na &gt;= b\n\n[1] FALSE\n\na == b\n\n[1] FALSE\n\na != b\n\n[1] TRUE\n\na &lt;= b\n\n[1] TRUE\n\na &lt; b\n\n[1] TRUE\n\n#function performs if else: \n# ifelse() involves 3 arguments: ( conditioin; what should it does if condition true ;\n# what should it does if condition false)\n\nifelse(a &lt; b, a+b, \"а more than b\")\n\n[1] 15\n\n# dataframes ---------------------------------------------------------------\n\neye_color &lt;- c(\"green\", \"brown\", \"grey\", \"blue\", \"red\")\n  \ndata &lt;- data.frame(vector1, age, answers, condition, eye_color) # data.frame() creates dataframe from vectors\nView(data) #  View() open this dataframe in the next tab\n\ndata$age # select column from dataframe\n\n[1] 18 22 25 20 21\n\n# описательные визуализации\nhist(data$age, breaks = c(18, 22, 27))\nlines(density(data$age)) \n\n\n\n\n\n\n\n?hist\nplot(density(data$age))\n\n\n\n\n\n\n\n# descriptive statistics\nmean(data$age)\n\n[1] 21.2\n\nsd(data$age)\n\n[1] 2.588436\n\nround(sd(data$age),1) #rounds to 1 digit\n\n[1] 2.6\n\nmin(data$age)\n\n[1] 18\n\nmax(data$age)\n\n[1] 25\n\nrange(data$age)\n\n[1] 18 25\n\nsummary(data$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   18.0    20.0    21.0    21.2    22.0    25.0",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "r_1.html#getting-started-in-the-environment",
    "href": "r_1.html#getting-started-in-the-environment",
    "title": "6  Introduction to Data Analysis in R",
    "section": "6.3 14.3 Getting Started in the Environment",
    "text": "6.3 14.3 Getting Started in the Environment\nIn this tutorial I will provide tasks for training in any data working environment and code for working with data in R.\nFor most interesting operations in R, we will need additional packages - a set of functions that someone has already written for us (the same as, for example, import numpy as npin Python). We will mainly need the package tidyverse, literally - “the universe of clean data”. Let me say right away that everything marked with the # symbol is comments, our hints that will not be executed. It is important to leave them for yourself, so as not to forget what is happening here.\nFor the package to work, it needs to be 1. installed and 2. connected\n\n# install package\ninstall.packages(\"tidyverse\")\n\n\n# switch on package\nlibrary(tidyverse)\n\nYou only need to install the package once after installing R, libraryyou always need to connect via when you open RStudio. An error like “could not find function” in 99% of cases indicates that the package from which it is used is not connected.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "r_1.html#getting-started-with-data",
    "href": "r_1.html#getting-started-with-data",
    "title": "6  Introduction to Data Analysis in R",
    "section": "6.4 14.4 Getting Started with Data",
    "text": "6.4 14.4 Getting Started with Data\nWe will work with the World Happiness Report data for 2016 https://www.kaggle.com/datasets/unsdsn/world-happiness . This is the World Happiness Report https://en.wikipedia.org/wiki/World_Happiness_Report , which shows how residents of different countries rate their level of happiness. They can also be downloaded from the link https://raw.githubusercontent.com/elenary/StatsForDA/main/2016.csv (right-click in the opened file - Save as). The 2016 data we will work with includes 157 countries. By the way, can you download data for different years and see how the number of participating countries changed? A little later we will learn how to do this using code . There are the following variables here:\n\nHappiness Rank - position in the ranking\nHappiness Score - the absolute value of the happiness score\nSE - standard error\nEconomy - GDP per capita\nFamily - Social support, feeling of family\nHealth - Life expectancy\nFreedom - Freedom\nTrust (Goverment Corruption) - perceived level of corruption\nGenerosity - how much is donated to charity\nDystopia is a dystopian country with the lowest indicators in all existing parameters.\n\nEconomy, Family, Health, Freedom, Trust, Generosity – 6 indicators by which the level of happiness is calculated.\nTo perform operations with data, they first need to be read , loaded into the data working environment. In modern traditions of R, this is easiest to do using a function read_csv()from the package tidyverse. Do not forget that any function call is accompanied by parentheses, into which arguments are passed. In the function read_csv()in parentheses, you need to pass the path to the file that we want to read. It can be copied through the file properties. But in order not to bother with the path, the easiest way is to put the data file in the same folder where our file with the code itself is located (for this, you need to save it). And the most elegant and commonly used solution is to create a project File - New Project, and store all files in the project folder, this saves from a large number of errors. Importing files is described in great detail by Ivan Pozdnyakov https://pozdniakov.github.io/tidy_stats/030-import_data.html\n\nwhr &lt;- read_csv(\"2016.csv\") # read data sheet \n\n# View(whr) \n\nNow I want to explore this data. Remember how we start exploring data? With descriptive statistics\nI can select all the variables one by one and calculate the mean and standard deviation for them – this is what is in the Descriptives tab in Jamovi. Since the column does not exist in a vacuum, but inside a dataset, we need to somehow indicate that we are interested in a specific column inside a specific dataset. For now, the easiest way to do this is with the icon $:\n\nwhr$Family # select column\n\n  [1] 1.16374 1.14524 1.18326 1.12690 1.13464 1.09610 1.02912 1.17278 1.10476\n [10] 1.08764 0.99537 1.08383 1.04782 1.02152 1.08113 1.09774 1.03938 1.05249\n [19] 1.16157 1.03999 0.71460 0.86758 1.08672 0.90587 0.98912 1.06612 1.00793\n [28] 0.87114 1.03143 1.09879 1.02169 1.00508 1.04477 0.84829 0.92624 0.87964\n [37] 1.12945 0.83309 0.87119 0.77866 0.87758 0.94397 0.98569 1.03302 1.08268\n [46] 0.80975 0.88025 0.89521 1.16810 1.04167 0.85974 0.68655 1.06054 0.95544\n [55] 0.83132 1.05261 1.04685 0.72194 0.83779 1.06411 1.04993 0.81826 1.05613\n [64] 0.81255 1.03817 0.75695 0.95076 0.95025 0.70697 1.11111 0.72803 1.05163\n [73] 0.96372 0.60809 0.87021 0.33613 0.66062 0.87717 0.87625 0.86216 0.76042\n [82] 0.87877 0.79381 0.90836 0.95434 0.81329 0.64367 0.74173 0.99496 0.38595\n [91] 0.93164 0.26135 0.64184 0.94367 0.78236 0.79117 0.75862 0.43165 0.75473\n[100] 0.75602 1.08983 0.54970 0.64498 0.75596 0.38857 0.63760 0.69699 0.71629\n[109] 0.50163 0.24749 0.62800 0.59205 0.70362 0.62542 0.37932 0.96053 0.84783\n[118] 0.29247 0.69981 0.49813 0.62477 0.76240 1.01413 0.49353 0.80676 0.19249\n[127] 0.47799 0.77416 0.92542 0.84142 0.71478 0.14700 0.81928 0.72368 0.86333\n[136] 0.29561 0.89186 0.60323 0.57576 0.53750 0.66366 0.60530 0.18519 0.63178\n[145] 0.63054 0.90981 0.47493 0.46115 0.77623 0.50353 0.31090 0.61586 0.10419\n[154] 0.11037 0.00000 0.14866 0.23442\n\n\nOr we can remember that a data frame has two dimensions, like a two-dimensional array , and we can access it by index (in square brackets): row number (first number) and column number (second number). If we are not interested in a specific row, but all rows, then nothing is put in place of this index, as if we are skipping it.\n\nwhr[,8] # select column but using other way\n\n# A tibble: 157 × 1\n   Family\n    &lt;dbl&gt;\n 1   1.16\n 2   1.15\n 3   1.18\n 4   1.13\n 5   1.13\n 6   1.10\n 7   1.03\n 8   1.17\n 9   1.10\n10   1.09\n# ℹ 147 more rows\n\n\nPay attention to the output: what data structures do you think the results of the first and second methods belong to?\nLet’s calculate the mean and standard deviation for this column.\n\nmean(whr$Family) # mean\n\n[1] 0.7936211\n\n\n\nsd(whr$Family) # standart deviation\n\n[1] 0.2667057",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "r_1.html#tasks-after-seminar",
    "href": "r_1.html#tasks-after-seminar",
    "title": "6  Introduction to Data Analysis in R",
    "section": "6.5 Tasks after seminar",
    "text": "6.5 Tasks after seminar\n\nRead the 2016 WHR data in the environment you are working in (in R, use the function read_csv()from the package tidyverse).\nCalculate the mean, standard deviation, median, and range (spread from maximum to minimum value) for all 6 indicators that make up the level of happiness (Economy, Family, Health, Freedom, Trust, Generosity). What can you say about them? Where is the largest range? Where are the mean and median close to each other, and where are they not so close? (To calculate the median and range in R, you will need to learn to google a little or use other materials)\nPlot a histogram and a probability density plot for all 6 indicators that make up the level of happiness.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "normdist.html",
    "href": "normdist.html",
    "title": "5  Normal distribution and standardization",
    "section": "",
    "text": "5.1 Normal distribution and its properties\nWe have already discussed the normal distribution when we talked about distributions in general [# Distributions]. This is a probability law, where given values of a feature are matched with the probability of encountering a feature with such a value, expressed by the formula\n\\(P(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} * e^{-\\frac{(x - \\mu)^2}{2(\\sigma)^2}}\\)\nIf you look closely at the formula for the normal distribution, you will notice that almost everything in it is constant, except for the variablex x and two unknown parameters:\\(\\mu\\) и \\(\\sigma\\).. These are our already familiar friends: the mean of the general population (the mathematical expectation and standard deviation of the general population ). Thus, in order to construct a distribution for any feature that is normally distributed in the general population, we only need to know two parameters of the distribution :\nIn addition to the formula itself, you can often see the following notation denoting a normal distribution: \\(\\sim \\mathcal{N}(\\mu, \\, \\sigma^2)\\)\nThe figure shows that the mean is responsible for the position of the distribution center, and the standard deviation is responsible for its “stretching” along the axis.XX, the width of the bell.\nImage from Wikipedia\nWhy do we pay so much attention to the normal distribution?\nThe normal distribution has a number of properties :\nThe latter property gives us very great opportunities when working with a normal distribution: for example, having a representative sample of height measurements, we can calculate the probability with which, for example, we will meet a woman with a height of 158 cm. Let’s recall the visualization from the first seminars https://ourworldindata.org/human-height\nTo calculate this probability, we essentially need to enter into the normal distribution formulaxxinsertx=158x=158, and asμμAndσσuse sample means and standard deviations.\nSome percentage values ​​from this distribution are worth remembering because they will appear frequently - these are the values ​​for the amount of data that lies within one or more standard deviations of the mean.\n“Three Sigma Rule” :",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Normal distribution and standardization</span>"
    ]
  },
  {
    "objectID": "normdist.html#normal-distribution-and-its-properties",
    "href": "normdist.html#normal-distribution-and-its-properties",
    "title": "5  Normal distribution and standardization",
    "section": "",
    "text": "average\nstandard deviation\n\n\n\n\n\n\n\nThe normal distribution is interesting to us at least because most of the features we study are random variables that are affected by a large number of random factors - and therefore, according to the central limit theorem , are distributed normally. As a consequence, physical, biological and psychological features (height, weight, expression of personality traits) are usually distributed normally: average features are found in nature more often than more radical ones.\nThe normal distribution is symmetrical and unimodal, and is easier to work with in some operations.\nIn order to estimate the probability of meeting a person of a certain height, we only need to know (or estimate) the mean and standard deviation of the general population - and then we calculate this probability using the normal distribution formula.\n\n\n\nsymmetrically\nunimodal (only one mode)\ndeviations from the mean obey the law of probability: we know what percentage of the data is contained in how many standard deviations from the mean\n\n\n\n\n\n\n\n\nwithin one standard deviation of the mean (\\(\\bar x ± \\sigma\\)) 68% of the values ​​are very frequent values;\nwithin two standard deviations from the mean (\\(\\bar x ± 2\\sigma\\)) contains 95% of the values ​​- the majority of the sample;\nwithin three standard deviations from the mean (\\(\\bar x ± 3\\sigma\\)) is almost 100% of the sample - that is, the entire sample.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Normal distribution and standardization</span>"
    ]
  },
  {
    "objectID": "normdist.html#standard-normal-distribution-z-distribution",
    "href": "normdist.html#standard-normal-distribution-z-distribution",
    "title": "5  Normal distribution and standardization",
    "section": "5.2 Standard Normal Distribution (z-Distribution)",
    "text": "5.2 Standard Normal Distribution (z-Distribution)\nWhat if we need to estimate which probability is higher: to meet a woman who is 158 cm tall or a man who is 174 cm tall? As we can see from the vuzilization above, these distributions are slightly different, and we cannot place these values ​​on one. Here, the standard normal distribution or z-distribution comes to our aid.\nThe standard normal distribution is a normal distribution centered at zero (\\(\\mu=0\\)) and standart distribution equal to 1 (\\(\\sigma=1\\)).\n\n\n\n\n\n\n\n\n\nThis distribution is universal and dimensionless: on the scale we no longer haveσσ, and numbers that have no dimension. This scale is called the z-scale, and the distribution itself is also called the z-distribution. Any normal distribution can be brought to the form of a standard normal - this procedure is called the z-transformation",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Normal distribution and standardization</span>"
    ]
  },
  {
    "objectID": "normdist.html#z-transformation-and-standardization",
    "href": "normdist.html#z-transformation-and-standardization",
    "title": "5  Normal distribution and standardization",
    "section": "5.3 Z-transformation and standardization",
    "text": "5.3 Z-transformation and standardization\nStandardization is the process of transforming a normal distribution to a standard dimensionless Z-score with a mean of 0 and a standard deviation of 1.\nTo achieve standardization, two things need to be done:\n\ncenter the distribution - if it has been shifted away from zero (e.g. \\(\\mu=15\\)), then we need to return it to the center \\(\\mu=0\\)\nnormalize a distribution - get rid of the difference in deviations \\(\\sigma\\), bring our distribution to the distribution with \\(\\sigma=1\\)\n\nGiven these two operations, the z-transform formula is as follows:\n\\(Z_x = \\frac{x - \\mu}{\\sigma}\\)\nThe Z-score can be placed on the Z-distribution and see what percentage of the data lies to the left of the Z-score, i.e. is less likely . This percentage can be calculated using the normal distribution formula by substituting \\(\\mu=0\\) и \\(\\sigma=1\\), but it is easier to use Z-score tables (they are easy to google), for example https://www.z-table.com/\nStandardization does not change the nature of the distribution, it only changes the position of its center and the “flattening” along the h-axis – that is, literally what we know from the properties of the mean and variance\n\n\n\n\n\n\n\n\n\nThe Z-scale can be interpreted as a scale of typicality or frequency of occurrence of values: everything that lies in +- 1 is very common, and the further from the center, the rarer the values ​​we encounter.\nThus, the Z-transform is used to:\n\nFind out how typical (frequent) the value we encounter is\nCompare the probabilities of finding values ​​from two different samples (normally distributed)\n\nFor example, we (somehow) know that the average height of women is 165 cm with a standard deviation of 7, and the average height of men is 178 and a standard deviation of 8.\nReturning to our question:\n\\(W_{158}\\): \\(Z_{158} = \\frac{x - \\mu_w}{\\sigma_w} = \\frac{158 - 165}{7} = -1\\)\n\\(M_{174}\\): \\(Z_{174} = \\frac{x - \\mu_m}{\\sigma_m} = \\frac{174- 178}{8} = -0.5\\)\nWe have converted the height of men and women into Z-scores and can now place them on the Z-distribution. It is already clear that the number -1 is located to the left of the number -0.5, which means that the probability of meeting a woman with a height of 158 cm is less than a man with a height of 174 cm. To estimate this probability accurately, we will have to use the Z-score tables https://www.z-table.com/\n\\(Z_{w_158} = -1\\), which corresponds to approximately 16% probability \\(Z_{m_174} = -0.5\\), which corresponds to approximately 30% probability\nThus, the probability of meeting a woman with a height of 158 cm is 16%, and this is 14% less than the probability of meeting a man with a height of 174 cm.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Normal distribution and standardization</span>"
    ]
  },
  {
    "objectID": "r_2.html",
    "href": "r_2.html",
    "title": "7  Data preprocessing",
    "section": "",
    "text": "7.0.1 Column selection\nAfter we have read the data into a variable (we used a function for this), we often need to preprocessread_csv() the data first . First, the data itself may not be of very good quality, and it needs to be cleaned. Second, we never work with the entire table at once - we select data, for example, a certain column, and often we do not need all the data, but only those that meet certain conditions (for example, we need to select reaction times in a group where subjects consumed caffeine, not a placebo).\nData preprocessing most often includes:\nLet’s go in order.\nAlmost anything can be done in different ways. Most often, there is no right way; if it works, it is right. But some solutions are more optimal in different contexts. Let’s look at different ways of selecting columns. Notice how they differ?\nA comment right away - when we do the assignment operation &lt;-, nothing is output to the console. To see what we assigned to the variable, you can output it by name or, if we are talking about data, look using the View() function. If you are just trying to write an operation - do not rush to assign it to a variable! This way we will immediately see the result in the console, and if it is wrong, it will be clear.\nSelection in basic R by column name\nvar1 &lt;- whr$`Happiness Score`\nvar1\n\n  [1] 7.526 7.509 7.501 7.498 7.413 7.404 7.339 7.334 7.313 7.291 7.267 7.119\n [13] 7.104 7.087 7.039 6.994 6.952 6.929 6.907 6.871 6.778 6.739 6.725 6.705\n [25] 6.701 6.650 6.596 6.573 6.545 6.488 6.481 6.478 6.474 6.379 6.379 6.375\n [37] 6.361 6.355 6.324 6.269 6.239 6.218 6.168 6.084 6.078 6.068 6.005 5.992\n [49] 5.987 5.977 5.976 5.956 5.921 5.919 5.897 5.856 5.835 5.835 5.822 5.813\n [61] 5.802 5.771 5.768 5.743 5.658 5.648 5.615 5.560 5.546 5.538 5.528 5.517\n [73] 5.510 5.488 5.458 5.440 5.401 5.389 5.314 5.303 5.291 5.279 5.245 5.196\n [85] 5.185 5.177 5.163 5.161 5.155 5.151 5.145 5.132 5.129 5.123 5.121 5.061\n [97] 5.057 5.045 5.033 4.996 4.907 4.876 4.875 4.871 4.813 4.795 4.793 4.754\n[109] 4.655 4.643 4.635 4.575 4.574 4.513 4.508 4.459 4.415 4.404 4.395 4.362\n[121] 4.360 4.356 4.324 4.276 4.272 4.252 4.236 4.219 4.217 4.201 4.193 4.156\n[133] 4.139 4.121 4.073 4.028 3.974 3.956 3.916 3.907 3.866 3.856 3.832 3.763\n[145] 3.739 3.739 3.724 3.695 3.666 3.622 3.607 3.515 3.484 3.360 3.303 3.069\n[157] 2.905\n\nstr(var1)\n\n num [1:157] 7.53 7.51 7.5 7.5 7.41 ...\nSelection in base R by index\nAn index is a number of an element in a data structure. We talked about them when we discussed multidimensional arrays: in a one-dimensional structure, for example, a vector, the index will consist of one number, in a two-dimensional structure (for example, a matrix or a data frame) the index consists of two numbers separated by a comma, in a three-dimensional structure it consists of three numbers, and so on. An index in R is always written in square brackets, for example, to find out what is in the second row of the third column, the index of the element will be [2,3]. First comes the row number, then the column number. If we want to output all the rows or all the columns, nothing is put in place of this index. For example, if I want to output all the rows from the third column, I will write[,3]\nvar2 &lt;- whr[,4]\nvar2\n\n# A tibble: 157 × 1\n   `Happiness Score`\n               &lt;dbl&gt;\n 1              7.53\n 2              7.51\n 3              7.50\n 4              7.50\n 5              7.41\n 6              7.40\n 7              7.34\n 8              7.33\n 9              7.31\n10              7.29\n# ℹ 147 more rows\n\nstr(var2)\n\ntibble [157 × 1] (S3: tbl_df/tbl/data.frame)\n $ Happiness Score: num [1:157] 7.53 7.51 7.5 7.5 7.41 ...\nFiltering by name using the tidyverse package\nFirst, a couple of important points about working with the package and the code writing culture tidyverse. The sequence of operations within one task is written line by line with a transfer to the next line in the form of a pipe %&gt;% - a symbol that allows you to use as an argument to the function of the next line what was obtained as a result of executing the previous one. On the first line, the data itself is transferred to the pipe, that is, the name of the variable to which we wrote them. Then, on each subsequent line, the result of executing the previous one will be used as the first argument of the function in brackets. More details about tidyverse https://pozdniakov.github.io/tidy_stats/110-tidyverse_basic.html and about pipes https://pozdniakov.github.io/tidy_stats/110-tidyverse_basic.html\nInternally, tidyversea data structure such as tibble( https://tibble.tidyverse.org/ ) is used. Tibble is a modified dataframe, as we already discussed when discussing data structures.\nOutput to dataframe (tibble):\nvar3 &lt;- whr %&gt;%\n  select(`Happiness Score`)\nvar3\n\n# A tibble: 157 × 1\n   `Happiness Score`\n               &lt;dbl&gt;\n 1              7.53\n 2              7.51\n 3              7.50\n 4              7.50\n 5              7.41\n 6              7.40\n 7              7.34\n 8              7.33\n 9              7.31\n10              7.29\n# ℹ 147 more rows\n\n#смотрим структуру данных\nstr(var3)\n\ntibble [157 × 1] (S3: tbl_df/tbl/data.frame)\n $ Happiness Score: num [1:157] 7.53 7.51 7.5 7.5 7.41 ...\nUsing this function, select()we can extract not just one column, but several:\nwhr %&gt;%\n  select(Country,`Happiness Score`)\n\n# A tibble: 157 × 2\n   Country     `Happiness Score`\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 Denmark                  7.53\n 2 Switzerland              7.51\n 3 Iceland                  7.50\n 4 Norway                   7.50\n 5 Finland                  7.41\n 6 Canada                   7.40\n 7 Netherlands              7.34\n 8 New Zealand              7.33\n 9 Australia                7.31\n10 Sweden                   7.29\n# ℹ 147 more rows\nFiltering by name using the tidyverse package\nNote that the previous output is also a tibble, not a vector. To output to a vector, you need to perform one more step using the function pull(), which sort of “pulls” the values ​​out of the tibble:\nvar4 &lt;- whr %&gt;%\n  select(`Happiness Score`) %&gt;%\n  pull() #вытягивает числовые значения и превращает в числовой вектор\nvar4\n\n  [1] 7.526 7.509 7.501 7.498 7.413 7.404 7.339 7.334 7.313 7.291 7.267 7.119\n [13] 7.104 7.087 7.039 6.994 6.952 6.929 6.907 6.871 6.778 6.739 6.725 6.705\n [25] 6.701 6.650 6.596 6.573 6.545 6.488 6.481 6.478 6.474 6.379 6.379 6.375\n [37] 6.361 6.355 6.324 6.269 6.239 6.218 6.168 6.084 6.078 6.068 6.005 5.992\n [49] 5.987 5.977 5.976 5.956 5.921 5.919 5.897 5.856 5.835 5.835 5.822 5.813\n [61] 5.802 5.771 5.768 5.743 5.658 5.648 5.615 5.560 5.546 5.538 5.528 5.517\n [73] 5.510 5.488 5.458 5.440 5.401 5.389 5.314 5.303 5.291 5.279 5.245 5.196\n [85] 5.185 5.177 5.163 5.161 5.155 5.151 5.145 5.132 5.129 5.123 5.121 5.061\n [97] 5.057 5.045 5.033 4.996 4.907 4.876 4.875 4.871 4.813 4.795 4.793 4.754\n[109] 4.655 4.643 4.635 4.575 4.574 4.513 4.508 4.459 4.415 4.404 4.395 4.362\n[121] 4.360 4.356 4.324 4.276 4.272 4.252 4.236 4.219 4.217 4.201 4.193 4.156\n[133] 4.139 4.121 4.073 4.028 3.974 3.956 3.916 3.907 3.866 3.856 3.832 3.763\n[145] 3.739 3.739 3.724 3.695 3.666 3.622 3.607 3.515 3.484 3.360 3.303 3.069\n[157] 2.905\n\n#смотрим структуру данных\nstr(var4)\n\n num [1:157] 7.53 7.51 7.5 7.5 7.41 ...\nIn this case, we can even calculate the median or average in the same pipe:\nwhr %&gt;%\n  select(`Happiness Score`) %&gt;% #результат -- тиббл\n  pull() %&gt;% #результат -- вектор\n  median()\n\n[1] 5.314\nOtherwise there will be a mistake\nwhr %&gt;%\n  select(`Happiness Score`) %&gt;% #результат -- тиббл\n  median()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "r_2.html#descriptive-statistics-and-aggregation",
    "href": "r_2.html#descriptive-statistics-and-aggregation",
    "title": "7  Data preprocessing",
    "section": "7.1 Descriptive Statistics and Aggregation",
    "text": "7.1 Descriptive Statistics and Aggregation\n\n7.1.1 Descriptive statistics\nWe’ve already calculated some descriptive statistics at the very beginning of working with the data, to make things more fun. Now let’s look at them in more detail.\nThe most common descriptive statistics of the central tendency measures are the mean, median, and mode. We have already constructed the mean and median, but how do we construct the mode?\n\nmean(whr_new$`Happiness Score`, na.rm = TRUE)\n\n[1] 5.321539\n\nmedian(whr_new$`Happiness Score`, na.rm = TRUE)\n\n[1] 5.285\n\nsummary(whr_new$`Happiness Score`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  2.905   4.387   5.285   5.322   6.105   7.526       5 \n\n# mode(whr_new$`Happiness Score`, na.rm = TRUE) #выдаст ошибку\n# mode(whr_new$`Happiness Score`)\n\nIt so happens that R does not have a special function for mode. There is a logical reason for this - the data we mostly work with is quantitative continuous, and we discussed that for this kind of data the concept of mode is a bit artificial. But we still need to calculate the mode sometimes. So you can either write a function to calculate it yourself, or use a ready-made function from the package. I googled what can be done, and I recommend you always do this in such situations - and chose the function mlv()(most likely value) from the packagemodeest\n\n# install.packages(\"modeest\")\nlibrary(modeest)\nmlv(whr_new$`Happiness Score`, na.rm = TRUE)\n\nWarning: argument 'method' is missing. Data are supposed to be continuous. \n            Default method 'shorth' is used\n\n\n[1] 5.239092\n\nmlv(whr_new$Region, na.rm = TRUE)\n\n[1] \"Sub-Saharan Africa\"\n\n\nOr, you can use a simple function table()that counts the number of values ​​of a specified variable.\n\ntable(whr_new$Region)\n\n\n      Australia and New Zealand      Central and Eastern Europe \n                              2                              29 \n                   Eastern Asia     Latin America and Caribbean \n                              6                              24 \nMiddle East and Northern Africa                   North America \n                             19                               2 \n              Southeastern Asia                   Southern Asia \n                              9                               7 \n             Sub-Saharan Africa                  Western Europe \n                             38                              21 \n\n\nIn addition to measures of central tendency, we can calculate measures of variability – minimum and maximum, range, standard deviation, dispersion, interquartile range.\n\nmin(whr_new$`Happiness Score`, na.rm = TRUE)\n\n[1] 2.905\n\nmax(whr_new$`Happiness Score`, na.rm = TRUE) \n\n[1] 7.526\n\nrange(whr$`Happiness Score`) #размах\n\n[1] 2.905 7.526\n\nsd(whr_new$`Happiness Score`, na.rm = TRUE) #стандартное отклонение\n\n[1] 1.108958\n\nvar(whr_new$`Happiness Score`, na.rm = TRUE) #дисперсия\n\n[1] 1.229787\n\nIQR(whr_new$`Happiness Score`, na.rm = TRUE) #межквартильный размах\n\n[1] 1.71825\n\n\nWe also calculated descriptive statistics using the function summary(). There are more advanced functions that calculate a large number of descriptive statistics at once. For example, the skim() function from the skimr package is often used - in addition to measures of central tendency and all four quartiles, it calculates the standard deviation and even builds a small histogram. summary(whr_new$ Happiness Score)\n\nsummary(whr_new$`Happiness Score`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  2.905   4.387   5.285   5.322   6.105   7.526       5 \n\n# install.packages(\"skimr\")\nlibrary(skimr)\nskim(whr_new$`Happiness Score`)\n\n\n\nData summary\n\n\n\n\nName\n\n\nwhr_new$Happiness Score\n\n\n\n\nNumber of rows\n\n\n157\n\n\n\n\nNumber of columns\n\n\n1\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n1\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\nVariable type: numeric\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\ndata\n\n\n5\n\n\n0.97\n\n\n5.32\n\n\n1.11\n\n\n2.9\n\n\n4.39\n\n\n5.28\n\n\n6.1\n\n\n7.53\n\n\n▂▆▇▇▃",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "r_2.html#assignments-after-the-seminar-2",
    "href": "r_2.html#assignments-after-the-seminar-2",
    "title": "7  Data preprocessing",
    "section": "7.2 Assignments after the seminar 2",
    "text": "7.2 Assignments after the seminar 2\n\nFollow all the examples in this section to make sure everything works correctly.\nSee which countries are in places 1-10 and 147-157 (the output should have two columns: Regionand Happiness Rank)\nCompare the average economic performance Economy (GDP per Capita)and happiness levels Happiness Scorein Central and Eastern Europe and Western Europe. What can you say about them? Where is the economic performance higher? Where are people happier?\nCompare the average economic performance Economy (GDP per Capita)and happiness level Happiness Scorein Western Europe and Southern Asia. What can you say about them? Where is the economic performance higher? Where are people happier?\n(For those working in R only) Calculate the average Happiness Scorefor all countries and print the countries that are within ± 1 of the average Happiness Score(the output should have two columns: Regionand Happiness Score) Hint: it might be easier to calculate the average separately and save it in a variable first\nCreate a column that will contain information about whether the current country Happiness Scoreis above or below the average (the column can be filled with, for example, upper and lower values ​​or any other designations)\nCalculate how many NAs are in columns Economy (GDP per Capita), Family, Health (Life Expectancy), Freedom, Trust (Government Corruption), Generosity (the task can be done in different ways, any one that gives the correct answer will do)\nCalculate descriptive statistics for the , , columns Economy (GDP per Capita): Familymean Freedom, median, mode, standard deviation, variance, and interquartile range, and (for those working in R only) calculate values ​​for quartiles 1 and 3. Answers should be different from NA (one way to find out values ​​by quartile is, for example, using separate common functions for descriptive statistics) .",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "6  Statistical inference",
    "section": "",
    "text": "6.1 The idea of ​​statistical inference\nWe finished the introduction to statistics, discussed random variables and probability, and learned how to calculate descriptive statistics: measures of central tendency, where the center of mass of our data tends to be (mean, median, mode), and measures of variability, showing how much our data varies from observation to observation (variance, standard deviation, interquartile range). We needed descriptive statistics to describe our data, to replace a big, incomprehensible table of raw data with specific values, and to make some assumptions about whether there are differences between them if we are talking about a sample from the general population, or some conclusions if our data represents the entire general population.\nIn addition to describing the data, we talked about sample estimates - parameters calculated on our sample in an attempt to estimate these parameters in the general population. As a rule, we are interested in parameters that define the distribution of a feature in a population. Recall that in psychological research, our features are most often random variables, and therefore, according to the central limit theorem, are normally distributed.\nThe ratings we received:\nLet us remember that population and sample estimates are written differently on purpose, so as not to get confused about whether we are talking about a sample or the general population:\nLet’s recall the calculation of the confidence interval:\nHow to make the right conclusion and not screw up?\nGarbage in, garbage out (GIGO).\nWe find it within the framework of frequentist (frequency) statistics - that is, we talk about frequencies and probabilities. We could say little based on the sample estimates of one study: let’s say we studied 100 married couples and calculated that the average time that parents spend with their children in Russia is 15 hours, but how can we understand whether the average in the general population is also 15, and not, for example, the same 14?\nThe idea of ​​statistical hypothesis testing (null hypothesis statistical testing) is based on the fact that we make some assumption about the general population and its parameters: the mean (mathematical expectation), standard deviation or some other parameters, and statistically test this assumption. If we select many samples and calculate their averages, what is the probability of obtaining such or more different from the population sample estimates purely by chance, if in fact this is not the case?\nLet’s return to our example about time spent with children, let’s recall the dataset about Portuguese students\nstudent\nschool\nsex\nage\naddress\nfamsize\nPstatus\nMedu\nFedu\nMjob\nFjob\nreason\nguardian\ntraveltime\nstudytime\nfailures\nschoolsup\nfamsup\npaid_mat\nactivities\nnursery\nhigher\ninternet\nromantic\nfamrel\nfreetime\ngoout\nDalc\nWalc\nhealth\nabsences_mat\nG1_mat\nG2_mat\nG3_mat\npaid_por\nabsences_por\nG1_por\nG2_por\nG3_por\nG_mat\nG_por\nansences_mat_groups\nansences_por_groups\n\n\n\n\nid1\nGP\nF\n18\nU\nGT3\nA\n4\n4\nat_home\nteacher\ncourse\nmother\n2\n2\n0\nyes\nno\nno\nno\nyes\nyes\nno\nno\n4\n3\n4\n1\n1\n3\n6\n5\n6\n6\nno\n4\n0\n11\n11\n5.666667\n7.333333\nmiddle\nless\n\n\nid2\nGP\nF\n17\nU\nGT3\nT\n1\n1\nat_home\nother\ncourse\nfather\n1\n2\n0\nno\nyes\nno\nno\nno\nyes\nyes\nno\n5\n3\n3\n1\n1\n3\n4\n5\n5\n6\nno\n2\n9\n11\n11\n5.333333\n10.333333\nless\nless\n\n\nid4\nGP\nF\n15\nU\nGT3\nT\n4\n2\nhealth\nservices\nhome\nmother\n1\n3\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nyes\n3\n2\n2\n1\n1\n5\n2\n15\n14\n15\nno\n0\n14\n14\n14\n14.666667\n14.000000\nless\nless\n\n\nid5\nGP\nF\n16\nU\nGT3\nT\n3\n3\nother\nother\nhome\nfather\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nno\nno\n4\n3\n2\n1\n2\n5\n4\n6\n10\n10\nno\n0\n11\n13\n13\n8.666667\n12.333333\nless\nless\n\n\nid6\nGP\nM\n16\nU\nLE3\nT\n4\n3\nservices\nother\nreputation\nmother\n1\n2\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nno\n5\n4\n2\n1\n2\n5\n10\n15\n15\n15\nno\n6\n12\n12\n13\n15.000000\n12.333333\nmiddle\nmiddle\n\n\nid7\nGP\nM\n16\nU\nLE3\nT\n2\n2\nother\nother\nhome\nmother\n1\n2\n0\nno\nno\nno\nno\nyes\nyes\nyes\nno\n4\n4\n4\n1\n1\n3\n0\n12\n12\n11\nno\n0\n13\n12\n13\n11.666667\n12.666667\nless\nless\n\n\nid8\nGP\nF\n17\nU\nGT3\nA\n4\n4\nother\nteacher\nhome\nmother\n2\n2\n0\nyes\nyes\nno\nno\nyes\nyes\nno\nno\n4\n1\n4\n1\n1\n1\n6\n6\n5\n6\nno\n2\n10\n13\n13\n5.666667\n12.000000\nmiddle\nless\n\n\nid9\nGP\nM\n15\nU\nLE3\nA\n3\n2\nservices\nother\nhome\nmother\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nyes\nno\n4\n2\n2\n1\n1\n1\n0\n16\n18\n19\nno\n0\n15\n16\n17\n17.666667\n16.000000\nless\nless\n\n\nid10\nGP\nM\n15\nU\nGT3\nT\n3\n4\nother\nother\nhome\nmother\n1\n2\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nno\n5\n5\n1\n1\n1\n5\n0\n14\n15\n15\nno\n0\n12\n12\n13\n14.666667\n12.333333\nless\nless\n\n\nid11\nGP\nF\n15\nU\nGT3\nT\n4\n4\nteacher\nhealth\nreputation\nmother\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nyes\nno\n3\n3\n3\n1\n2\n2\n0\n10\n8\n9\nno\n2\n14\n14\n14\n9.000000\n14.000000\nless\nless\nLet’s ask three questions:\nWe saw the question of statistical significance. Statistical significance is the probability level that our values ​​found in the sample must exceed in order for us to draw a conclusion about the general population . Why can’t we just say that 14 and 15 are obviously different numbers, which means that the values ​​are different? Because we measured these values ​​only in one sample, and the conclusions we draw are based on the probabilities of obtaining these or those values ​​or the frequencies of their occurrence (frequentist statistics!). And we know that if we repeat the study many, many times, the means in them will not be the same, according to the central limit theorem, they will be distributed normally, where the mean will be the mean of the general population (the mathematical expectation).\nIn order to answer the question of whether there are statistically significant differences, it is necessary to test the hypothesis about the differences using the NHST algorithm (null hypothesis statistical testing)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "inference.html#the-idea-of-statistical-inference",
    "href": "inference.html#the-idea-of-statistical-inference",
    "title": "6  Statistical inference",
    "section": "",
    "text": "Formulate a meaningful testable hypothesis\nSelect a representative sample\nCollect quality (good) data on it\nProcess the data according to the quality processing algorithm\n\n\n\n\n\n\n\n\n\nLet’s say we studied families with teenagers in large (GP school) and small cities (MS school) and calculated on our sample that in large cities parents spend an average of 14 hours a week with their children, and in small cities - 15 hours (from the data of the confidence interval task). 1. Is there a statistically significant difference in the time that parents spend with their children in Russia in large and small cities? 2. Is there a statistically significant difference in the frequency of alcohol consumption in families with less supportive relationships and more supportive ones? 3. Is there a statistically significant difference in the average score in mathematics for those who skip classes more or less often?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "inference.html#nhst-hypothesis-testing-and-data-analysis-algorithm",
    "href": "inference.html#nhst-hypothesis-testing-and-data-analysis-algorithm",
    "title": "6  Statistical inference",
    "section": "6.2 8.2 NHST: Hypothesis Testing and Data Analysis Algorithm",
    "text": "6.2 8.2 NHST: Hypothesis Testing and Data Analysis Algorithm\n\nWe formulate an empirical hypothesis (a hypothesis in the language of the possibilities of the study that will be conducted), identify the dependent and independent variables and the nature of the studied relationship between them. We understand what the collected data will look like.\nWe select the conditions under which we will calculate the statistical criterion – the significance level \\(\\alpha\\) (probability of a false positive finding, most often α=0.05) and statistical power \\(power = 1-\\beta\\) (the probability of detecting an effect, if there is one, is most often power=0.8).\nWe select a statistical criterion for testing the hypothesis (for example, t-test, Mann-Whitney test, ANOVA, Kruskal-Wallis test, correlation test, linear regression, etc.).\nWe formulate the null hypothesis \\(H_0\\) (about the absence and differences or connections) and an alternative hypothesis \\(H_1\\) (about the presence of differences or connections)\nBased on α, power, and the effect size of previous studies for the selected statistical criterion, we calculate the required sample size .\n\n– collecting data –\n\nWe select the data analysis environment and pre-process the data.\nFor the selected variables, we calculate descriptive statistics separately ; the variables are visualized using a histogram or a barplot or a density plot.\nWe calculate the statistical criterion : we calculate the key statistics (t-value, F-value, R, etc.) and p-value, we calculate the effect size (Cohen’s d, eta squared, etc.).\nWe draw conclusions and interpret the statistics obtained during the statistical test: we compare the obtained p-value with the selected level \\(\\alpha\\), if p-value &lt; \\(\\alpha\\) – we believe that we have enough evidence to reject the null hypothesis \\(H_0\\), and we decide on the correctness of the alternative hypothesis \\(H_1\\).\nWe draw a conclusion regarding the empirical hypothesis .\nWe visualize the data with a graph illustrating the results (scatterplot with trend line, boxplot, violet plot, etc.).\n\nLet’s look at each step.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "inference.html#empirical-hypotheses-and-variables",
    "href": "inference.html#empirical-hypotheses-and-variables",
    "title": "6  Statistical inference",
    "section": "6.3 Empirical hypotheses and variables",
    "text": "6.3 Empirical hypotheses and variables\nEmpirical hypotheses are those that we can test empirically. To formulate them correctly, we also need to decide on the type of empirical research that is possible to test our theoretical hypotheses. In psychological research, this is most often an experiment, a quasi-experiment, a correlational study (and a longitudinal study as its subtype). Sometimes there is a case study , a study of one specific case, more often in cognitive psychology and neuroscience.\nEmpirical hypotheses are formulated in a form that can be tested for our study, that is, translated into the language of the study, based on our assumed variables and their values.\nLet’s look at some examples. What methods can be used to study these issues?\nCase 1\n\nSuppose a group of scientists wants to study the effect of smoking on the development of mental disorders. What studies could be conducted to investigate this issue?\n\nCase 2\n\nLet’s assume that before the elections to the State Duma, a group of sociologists conducts a public opinion poll to identify preferences for candidates. To do this, call center operators called phone numbers selected randomly from the Moscow phone directory, according to the principle of “every tenth number.” The calls continued for a week during the call center operators’ working hours from 10 to 18. What methods are used in such research? What can be said about them, do they have any shortcomings?\n\nMethods\nAnd let’s return to our situation with the data on Portuguese students.\n\nLet’s say we studied families with teenagers in large (GP school) and small cities (MS school) and calculated on our sample that in large cities parents spend an average of 14 hours a week with their children, and in small cities - 15 hours (from the data of the confidence interval task). 1. Is there a statistically significant difference in the time that parents spend with their children in Russia in large and small cities? 2. Is there a statistically significant difference in the frequency of alcohol consumption in families with less supportive relationships and more supportive ones? 3. Is there a statistically significant difference in the average score in mathematics for those who skip classes more or less often?\n\nThe method used here is a correlational study. If there were any other manipulations in this study with different schools, it would be a quasi-experiment (since we can’t take identical families and tell some of them “spend 15 hours with the kids” and others “spend 14 hours with the kids”, we already have these groups, we don’t do the manipulations ourselves).\nEmpirical hypothesis 1 might sound like this:families in cities spend less time comparatively with families in rural areas\nEmpirical hypothesis 2:studens evaluatiin their relationship with parents as less supportive (variable famrel, less supportive coded as 1-2), drink more alcohol (variable Walc, values 4-5)\nEmpirical hypothesis 3Studens with more absences will have less grade (avarage amoung G1_mat, G2_mat, G3_mat или G1_por, G2_por, G3_por -- small values)\nLet’s first think about what these hypotheses are, what dependent and independent variables, the presence or absence of what relationship between the dependent and independent variables we want to test?\n\n6.3.1 Dependent and independent variables\nIndependent variables are everything we manipulate during the course of the study, what differences we create in order to obtain a different result. The dependent variable is the target variable, the differences in which we want to obtain by manipulating the independent variables, what we measure. When describing a study, it is always important to write down the PP and NP, along with the empirical (if the method is an experiment, then the experimental) hypothesis. The empirical hypothesis and variables are the heart of our study, the most basic of the language in which the study is described, without understanding what we want to test, nothing will work.\nWhen describing salary and wages, is it important to indicate what scale they belong to? It is important to understand this in order to later choose how to analyze the data.\nFor hypothesis 1\nWe assume that the average time parents spend with their children per week is correlated with school, meaning that the average time will vary across schools:\n\n\\(Time \\sim School\\)\n\n\nDV – average time spent by parents with children per week, quantitative continuous – relationship scale\nIV – school location (urban / rural) – categorical nominative\n\nFor hypothesis 2\n\n\\(Walc \\sim famrel\\)\n\n\nDV – Walc, the amount of alcohol consumed by students on weekends, expressed by a questionnaire with a scale from 1 to 5. DV is an ordinal scale.\nIV – famrel, an assessment of how supportive the relationships in the family are, expressed by a questionnaire with a scale from 1 to 5. IV – ordinal scale.\n\nFor hypothesis 3\n\n\\(?? \\sim ??\\)\n\n\nDV – ?\nIP – ?\n\n\n\n6.3.2 Levels in categorical variables\nLevels in an independent variable (IV) are spoken of in categorical – ordinal or nominative – variables. This is a list of possible values ​​that a categorical IV can take. For example, the conditions “often” and “rarely”, the conditions “well-off family” or “poorly-off”. In the examples above, for the hypothesis, families in cities spend less time comparatively with families in rural areas we are going to compare two groups, and here we can speak of IV levels город– urbanand `rural``.\nDepending on what conclusions we want to draw, we may need a different number of levels in the data. Compare hypotheses: studens evaluatiin their relationship with parents as less supportive (variable famrel, less supportive coded as 1-2), drink more alcohol (variable Walc, values 4-5) and Less assesment of support ( famrel), more alcohol consumption (Walc). How are they different?\nDifferences\nThe statistical method we choose will depend on what conclusion we want to make, whether we want to discover the nature of the relationship between the levels of a categorical variable (usually linear, but it can also be something else, exponential, for example) or whether it is enough for us to compare two groups with each other. We will return to this a little later.\n\n\n6.3.3 Relationship between variables\nWhat kind of connection can we conclude?\nThere are two main types of relationships between variables:\n\nAssociative or correlational\nCause and effect\n\nThree necessary conditions for establishing a cause-and-effect relationship :\n\nThe change in IV occurred before we observed a change in the DV\nChanges in the IV have an associative connection with changes in the DV\nThere are no alternative explanations for the changes in the DV, other than the changes in the IV\n\nAt the moment, we only have survey data, in which students answered questions simultaneously. It turns out that in such a research design, we do not go through the necessary conditions for establishing a cause-and-effect relationship, therefore, we cannot draw a conclusion about a cause-and-effect relationship. We can establish a cause-and-effect relationship only in the course of an experiment or quasi-experiment (it differs from an experiment in that the subjects are not randomly distributed into groups, but groups that already exist in the population are used, for example, different countries). In all other types of research, especially when the relationship of variables in one self-report or questionnaire is studied, we can only talk about an associative or correlational relationship.\nIt often happens that it is quite possible that bad relationships in the family lead to alcoholism in children. But in our sample we only have such data - from the questionnaire, and from the data of that study that we have, we can only judge the presence or absence of a correlation.\nWhen formulating conclusions, these conclusions may differ as follows, compare: - Conclusion 1: “Less supportive family relationships cause alcoholism.” - Conclusion 2: “Less supportive family relationships are associated with a high risk of alcoholism.”\nIn order to test whether there are statistically significant differences between these groups, it is necessary to formulate a null and alternative hypotheses.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "inference.html#null-and-alternative-statistical-hypotheses",
    "href": "inference.html#null-and-alternative-statistical-hypotheses",
    "title": "6  Statistical inference",
    "section": "6.4 Null and Alternative Statistical Hypotheses",
    "text": "6.4 Null and Alternative Statistical Hypotheses\nIn order to conduct a statistical test, it is necessary to define statistical hypotheses - artificially introduced testable statements regarding the general population. They are introduced precisely in order to be able to somehow calculate the data and statistically draw a conclusion based on them about the general population for our small sample. There are two statistical hypotheses, they are mutually opposite: the null (main) and alternative hypotheses.\nNull (main) hypothesis \\(H_0\\) – this is always a hypothesis about the absence of differences in the general population. The hypothesis about the absence of differences between groups (if the hypothesis implies a comparison of groups) or about the absence of a relationship between variables (if the hypothesis is about the relationship of quantitative continuous variables). We try to refute it in statistical testing (yes, exactly refute, not confirm – the conclusions that we can make regarding the null hypothesis are only to reject or not to reject, we cannot confirm it and accept it).\nAlternative hypothesis \\(H_1\\)- a hypothesis opposite toH0H0, that is, the hypothesis about the presence of differences in the general population. \\(P(H_0) + P(H_1) = 1\\)\nWhat about \\(H_0\\) и \\(H_1\\) for our hypotheses?\nFor hypothesis 1\n\n\\(Time \\sim School\\)\n\nFor hypothesis 2\n\n\\(Walc \\sim famrel\\)\n\n\nDV – Walc, the amount of alcohol consumed by students on weekends, expressed by a questionnaire with a scale from 1 to 5. DV is an ordinal scale.\nIV – famrel, an assessment of how supportive the relationships in the family are, expressed by a questionnaire with a scale from 1 to 5. IV – ordinal scale.\n\nFor hypothesis 3\n\n\\(?? \\sim ??\\)\n\n\nDV – ?\nIV – ?\n\n\n6.4.1 Directional and non-directional hypotheses\nAt this stage, it is important for us how the empirical hypothesis is formulated: towards an increase/decrease in the value of the salary in one of the groups or, in general, that the values ​​of the salary in the groups differ in a non-directional manner?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "inference.html#significance-level-and-statistical-power",
    "href": "inference.html#significance-level-and-statistical-power",
    "title": "6  Statistical inference",
    "section": "6.5 Significance level and statistical power",
    "text": "6.5 Significance level and statistical power\nOnce we have sorted out the hypotheses, empirical and statistical, we need to set the criteria on the basis of which we will make decisions about the conclusions we draw from our sample to the general population.\nLevel of significance \\(\\alpha\\) and statistical \\(power = 1-\\beta\\) – these are two of the most important parameters in hypothesis testing. These concepts set the probability framework in which we will conduct the test. The first framework is the probability of obtaining a significant result (significant differences between groups or a relationship between variables) if it is not actually in the general population – a false positive, aka a type I error \\(\\alpha\\).\n\n\n\n\n\n\n\n\n\n\n\nThe second frame is the probability of obtaining an insignificant result, if it actually exists in the general population - a false negative result, also known as a type II error \\(\\beta\\).\n\n\n\n\n\n\n\n\n\n\n\nLevel of significance \\(\\alpha\\), it is also a type I error - to make a false positive conclusion, that is, when there is no effect in the general population, but we conclude that it is there - we set it ourselves (!). In psychology, there is a conventional agreement to consider the level of significance \\(\\alpha = 0.05\\) That is, we simply agreed to consider the 5% probability of making a false positive conclusion as a compromise between being able to make any conclusions at all about the general population, and the fact that 5% of our studies will contain incorrect conclusions (in fact, much more, we will talk about this later).\nBut this is an agreement, and a controversial one at that! In elementary particle physics \\(\\alpha = 0.000003\\)!\nArticle “Justify your alpha” in Nature Human Behaviour https://www.nature.com/articles/s41562-018-0311-x\nThe probability that in the population there actually is a difference between groups or a relationship, but we were unable to detect it in our data \\(\\beta\\), it is also a type II error - this is a type II error, the probability of making a false negative conclusion. And if a type I error \\(\\alpha\\) explicitly appears in hypothesis testing - this is the value with which we compare our resulting p-value, then \\(\\beta\\) does not participate in this testing. That is, we can easily get a false negative conclusion, the absence of results, although in fact they are. Therefore, we need to enter \\(\\beta\\) in testing the hypothesis and minimize such probability. For this purpose, the concept of statistical power of the test was introduced \\(power = 1 - \\beta\\).\nThe level of statistical power is a positive metric, the level of probability at which we are guaranteed that if differences between groups or a relationship between variables exist in the general population, we will be able to find it in our data using our statistical test. If you look at a table of type I and type II errors, statistical power is the inverse of the probability of a type II error, not finding a significant effect if it exists. In psychology, it has become conventional that statistical power is most often taken at the level \\(power = 1 - \\beta\\). Another agreement!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "inference.html#selection-of-statistical-criterion",
    "href": "inference.html#selection-of-statistical-criterion",
    "title": "6  Statistical inference",
    "section": "6.6 Selection of statistical criterion",
    "text": "6.6 Selection of statistical criterion\nHow do you know which test to choose? This is probably one of the most difficult questions in statistics. It is influenced by a large number of nuances, which we will consider in the next section",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "inference.html#sample-size-and-effect-size",
    "href": "inference.html#sample-size-and-effect-size",
    "title": "6  Statistical inference",
    "section": "6.7 Sample size and effect size",
    "text": "6.7 Sample size and effect size\nCalculating the sample size is a critical step because it is responsible for leveling out the type II error and making a false negative conclusion. It may well turn out that due to the small sample, we simply were unable to record differences that actually exist in the general population!\nThis point was often overlooked before: it seemed that the sample size did not require a specific calculation, and it was enough to rely on previous research. It turned out that this is not the case, we will talk about the consequences of this approach in more detail below.\nTo calculate the required sample, we need to know the approximate size of the effect that we can detect, the level of significance \\(\\alpha\\), and statistical power \\(power = 1 - \\beta\\).\nThe effect size is the magnitude of the observed differences. The degree of difference is determined not by the statistics obtained after the statistical criterion is applied (e.g., t-value, F-value), and not by the p-value, but by a separate metric . This metric is calculated using formulas individually for each statistical test. For example, for a t-test, the effect size is \\(Cohen's \\ d\\) or its normalized version \\(Hedges`\\  g\\) (used less frequently in practice). The only exception is the correlation coefficient \\(r\\) – it will be both a statistic and an effect size. For a linear regression with one predictor (factor, also an independent variable), the effect size can also be \\(R^2\\), and for multiple, when there are many predictors (factors or NP), it is better to use the metric \\(Cohen’s \\ f^2\\) or \\(partial \\ \\eta^2 2\\). In psychological research, unless it involves psychophysiology, the effect size is rarely large - usually in the small to medium range. And the smaller the expected effect size, the more observations we need to collect to be able to draw an accurate conclusion about the presence or absence of differences in the general population!\nThere is a good visualization of the effect size for Cohen’s d t-test https://rpsychologist.com/d3/cohend/\nEffect size is typically involved in hypothesis testing at two stages of this algorithm :\n\nTo calculate the sample size at the planning stage of the study - in this case, the effect size from similar studies already conducted is used in order to roughly estimate the possibility of catching the population effect in our sample and collecting the required number (step 5 of this algorithm).\nWhen interpreting a statistical test performed on data. The effect size is one of the key numbers for understanding the results of statistical tests (step 10 of this algorithm)\n\nIf the articles you rely on when planning your study do not indicate the effect size, you can calculate it yourself based on the data provided in the articles: the sample size in the study, the adopted level of statistical power (if not specified, then it is usually taken as 0.8) and the selected level of statistical significance \\(\\alpha\\) (if not specified, then it is usually taken as 0.5).\nTable with effect size metrics and interpretation of their magnitudes on the Cambridge University website and https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize\n\n\n\n\n\n\n\n\n\n\nStat criterion\nEffect size metric\nSmall effect\nAverage effect\nStrong effect\n\n\n\n\nt-test\nCohen′s d,Hedges′ gCohen′s d,Hedges′ g\n0.2\n0.5\n0.8\n\n\nANOVA\nη2,ω2η2,ω2\n0.01\n0.06\n0.14\n\n\nANOVA\nCohen′ fCohen′ f\n0.1\n0.25\n0.4\n\n\nlinear regression (one factor)\nCohen′ fCohen′ f\n0.1\n0.25\n0.4\n\n\nlinear regression (multiple factors)\npartial η2partial η2\n0.02\n0.13\n0.26\n\n\nlinear regression (multiple factors)\nCohen′ fCohen′ f\n0.14\n0.39\n0.59\n\n\ncorrelation test\nrxyrxy\n0.1\n0.3\n0.7\n\n\n\n“Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs” https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full\nEffect size, significance level \\(\\alpha\\), statistical power and sample size are related parameters. Knowing 3 of them, you can always calculate the fourth! https://rpsychologist.com/d3/nhst/\nThis can be done in G*Power https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower or in R, for example, using the package pwr https://cran.r-project.org/web/packages/pwr/pwr.pdf .\nLet’s say we decided to test the hypothesis that students whose parents received a master’s degree do better than those whose parents graduated from college. We chose the level of statistical significance \\(\\alpha = 0.05\\), with which we will compare the p-values ​​that we obtain. We chose the level of statistical \\(power = 0.8\\) (that is, a type II error \\(\\beta = 0.2\\)). We chose to test this hypothesis using a t-test. From a previous similar study, we learned that the effect size in a similar study was Cohen’s d = 0.37. And now we can estimate how much data we need to collect so that if there was a difference between such students in the general population, we could detect it in our data. These 4 values ​​are mathematically related, so knowing any 3, we can calculate the fourth.\n\n\n\n\n\n\n\n\n\nUnderpowered studies are a huge problem for many sciences. “Power failure: why small sample size undermines the reliability of neuroscience” https://www.nature.com/articles/nrn3475\n\nlibrary(pwr)\npwr::pwr.t.test(d=0.37,power=0.8,sig.level=0.05,type=\"two.sample\", alternative=\"greater\")\n\n\n     Two-sample t test power calculation \n\n              n = 91.00624\n              d = 0.37\n      sig.level = 0.05\n          power = 0.8\n    alternative = greater\n\nNOTE: n is number in *each* group\n\n\nSimilarly, calculating the sample size to test the hypothesis that parental education and preparatory course completion are somehow related to student achievement was done by fitting a linear model.\n\nlibrary(WebPower)\nWebPower::wp.regression(n = NULL, p1 = 2, f2 = 0.24, alpha = 0.05, power = 0.8)\n\nPower for multiple regression\n\n           n p1 p2   f2 alpha power\n    43.28562  2  0 0.24  0.05   0.8\n\nURL: http://psychstat.org/regression\n\n\nThe package has a web application https://webpower.psychstat.org/models/reg01/\nAn important point about statistical power: it is precisely this that shows how often the errors will occur \\(p\\)-values &lt; \\(\\alpha\\) if the alternative hypothesis is true (that there are differences or a connection!)\n\n\n\n\n\n\n\n\n\nTo get a detailed understanding of the nuances of effect size and sampling, you can take Lakens’ course https://www.coursera.org/learn/statistical-inferences (it seems that now only with a VPN)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "inference.html#calculation-of-the-static-criterion-and-significance-testing",
    "href": "inference.html#calculation-of-the-static-criterion-and-significance-testing",
    "title": "6  Statistical inference",
    "section": "6.8 Calculation of the static criterion and significance testing",
    "text": "6.8 Calculation of the static criterion and significance testing\nLet’s return to our questions again.\n\nLet’s say we studied families with teenagers in large (GP school) and small cities (MS school) and calculated on our sample that in large cities parents spend an average of 14 hours a week with their children, and in small cities - 15 hours (from the data of the confidence interval task). 1. Is there a statistically significant difference in the time that parents spend with their children in large and small cities? 2. Is there a statistically significant difference in the frequency of alcohol consumption in families with less supportive relationships and more supportive ones? 3. Is there a statistically significant difference in the average score in mathematics for those who skip classes more or less often?\n\nUsing the first question as an example, we will analyze the process of testing the null hypothesis, how any statistical criterion works.\n\\(\\mu\\) = 14, M = 15, n = 100, sd = 4.5\nLet us formulate the null and alternative hypotheses. The null hypothesis is always a hypothesis of no differences, i.e. that the average time parents spend with their children per week does not differ in large and small cities.\n\n\\(H_0\\): \\(\\mu_{urban} = \\mu_{rural}\\)\n\n\n\\(H_1\\): \\(\\mu_{urban} \\neq \\mu_{rural}\\)\n\nAnother important point is to choose the significance level, the boundary value, upon crossing which we will make a decision that we can reject the null hypothesis. Usually\\(\\alpha = 0.05\\)\nNext we begin to test our null assumption, we will try to disprove the null hypothesis.\nSo, we assume that the null hypothesis is true. Further, if we assume that it is true, then \\(\\mu_{urban}\\) и \\(\\mu_{rural}\\) will be at one point. Since we are talking about two averages of different samples, we need a distribution of sample averages on which we can place \\(\\mu_{urban}\\) and \\(\\mu_{rural}\\).\n\n\n\n\n\n\n\n\n\nNext, we need to somehow estimate how much \\(\\mu_{urban}\\) far from from \\(\\mu_{rural}\\). What do we always do when we need to compare which of the values ​​taken from different samples is greater or less than the other? We switch to the standard normal distribution - the Z-score distribution! \\(Z = \\frac{X - \\bar X}{sd}\\)\nMoreover, the standard deviation of this distribution will be the standard error of the mean– the value of the standard deviation for the distribution of sample means.\nLet’s take as the middle of the distribution \\(\\mu_{urban}\\) (arbitrary, we can take any of the averages). To translate \\(\\mu_{rural}\\) in Z-score, first we calculate the standard error of the mean \\(\\mathrm{se} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{sd}{\\sqrt{n}}\\). \\(se = \\frac{4.5}{\\sqrt{10}} = 0.45\\) (we have already calculated it to calculate the confidence interval - the mathematics of these processes are almost identical!)\n\\(Z_{Mrural} = \\frac{\\mu_{rural} - \\mu_{urban}}{se}\\).\n\\(Z_{Mrural} = \\frac{15-14}{0.45} = 2.2\\)\nNow we can arrange \\(\\mu_{rural}\\) on the Z-distribution with the mean at \\(\\mu_{urban}\\) and see how far apart they are.\n\n\n\n\n\n\n\n\n\nWe know that the number 2 in the Z-score distribution corresponds to approximately 96% of the data (we know the data distribution for any normal distribution). And remember that we constructed this distribution based on the assumption that the null hypothesis is equal, that \\(\\mu_{urban} = \\mu_{rural}\\) How then can we interpret the obtained estimates?\nIt turned out that the probability of getting the value \\(\\mu_{rural} = 15\\) or even further from \\(\\mu_{urban}= 14\\) if the assumption is true that \\(\\mu_{urban} = \\mu_{rural}\\) составляет \\(P(M_{rural} | \\mu_{urban} = \\mu_{rural}) = 1 - 0.96 = 0.04\\).This is our p-value , the target statistic we are trying to obtain.\nP-value is the probability (area of ​​the graph under the curve) of obtaining such and even more radical differences (going into the tails) under the condition that the null hypothesis is true (that in fact there are no differences in the general population, and we obtain this result by chance).\nThere is a lot of confusion in the interpretation of p-value, to unravel the confusion you can read these articles:\n\nArticle “A Dirty Dozen: Twelve P-Value Misconceptions” https://sixsigmadsi.com/wp-content/uploads/2020/10/A-Dirty-Dozen-Twelve-P-Value-Misconceptions.pdf\nMisuse of p-values ​​- Misuse of p- values ​​- https://ru.abcdef.wiki/wiki/Misuse_of_p-values ​​https://ru.abcdef.wiki/wiki/Misuse_of_p-values\n\nNext, after we have obtained the p-value, we compare this value with our chosen level \\(\\alpha\\)– the probability that we can accept of making a false positive conclusion, of making a type I error.\n\nIf \\(p \\ value &lt; \\alpha\\):\n\nthe location of our means looks atypical for the null hypothesis - we assumed the means would be at one point, but they moved apart quite a bit\nwe believe that the probability of obtaining such or even stronger differences between the means, given the validity of the null hypothesis, is statistically significantly small\ntherefore we can reject the null hypothesis of no difference\nwe accept the alternative hypothesis that there are differences\n\nIf \\(p \\ value \\ge \\alpha\\):\n\nthe location of our means looks typical for the null hypothesis - the means are not spread out enough, so our data looks like the null hypothesis is true\nwe cannot reject the null hypothesis\n\n\nThe p-value is related, we have already discussed this above in sample size diiscussioin:\n\nSample size: the larger the sample size, the greater the statistical power and the more often and confidently p-values &lt; 0.05 will appear\nVariance: the smaller the variance and the more homogeneous the data, the lower the p-value will be\n\nIn our case: \\(p-value = 0.04 &lt; \\alpha = 0.05\\).\nIn this case, since the p-value is less than alpha, we can say that the probability of obtaining such or even stronger differences between the means is sufficiently small, provided that the null hypothesis is true, so we can reject the null hypothesis of no differences and accept the alternative hypothesis that there are differences between the means. That is, we have tested the statistical hypothesis and concluded that in the general population \\(\\mu_{urban}\\) и \\(\\mu_{rural}\\) are different!\nWhat did we do just now?\n\nWe put forward a null hypothesis about the absence of differences between the groups, which we will reject - that is, that the means of the two groups are at the same point\nFormulated an opposite alternative hypothesis\nWe have stated that we will test the null hypothesis at the significance level \\(\\alpha = 0.05\\)\nPlaced the first mean on the distribution of sample means in the middle and agreed to estimate the actual (in the data) location of the second mean from it\nTo estimate how far the second mean was from the first in our data, we converted everything into Z-scores and calculated the Z-score for the second mean.\nBased on the calculated Z-score, we placed the second mean on the distribution of sample means and calculated what percentage of the data lies beyond this value (p-value)\nComparedp-value \\(p-value\\) with \\(\\alpha\\): it turned out to be less than 0.05, so we rejected the null hypothesis of no differences and concluded that the means in the general population differ.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  }
]