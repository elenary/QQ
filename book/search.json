[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Qualitative and Quantitative Research Methods in Psychology",
    "section": "",
    "text": "1 Sample\nWe looked at two examples - about the effect of caffeine on attention and the effect of a communication work tool on productivity. In these examples, we assumed that we conducted a study - that is, we recruited a certain number of people and drew a conclusion about how it works in general, not only for these people, but for people in general. Why does this happen? Why can we extend these conclusions to all people who can theoretically drink coffee and use this communication tool? And why do we need this at all?\nImage https://towardsdatascience.com/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sample</span>"
    ]
  },
  {
    "objectID": "index.html#sample-estimates",
    "href": "index.html#sample-estimates",
    "title": "Qualitative and Quantitative Research Methods in Psychology",
    "section": "1.1 Sample estimates",
    "text": "1.1 Sample estimates\nHow can we make a generalization about all people? What do we do if we want to know the average height of people on the entire planet? Do we measure every person out of the nearly 8 billion on the planet and take the arithmetic mean?\n\n\n\n\n\n\n\n\n\n\nhttps://ourworldindata.org/human-height\n\nThe general population is the set (or totality) of all objects under study within the framework of a specific research question.\nA sample or sample population is a portion of a general population selected to study a specific research question.\nWhen we talk about a general population, we talk about its parameters , for example, the average height of people, we denote it by X. We cannot calculate it directly, so we have to resort to sample estimates , which are often denoted by the same letter as the parameter of the general population, only “with a cap” or “with a line” X̅ is the sample mean, the average height of people in a specific sample.\nSo, when we want to make a conclusion about all people from the general population, we conduct a study on a small sample (depending on the research method, the required sample size can be from 10 to 1000 people) and estimate the studied parameter of the general population based on sample estimates.\nTo avoid confusion about what we are talking about, key population and sample estimates are referred to differently:\n\n\n\n\n\n\n\n\n\nGeneral population\nSample\n\n\n\n\nMean (mathematical expectation)\n\\(\\mu\\)\nM, \\(\\overline X\\)\n\n\nStandard deviation\n\\(\\sigma\\)\ns, sd",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sample</span>"
    ]
  },
  {
    "objectID": "index.html#representativeness-of-the-sample",
    "href": "index.html#representativeness-of-the-sample",
    "title": "Qualitative and Quantitative Research Methods in Psychology",
    "section": "1.2 Representativeness of the sample",
    "text": "1.2 Representativeness of the sample\nAn important concept is the representativeness of the sample – the ability of the sample to reflect the studied parameter of the general population.\nThe same sample may be representative for one research question and not representative for another!\nRepresentativeness is achieved:\n\nBy random selection of people (we don’t take specific ones that we like more, but randomly)\nSample size (it has been proven that with a large sample size it is sufficiently representative of the general population to provide an answer about the parameter being studied)\n\nMethods for forming representative samples\n\nSimple random sample\nSystematic sample\nStratified sample\nCluster sample\n\nA simple random sample is drawn from a large population at random - all elements of the population have an equal chance of being included in the sample.\nSystematic sampling imitates random selection and is commonly used by sociologists in field research when a certain step in selecting people is set: for example, every 5th or 10th person.\nStratified sampling is needed if the feature we are studying can be affected by some parameter that is distributed unevenly in the general population: for example, we are studying the level of life satisfaction, but we know that it is affected by the level of income - and we need to ensure an equivalent distribution of income levels in the sample. Then, from these income strata, subjects are selected randomly.\nCluster sampling includes entire individual clusters, without random selection within them: for example, we are studying schoolchildren, and instead of selecting them randomly, we include several specific schools (clusters) in the sample.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sample</span>"
    ]
  },
  {
    "objectID": "index.html#dependent_samples",
    "href": "index.html#dependent_samples",
    "title": "Qualitative and Quantitative Research Methods in Psychology",
    "section": "1.3 Dependent and independent samples",
    "text": "1.3 Dependent and independent samples\nAnother important concept regarding samples is that they can be dependent or independent.\nIndependent samples are used when we compare groups of observations that belong to different people (schools/institutes/etc.). For example, in our example of the effect of caffeine on attention, we can recruit a group of people who will drink 3 cups of coffee a day, and another group who will drink decaffeinated coffee. These are different people, not related to each other in any way.\nBut if we took the same group of people and measured their attention after 3 cups of coffee in one week, and then tested the effects of a decaffeinated drink on these same people , these would be dependent samples , since these are still the same people.\n\n\n\n\n\n\n\n\n\n\n\nCase\n\n\nResearcher Nikita studied the relationship between age, teaching experience and professional burnout of university teachers. For this, he selected several universities: Moscow State University, the Higher School of Economics and RANEPA. Nikita found 10 teachers from the psychology departments of each university on the VKontakte social network and asked them to fill out the Maslach Burnout Inventory (MBI). Based on the processed data, Nikita made the following conclusions: 1) in Russia, younger teachers burn out more often; 2) teachers with less teaching experience burn out more often; 3) MSU teachers generally burn out less than HSE teachers. What sampling methods did Nikita use? Are the conclusions correct?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sample</span>"
    ]
  },
  {
    "objectID": "scales.html",
    "href": "scales.html",
    "title": "2  Data and types of scales",
    "section": "",
    "text": "2.1 Measurement and variable\nWhat does it mean to measure something? It means to bring some value on a scale into conformity with the studied feature .\nWhat could be an example of a trait? Anything that we need to measure in a study: number of cups of coffee per day, concentration level, number of errors, reaction time, degree of burnout, number of tasks completed, level of neuroticism, student rating, number of children in the family, temperature, etc.\nThe feature we are studying is also called a variable . We will encounter this concept constantly in data analysis. In fact, if we look at the observation table, any column with measurements is a variable. The rows contain observations, for example, each new person from our sample. The value in a certain column is the value of the variable of this observation.\nLet’s return to the case with Nikita, who studies burnout of university employees. Let’s make a table of data that Nikita could measure.\nteacher_number &lt;- seq(1,30,1)\nage &lt;-  sample(22:60, size = 30, replace = T)\nexp_years &lt;- sample(1:8, size = 30, replace = T)\nexp_scaled &lt;- ifelse(exp_years &gt;= 1 & exp_years &lt;= 2, \"от 1 до 2\",\n                     ifelse(exp_years &gt; 2 & exp_years &lt;= 5, \"от 3 до 5\",\n                            ifelse(exp_years &gt; 5, \"больше 5\", exp_years)))\nburnout_MBI &lt;- sample(19:70, size = 30, replace = T)\nuniver &lt;-  rep(c(\"MSU\", \"HSE\", \"MSU\", \"RANEPA\", \"HSE\", \"RANEPA\"),5)\nburnout &lt;- tibble(teacher_number, age, exp_years, exp_scaled, burnout_MBI, univer)\nkable(burnout[1:10,])\n\n\n\n\nteacher_number\nage\nexp_years\nexp_scaled\nburnout_MBI\nuniver\n\n\n\n\n1\n28\n2\nот 1 до 2\n60\nMSU\n\n\n2\n38\n6\nбольше 5\n45\nHSE\n\n\n3\n47\n8\nбольше 5\n68\nMSU\n\n\n4\n51\n7\nбольше 5\n54\nRANEPA\n\n\n5\n25\n4\nот 3 до 5\n22\nHSE\n\n\n6\n25\n6\nбольше 5\n61\nRANEPA\n\n\n7\n58\n1\nот 1 до 2\n64\nMSU\n\n\n8\n35\n2\nот 1 до 2\n22\nHSE\n\n\n9\n53\n6\nбольше 5\n33\nMSU\n\n\n10\n26\n1\nот 1 до 2\n60\nRANEPA\nWhat will be the variables here?\nIn the definition of measurement, in addition to the feature, there is a second important concept - the scale.\nA scale is a system of measurement. So that we can all use the same units of measurement and not go crazy, we, the people of planet Earth, use uniform scales. There are 4 of them, and they are metric and non-metric - that is, whether we can attach a measuring ruler to them or not. Here, a measuring ruler is any conventional device that has a division value (centimeter, gram, second, piece).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and types of scales</span>"
    ]
  },
  {
    "objectID": "scales.html#measurement-and-variable",
    "href": "scales.html#measurement-and-variable",
    "title": "2  self-contained: true",
    "section": "3.1 Measurement and variable",
    "text": "3.1 Measurement and variable\nWhat does it mean to measure something? It means to bring some value on a scale into conformity with the studied feature .\nWhat could be an example of a trait? Anything that we need to measure in a study: number of cups of coffee per day, concentration level, number of errors, reaction time, degree of burnout, number of tasks completed, level of neuroticism, student rating, number of children in the family, temperature, etc.\nThe feature we are studying is also called a variable . We will encounter this concept constantly in data analysis. In fact, if we look at the observation table, any column with measurements is a variable. The rows contain observations, for example, each new person from our sample. The value in a certain column is the value of the variable of this observation.\nLet’s return to the case with Nikita, who studies burnout of university employees. Let’s make a table of data that Nikita could measure.\n\nteacher_number &lt;- seq(1,30,1)\nage &lt;-  sample(22:60, size = 30, replace = T)\nexp_years &lt;- sample(1:8, size = 30, replace = T)\nexp_scaled &lt;- ifelse(exp_years &gt;= 1 & exp_years &lt;= 2, \"от 1 до 2\",\n                     ifelse(exp_years &gt; 2 & exp_years &lt;= 5, \"от 3 до 5\",\n                            ifelse(exp_years &gt; 5, \"больше 5\", exp_years)))\nburnout_MBI &lt;- sample(19:70, size = 30, replace = T)\nuniver &lt;-  rep(c(\"MSU\", \"HSE\", \"MSU\", \"RANEPA\", \"HSE\", \"RANEPA\"),5)\nburnout &lt;- tibble(teacher_number, age, exp_years, exp_scaled, burnout_MBI, univer)\nkable(burnout[1:10,])\n\n\n\n\nteacher_number\nage\nexp_years\nexp_scaled\nburnout_MBI\nuniver\n\n\n\n\n1\n48\n1\nот 1 до 2\n25\nMSU\n\n\n2\n39\n3\nот 3 до 5\n52\nHSE\n\n\n3\n45\n4\nот 3 до 5\n30\nMSU\n\n\n4\n33\n2\nот 1 до 2\n33\nRANEPA\n\n\n5\n50\n6\nбольше 5\n22\nHSE\n\n\n6\n56\n8\nбольше 5\n26\nRANEPA\n\n\n7\n44\n1\nот 1 до 2\n54\nMSU\n\n\n8\n48\n6\nбольше 5\n58\nHSE\n\n\n9\n28\n2\nот 1 до 2\n39\nMSU\n\n\n10\n38\n6\nбольше 5\n20\nRANEPA\n\n\n\n\n\n\n\nWhat will be the variables here?\nIn the definition of measurement, in addition to the feature, there is a second important concept - the scale.\nA scale is a system of measurement. So that we can all use the same units of measurement and not go crazy, we, the people of planet Earth, use uniform scales. There are 4 of them, and they are metric and non-metric - that is, whether we can attach a measuring ruler to them or not. Here, a measuring ruler is any conventional device that has a division value (centimeter, gram, second, piece).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>self-contained: true</span>"
    ]
  },
  {
    "objectID": "scales.html#quantitative-and-non-quantitative-data",
    "href": "scales.html#quantitative-and-non-quantitative-data",
    "title": "2  Data and types of scales",
    "section": "2.2 3.2 Quantitative and non-quantitative data",
    "text": "2.2 3.2 Quantitative and non-quantitative data\nData measured by metric scales are quantitative data (e.g. height, weight, number of cases, temperature). What cannot be measured by metric scales (e.g. eye color, well-being, level of neuroticism, level of education) are non-quantitative data, which can have different names: categorical , sometimes qualitative .\nSometimes data is called qualitative as opposed to quantitative, but this is not entirely correct: the distinction between quantitative and qualitative is usually applied to types of research, where qualitative research is, for example, interviews or analysis of blocks of text. But as a result of this analysis, we may well end up with quantitative variables, for example, the number of times a particular word was used, so I recommend not using the word “qualitative” in relation to data, rather than types of research.\nWith quantitative data it’s simpler – this is everything that can be measured with a metric scale, a conventional ruler.\nAmong non-quantitative data, there are two types: categorical and rank (ordinal)\nWe will examine this data in detail on the scales.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and types of scales</span>"
    ]
  },
  {
    "objectID": "scales.html#types-of-scales",
    "href": "scales.html#types-of-scales",
    "title": "2  Data and types of scales",
    "section": "2.3 3.3 Types of scales",
    "text": "2.3 3.3 Types of scales\nAs we have already understood, different data refer to different scales.\nDifferent scales have different measuring power – the precision with which we measure a feature. The same feature can be measured with different precision: for example, depending on the research question, height can be expressed quantitatively in centimeters on the interval {0; ∞}, or it can be coded as {“less than 150 cm”; 150 cm and more} if we are only interested in prevalence over a certain feature.\nThere are 4 scales in total, if you arrange them from bottom to top according to measuring power: names, ordinal, interval, ratios.\n\n\n\n\n\n\n\n\n\nScale\nDescription\nPossible operations\nExamples\n\n\n\n\nRation\nQuantitative, there is absolute zero, you can calculate how much more or less, and how many times\n=, ≠≠, &gt;, &lt;, +, -, ×, ÷\nHeight, weight, number of cases\n\n\nInterval\nQuantitative, but there is no absolute zero, you can calculate how much more or less, but you can’t calculate how many times\n=,≠≠, &gt;, &lt;, +, -\nTemperature in degrees Celsius, timekeeping according to different calendars\n\n\nOrdinal (rank)\nCategorical (qualitative), you can set “more” or “less”, but you can’t calculate quantitatively how much more or less\n=, ≠≠, &gt;, &lt;\nEducation level, neuroticism level, sports rating\n\n\nNominal\nCategorical (qualitative), cannot be set to “more” or “less”\n=,≠≠\nGender, color, place of residence, name of university",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and types of scales</span>"
    ]
  },
  {
    "objectID": "scales.html#continuous-and-discrete-data",
    "href": "scales.html#continuous-and-discrete-data",
    "title": "2  Data and types of scales",
    "section": "2.4 3.4 Continuous and discrete data",
    "text": "2.4 3.4 Continuous and discrete data\nWe see that our richest possibilities for measurement lie in quantitative scales – on the ratio scale (it is the steepest) and the interval scale (it is worse and, in fact, is rarely encountered in research in our field).\nQuantitative data can be discrete, when the variable takes strictly defined values, and continuous, when it can take any values, to infinity or over a given interval.\nFor example, in our example with the burnout study, the variable age can take any value: teachers can be 25 years old, 27.5 years old, or 31,666.. years old - these are all values ​​from the range of acceptable values ​​for this variable . But if we consider the number of people infected with coronavirus, there is no way it can be 27.5 or 31,666.. - the number of people infected is not expressed as fractional shares of one person.\nAn important concept here is the range of admissible values . For a continuous variable, this is always an interval, for example {0;+∞}, for discrete variables, it is strictly defined values, which, nevertheless, can also tend to infinity, for example, {0;1;2;3;4;5;6;7;8…}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and types of scales</span>"
    ]
  },
  {
    "objectID": "scales.html#how-can-i-determine-what-scale-the-data-is-on",
    "href": "scales.html#how-can-i-determine-what-scale-the-data-is-on",
    "title": "2  Data and types of scales",
    "section": "2.5 3.5 How can I determine what scale the data is on?",
    "text": "2.5 3.5 How can I determine what scale the data is on?\nThis is quite a difficult question, and at first it is very difficult to answer it ( and this is normal ). The most difficult thing is to distinguish an ordinal scale from an interval scale, and from the point of view of measurement theory this establishment is not an easy task at all. We rarely encounter a classical interval scale like temperature in degrees Celsius or a scale of chronology in our studies, but the data of the sum scores of questionnaires, most often, also belong to the interval scale, although they have more ordinal properties (it is difficult to say that between 20 and 30 points on the Beck anxiety scale there is exactly the same anxiety interval as between 60 and 70 points). We would rather classify it as an ordinal scale due to the fact that there are a lot of divisions on this scale (more than when we evaluate something on a scale from 1 to 5), and we do not want to limit ourselves to statistical methods suitable for working only with an ordinal scale and not higher. In other cases, defining the scale is a little easier. I suggest using the following simplified algorithm for determining the scale for the first time:\n\nAre there any letters in the variable values? If so, it is either a nominative scale or an ordinal scale.\nCan we arrange these letter values ​​in a certain order on the x-scale? Will anything change if we swap adjacent values? If yes, it is an ordinal scale, if no, it is a nominative scale (for example, I can arrange the names of universities in any order, but the places taken in competitions, only in the order 1-2-3, the change 1-3-2 ruins the meaning)\nIf the answer to (1) is no, and there are no letter values, can the numbers be replaced with letters without changing anything? If yes, then this is also a nominative scale (e.g., subject ID)\nIf there are no letters and the numbers in the data cannot be replaced by letters, it can be either an ordinal scale, an interval scale, or a ratio scale.\nAre the intervals on the scale equal at different places? Will the interval, for example, 20 to 30 include exactly the same number of values ​​as the interval from 50 to 60? If not, it is an ordinal scale; if yes, it is either an interval or a ratio scale.\nIs there an absolute zero on the scale? Doesn’t it sound absurd to say that “value 1” is so many times greater or less than “value 2”? If so, it’s a ratio scale; if not, it’s an interval scale. Done!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and types of scales</span>"
    ]
  },
  {
    "objectID": "descriptives.html",
    "href": "descriptives.html",
    "title": "4  Descriptive statistics",
    "section": "",
    "text": "4.1 Descriptive Visualizations\nWe now turn to a discussion of the first of the two types of statistics, descriptive statistics . When we talk about descriptive statistics, we are always talking about empirically obtained data, not about the population. To say anything about the population, we need to deal with inferential statistics, but for now we will stick with empirically obtained data and sample distributions.\nWhy do we need descriptive statistics? We discussed that descriptive statistics:\nIf it so happens that our collected data represents the entire general population, then we do not even need to engage in inferential statistics - we can draw some conclusions about this data from empirically collected data (for example, compare the performance in class A and B of just one school)\nWe have already discussed two important graphs – the histogram and the probability density plot – when we talked about distributions. These graphs are worth making whenever we want to explore empirically obtained data using descriptive statistics. There seems to be no official term for this, but I also call these graphs descriptive.\nFor example, we built them for data on emotional burnout:\nTo visualize the ordinal variable Teaching Experience exp_scaled\nburnout %&gt;% \n  ggplot(aes(x=exp_scaled)) +\n  geom_bar() +\n  theme_minimal()\nTo visualize the quantitative variable Age (ratio scale)\nburnout %&gt;% \n  ggplot(aes(x=age)) +\n  geom_density() +\n  theme_minimal()\nDistribution graphs, in fact, already give us a lot - at least an understanding of what family of distributions a variable may belong to, and what the distribution properties of these data are. When talking about the properties of graphs, there are usually two.\nThe asymmetry coefficient, or “skewness”, is an indicator of the symmetry of the graph: if the graph is symmetrical6, this indicator is equal to 0; if not, it has a value different from zero in the direction in which the graph is skewed.\nExcess, “stretching” (kurtosis) is an indicator of how much the graph is stretched upward or “flattened” along the abscissa axis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>descriptive</span>"
    ]
  },
  {
    "objectID": "descriptives.html#descriptive-visualizations",
    "href": "descriptives.html#descriptive-visualizations",
    "title": "4  Descriptive statistics",
    "section": "",
    "text": "If the asymmetry coefficient is positive,S&gt;0S&gt;0– they talk about right-sided assimilation (the right side is more elongated than the left).\nIf the asymmetry coefficient is negative,S&lt;0S&lt;0– they talk about left-sided assimilation (the left side is more elongated than the right).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>descriptive</span>"
    ]
  },
  {
    "objectID": "descriptives.html#ctm",
    "href": "descriptives.html#ctm",
    "title": "4  Descriptive statistics",
    "section": "4.2 Measures of Central Tendency",
    "text": "4.2 Measures of Central Tendency\nMeasures of central tendency are descriptive statistics that tell you something about the “center of mass” of a distribution: where is its center, where is the most data in this distribution? This can be very useful in applied questions.\n\nLet’s imagine that we are interested in the question: what is the well-being of Russians? Has it changed from 2008 to 2022, and if so, how? How can we answer this question? ( let’s imagine that we have access to real data of all Russians collected on this issue, and there is no need to make a statistical conclusion on a small sample )\n\nLet’s say we evaluate well-being by annual per capita income. The first thing that comes to mind is to calculate the average income in 2008 and in 2022 and see how they differ. Will this be the right calculation? Most likely, not.\nThe point is that per capita income is a very uneven indicator (variable). https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%94%D0%B6%D0%B8%D0%BD%D0%B8\n\n4.2.1 Mean, median, mode\nThe arithmetic mean (mean) is the sum of all values divided by the number of observations. It is calculated using the formula\n\\(\\bar X = \\dfrac{\\sum_{i=1}^{n}x_i}{n}\\)\nFor example, we have a data set:\n\n\n [1] 49 37  1 25 10 36 18 24  7 45 47\n\n\nIts average\n\nmean(sample_data1)\n\n[1] 27.18182\n\n\nProperties of the average\nIt is useful to remember the properties of the average:\n\nIf you add (or subtract) the same number to each sample value, the average will also increase (or decrease) by the same number:¯\\(\\bar X_(x_i+c) = \\bar X + c\\)\nIf each sample value is multiplied (or divided) by the same number, then the average will also increase (or decrease) by the same number of times.¯\\(\\bar X_(x_i \\times c) = \\bar X \\times c\\)\nThe sum of all deviations from the mean is zero. \\(\\sum_{i=1}^{n}(\\bar X - x_i) = 0\\)\n\nThe median is the boundary that divides an ordered set of data into two. To calculate the median, we need to: 1) arrange all the available values in ascending order; 2) find the middle: this will be either the value corresponding to the place \\(\\frac{n}{2}+1\\), if n is odd, or the average of two central values \\((\\frac{1}{2}(X_{n/2}+X_{n+1})/2\\), if n is even.\n\nsort(sample_data1)\n\n [1]  1  7 10 18 24 25 36 37 45 47 49\n\nmedian(sample_data1)\n\n[1] 25\n\n\n\n\n [1] 37 46 20 26  3 41 25 27 36 50\n\n\n\nsort(sample_data2)\n\n [1]  3 20 25 26 27 36 37 41 46 50\n\nmedian(sample_data2)\n\n[1] 31.5\n\n\nIn practice, we rarely encounter data where each value is presented only once – we have already discussed at length that in statistics we work with probabilities and frequencies. Therefore, in reality, we must remember that we are not halving the values ​​themselves, as in the case of the average – but the distribution of values. In a histogram or probability density graph, the median is a line that divides the graph into two equal parts: the same number of data (values ​​taken with their frequencies) should remain on the left and right.\n\nburnout %&gt;% \n  ggplot(aes(x=exp_years)) +\n  geom_histogram(binwidth = 1) +\n  geom_vline(xintercept = median(exp_years)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nMode is the value of a feature that occurs more often than others.\nOn the histogram, the mode will always be on the highest bar, and on the probability density graph – at its peak (or very, very close to it – remember that the probability density graph is a trick, where we draw each point as if “taking” its neighborhood with us, therefore, depending on the size of the neighborhood, the median may drift a little from the peak, but this is a tiny deviation)\nWeighted average.\nSquare mean.\n\n\n4.2.2 Features of use\nIn the welfare valuation example, we found that the mean is a poor choice for describing these data using measures of central tendency. So which measures of central tendency should we choose?\n\n\n\n\n\n\n\n\nMeasure of CT\nData and scale\nFeatures of use\n\n\n\n\nAverage\nQuantitative only: ratio or interval scale\na fairly large sample, the distribution is symmetrical, there are no noticeable outliers\n\n\nMedian\nQuantitative or ordinal: ratio scale, interval or ordinal (rank) scale\nwe can use it when the distribution is not symmetrical, there are outliers, we cannot use it for the nominative scale\n\n\nFashion\nAny scale, Nominal, Ordinal, Quantitative (ratio or interval)\nmost often used where we cannot perform metric operations, but cannot accurately calculate the mode for continuous quantities\n\n\n\nWhere is the mean, median and mode on the graphs?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have come close to the concepts that can be (and we will be) directly applied to the data. To make these concepts close to reality, we will take a real dataset (collected data) on which we will work further. As an example, we will take a dataset from kaggle https://www.kaggle.com/datasets/uciml/student-alcohol-consumption . This is data from two Portuguese schools (more likely colleges) with detailed socio-demographic information about students, including how they study in mathematics and Portuguese and how often they drink alcohol. I took this dataset, because it contains variables of different types of data on different scales (both a relationship scale, such as age, and an ordinal scale, such as ratings of mother’s or father’s education or support in the family).\n\nkable(students[1:10,]) %&gt;% scroll_box(width = \"100%\") \n\n\n\n\n\nstudent\nschool\nsex\nage\naddress\nfamsize\nPstatus\nMedu\nFedu\nMjob\nFjob\nreason\nguardian\ntraveltime\nstudytime\nfailures\nschoolsup\nfamsup\npaid_mat\nactivities\nnursery\nhigher\ninternet\nromantic\nfamrel\nfreetime\ngoout\nDalc\nWalc\nhealth\nabsences_mat\nG1_mat\nG2_mat\nG3_mat\npaid_por\nabsences_por\nG1_por\nG2_por\nG3_por\nG_mat\nG_por\nansences_mat_groups\nansences_por_groups\n\n\n\n\nid1\nGP\nF\n18\nU\nGT3\nA\n4\n4\nat_home\nteacher\ncourse\nmother\n2\n2\n0\nyes\nno\nno\nno\nyes\nyes\nno\nno\n4\n3\n4\n1\n1\n3\n6\n5\n6\n6\nno\n4\n0\n11\n11\n5.666667\n7.333333\nmiddle\nless\n\n\nid2\nGP\nF\n17\nU\nGT3\nT\n1\n1\nat_home\nother\ncourse\nfather\n1\n2\n0\nno\nyes\nno\nno\nno\nyes\nyes\nno\n5\n3\n3\n1\n1\n3\n4\n5\n5\n6\nno\n2\n9\n11\n11\n5.333333\n10.333333\nless\nless\n\n\nid4\nGP\nF\n15\nU\nGT3\nT\n4\n2\nhealth\nservices\nhome\nmother\n1\n3\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nyes\n3\n2\n2\n1\n1\n5\n2\n15\n14\n15\nno\n0\n14\n14\n14\n14.666667\n14.000000\nless\nless\n\n\nid5\nGP\nF\n16\nU\nGT3\nT\n3\n3\nother\nother\nhome\nfather\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nno\nno\n4\n3\n2\n1\n2\n5\n4\n6\n10\n10\nno\n0\n11\n13\n13\n8.666667\n12.333333\nless\nless\n\n\nid6\nGP\nM\n16\nU\nLE3\nT\n4\n3\nservices\nother\nreputation\nmother\n1\n2\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nno\n5\n4\n2\n1\n2\n5\n10\n15\n15\n15\nno\n6\n12\n12\n13\n15.000000\n12.333333\nmiddle\nmiddle\n\n\nid7\nGP\nM\n16\nU\nLE3\nT\n2\n2\nother\nother\nhome\nmother\n1\n2\n0\nno\nno\nno\nno\nyes\nyes\nyes\nno\n4\n4\n4\n1\n1\n3\n0\n12\n12\n11\nno\n0\n13\n12\n13\n11.666667\n12.666667\nless\nless\n\n\nid8\nGP\nF\n17\nU\nGT3\nA\n4\n4\nother\nteacher\nhome\nmother\n2\n2\n0\nyes\nyes\nno\nno\nyes\nyes\nno\nno\n4\n1\n4\n1\n1\n1\n6\n6\n5\n6\nno\n2\n10\n13\n13\n5.666667\n12.000000\nmiddle\nless\n\n\nid9\nGP\nM\n15\nU\nLE3\nA\n3\n2\nservices\nother\nhome\nmother\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nyes\nno\n4\n2\n2\n1\n1\n1\n0\n16\n18\n19\nno\n0\n15\n16\n17\n17.666667\n16.000000\nless\nless\n\n\nid10\nGP\nM\n15\nU\nGT3\nT\n3\n4\nother\nother\nhome\nmother\n1\n2\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nno\n5\n5\n1\n1\n1\n5\n0\n14\n15\n15\nno\n0\n12\n12\n13\n14.666667\n12.333333\nless\nless\n\n\nid11\nGP\nF\n15\nU\nGT3\nT\n4\n4\nteacher\nhealth\nreputation\nmother\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nyes\nno\n3\n3\n3\n1\n2\n2\n0\n10\n8\n9\nno\n2\n14\n14\n14\n9.000000\n14.000000\nless\nless\n\n\n\n\n\n\n\n\nstudents %&gt;% \n  ggplot(aes(x=age)) +\n  geom_histogram(binwidth = 1) +\n  geom_density(aes(x = age)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ntable(students$age)\n\n\n15 16 17 18 19 22 \n71 91 87 64  6  1 \n\nprop.table(table(students$age))\n\n\n      15       16       17       18       19       22 \n0.221875 0.284375 0.271875 0.200000 0.018750 0.003125 \n\n# round(prop.table(table(students$age, students$Walc), 1), 2)\n\n\nmean(students$age)\n\n[1] 16.525\n\n\n\nmedian(students$age)\n\n[1] 16\n\n\n\nmode(students$age)\n\n[1] 16\n\n\n\nstudents %&gt;% \n  ggplot(aes(x=G_mat)) +\n  geom_density() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nmean(students$G_mat)\n\n[1] 11.16979\n\n\n\nmedian(students$G_mat)\n\n[1] 11\n\n\n\ntable(students$Mjob)\n\n\n at_home   health    other services  teacher \n      44       30      116       75       55 \n\nprop.table(table(students$Mjob))\n\n\n at_home   health    other services  teacher \n0.137500 0.093750 0.362500 0.234375 0.171875 \n\nmode(students$Mjob)\n\n[1] \"other\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>descriptive</span>"
    ]
  },
  {
    "objectID": "descriptives.html#quantiles-quartiles-and-percentiles",
    "href": "descriptives.html#quantiles-quartiles-and-percentiles",
    "title": "4  Descriptive statistics",
    "section": "4.3 Quantiles, quartiles and percentiles",
    "text": "4.3 Quantiles, quartiles and percentiles\nWe have found out that the median divides the distribution in half. This is convenient. For example, to estimate below / above what value the first or second half of the data is.\nThese questions come up quite often, and not just about half of the data, but about specific chunks of data. Quantiles are a general term for points that divide a distribution into equal chunks of data—say, ten percent chunks.\nDeciles are special cases of quantiles, 9 points that divide the entire distribution into 10 equal parts. Thus, each part contains 10% of the data.\nPercentiles are special cases of quantiles, 99 points that divide the entire distribution into 100 equal parts, so that each part contains 1% of the data.\nQuartiles are special cases of quantiles, they divide the entire distribution into four parts, 25% of the data each. That is, there are 4 quartiles in total, and they are designated by the letter Q: Q1, Q2, Q3, Q4.\nIt turns out that:\n\nto the left of the first (lower) quartileQ1Q125% of observations are\nto the left of the second (middle) quartile (median)Q2Q250% of observations lie\nto the left of the third (upper) quartileQ3Q375% of observations are\n\nknitr::include_graphics(\"docs/images/quartiles-2.png\")\n\nknitr::include_graphics(\"images/quartiles-2.png\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>descriptive</span>"
    ]
  },
  {
    "objectID": "descriptives.html#measures-of-variability-variability",
    "href": "descriptives.html#measures-of-variability-variability",
    "title": "4  Descriptive statistics",
    "section": "4.4 Measures of variability (variability)",
    "text": "4.4 Measures of variability (variability)\nIn addition to measures of central tendency, there are measures of variability, also known as measures of changeability or dispersion. Variability is the variability of values, how much the data differs from observation to observation within the sample.\n\nLet’s say you find that your red blood cell count is 3.8*10, while the norm is 4*10. How big of a deviation is this?\n\nWhy do we need them? Isn’t it enough to use measures of central tendency to describe a distribution? No, it’s not enough. While measures of central tendency show where our data tends to be, where their “center of mass” is, that is, what single number can characterize our data in general - measures of variability show how much spread there is in our data, how close each of our observations lies to the mean?\nMeasures of central tendency may coincide although measures of variability differ significantly, for example\n\n\n\n\n\n\n\n\n\nImages from Anton Angelhardt’s textbook\nThese distributions are very different, butthey are together after allif you look at their averages, they will be the same\n\n\n\n\n\n\n\n\n\n\n4.4.1 Range, standard deviation and variance\nRange is the difference between the maximum and minimum values in a sample \\(x_{max}-x_{min}\\)\n\nsample_data1\n\n [1] 49 37  1 25 10 36 18 24  7 45 47\n\nrange(sample_data1)\n\n[1]  1 49\n\n\nHere are the minimum and maximum values, to calculate the range, you need to subtract the minimum from the maximum. And as you can see, the range is very sensitive to outliers - if you add the value “3000” to this sample, the range will change dramatically.\nWe have already heard about standard deviation (at least, it appears in the formula of normal distribution). Now let’s get to know it better and try to derive it.\nSince if we calculate the deviation of each observation from the mean and try to average them, we will get zero, we need a different approach to estimating the deviation. If we want to get rid of the problem of getting zero, we usually have two options: take the values ​​by module or square them. To obtain an estimate of the deviation, we used the second option and introduced another important measure of variability for - variance .\nVariance is a measure of variability, which is the square of the standard deviation and is calculated using the formula \\(D = \\dfrac{\\sum_{i=1}^{n}(x_i - \\bar x)^2}{n}\\)\nThe greater the variability in the data, the greater (proportional to the square of the deviations) the dispersion.\nAn important point is the formula for the dispersion for the general population. For sample dispersion, the number of degrees of freedom is reduced by one, so the formula will take the following form \\(D = \\dfrac{\\sum_{i=1}^{n}(x_i - \\bar x)^2}{n-1}\\)\nStandard deviation, also known as root mean square deviation , is the average deviation of observations from their mean. It is calculated as the square root of the variance: \\(sd = \\sqrt{\\dfrac{\\sum_{i=1}^{n}(x_i - \\bar x)^2}{n-1}}\\)\nProperties of variance and standard deviation\nJust like the mean, there are important properties for the variance/standard deviation\n\nIf the same number (constant) is added (or subtracted) from each sample value \\(c\\), then the dispersion and standard deviation will not change: \\(D_(x_i+c) = D\\), \\(sd_(x_i+c) = sd\\)\nIf each sample value is multiplied (or divided) by the same numbercc, then the dispersion will increase (decrease) in \\(c^2\\) times \\(D_(x_i \\times c) = D \\times c^2\\), and the average will increase (or decrease) in \\(c\\) times: \\(sd_(x_i \\times c) = sd \\times c\\)\nIf the values ​​of two samples do not differ from each other, then they have equal variance and standard deviation\n\nUsing student data as an example:\n\nrange(students$age)\n\n[1] 15 22\n\nrange(students$G_mat)\n\n[1]  2.00000 19.33333\n\nrange(students$Walc)\n\n[1] 1 5\n\nrange(students$Mjob)\n\n[1] \"at_home\" \"teacher\"\n\n\n\nsd(students$age)\n\n[1] 1.141687\n\nsd(students$G_mat)\n\n[1] 3.559905\n\nsd(students$Walc)\n\n[1] 1.264167\n\n\nInterquartile range",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>descriptive</span>"
    ]
  },
  {
    "objectID": "descriptives.html#boxplot-and-violinplot",
    "href": "descriptives.html#boxplot-and-violinplot",
    "title": "4  Descriptive statistics",
    "section": "4.5 Boxplot and Violinplot",
    "text": "4.5 Boxplot and Violinplot\n\n\n# knitr::include_graphics(\"docs/images/Boxplot_vs_PDF.png\")\n\n\n\n\n# knitr::include_graphics(\"docs/images/boxplots_hist.png\")\n\n\n\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nstudents %&gt;% \n  ggplot(aes(x=ansences_mat_groups, y = G_mat)) +\n  geom_violin(aes(fill = ansences_mat_groups)) +\n  geom_boxplot(aes(fill = ansences_mat_groups), width=.1) +\n  scale_fill_viridis(discrete=TRUE) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>descriptive</span>"
    ]
  },
  {
    "objectID": "distributions.html",
    "href": "distributions.html",
    "title": "3  Distributions",
    "section": "",
    "text": "3.1 5.1 Types of distributions\nWhat is distribution? We use this word all the time.\nWhen speaking about distributions, we mean the law of distribution of a random variable – the correspondence between the possible values ​​of this variable from the range of its admissible values ​​and: the probabilities of these values ​​– for a discrete variable, or the probability density – for a continuous variable.\nThe laws of probability distributions can not be derived for all quantities! But it so happened that we can describe some patterns, probability distributions, with formulas, as in physics: the force of gravity (the force with which the Earth attracts all bodies) is directly proportional to the mass of the object, taken with the coefficient of acceleration of free fall \\(F = m*g\\).. We noticed this in relation to the surrounding world and derived a law (not we personally, but representatives of the planet Earth in general). The same is with distributions: we noticed that the probabilities of the distribution of some random variables obey certain laws, and wrote them down. For example, it has been proven that continuous random variables, which are affected by many random factors (for example, height, weight, etc.), are distributed in accordance with the Gaussian distribution , also known as the normal distribution . Its formula is:\n\\(P(x) = \\frac{e^{-(x - \\mu)^{2}/(2\\sigma^{2}) }} {\\sigma\\sqrt{2\\pi}}\\)\nplot(dnorm(1:1000, mean = 500, sd = 100))\nOr, for example, the exponential distribution:\n\\(P(x)= \\lambda \\times e^{-\\lambda x}\\)\nplot(dexp(x = 1:100, rate = 0.1))\n\\(P(x)= \\lambda \\times e^{-\\lambda x}\\)\nplot(dchisq(1:1000, df=100))\nOr, for example, the binomial distribution:\n\\(P(k | n, p) = \\frac{n!}{k!(n-k)!} \\times p^k \\times (1-p)^{n-k} =  {n \\choose k} \\times p^k \\times (1-p)^{n-k}\\)\nplot(dbinom(1:100, size=100, prob=0.5))\nYou can look and be horrified here http://www.math.wm.edu/~leemis/chart/UDR/UDR.html . Fortunately, we won’t need any of this.\nLess scary version https://www.johndcook.com/blog/distribution_chart/#normal",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>distributions</span>"
    ]
  },
  {
    "objectID": "distributions.html#types-of-distributions",
    "href": "distributions.html#types-of-distributions",
    "title": "3  distributions",
    "section": "4.1 5.1 Types of distributions",
    "text": "4.1 5.1 Types of distributions\nThe laws of probability distributions can not be derived for all quantities! But it so happened that we can describe some patterns, probability distributions, with formulas, as in physics: the force of gravity (the force with which the Earth attracts all bodies) is directly proportional to the mass of the object, taken with the coefficient of acceleration of free fall \\(F = m*g\\).. We noticed this in relation to the surrounding world and derived a law (not we personally, but representatives of the planet Earth in general). The same is with distributions: we noticed that the probabilities of the distribution of some random variables obey certain laws, and wrote them down. For example, it has been proven that continuous random variables, which are affected by many random factors (for example, height, weight, etc.), are distributed in accordance with the Gaussian distribution , also known as the normal distribution . Its formula is:\n\\(P(x) = \\frac{e^{-(x - \\mu)^{2}/(2\\sigma^{2}) }} {\\sigma\\sqrt{2\\pi}}\\)\n\nplot(dnorm(1:1000, mean = 500, sd = 100))\n\n\n\n\n\n\n\n\nOr, for example, the exponential distribution:\n\\(P(x)= \\lambda \\times e^{-\\lambda x}\\)\n\nplot(dexp(x = 1:100, rate = 0.1))\n\n\n\n\n\n\n\n\n\\(P(x)= \\lambda \\times e^{-\\lambda x}\\)\n\nplot(dchisq(1:1000, df=100))\n\n\n\n\n\n\n\n\nOr, for example, the binomial distribution:\n\\(P(k | n, p) = \\frac{n!}{k!(n-k)!} \\times p^k \\times (1-p)^{n-k} =  {n \\choose k} \\times p^k \\times (1-p)^{n-k}\\)\n\nplot(dbinom(1:100, size=100, prob=0.5))\n\n\n\n\n\n\n\n\nYou can look and be horrified here http://www.math.wm.edu/~leemis/chart/UDR/UDR.html . Fortunately, we won’t need any of this.\nLess scary version https://www.johndcook.com/blog/distribution_chart/#normal",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>distributions</span>"
    ]
  },
  {
    "objectID": "distributions.html#distribution-functions",
    "href": "distributions.html#distribution-functions",
    "title": "3  Distributions",
    "section": "3.2 5.2 Distribution functions",
    "text": "3.2 5.2 Distribution functions\nAbove we got acquainted with probability density, let’s record all the functions applicable to probability that are useful:\n\nprobability density function for continuous variables (e.g. height, weight) and probability mass function for discrete variables (e.g. number of cases) – the simplest basic function, often denoted by the letterd*\ncumulative distribution function / probability density function for continuous variables, often denoted by the letterp*\nquantile function , also known as the inverse cumulative distribution density function (more on that later), is often denoted by the letter q* probability density function (cumulative distribution function; cdf)\n\nWhy are they needed?\nLet’s look at the example of IQ tests.\nProbability density function\n\niq &lt;- seq(50,150, 0.1)\nplot(iq, dnorm(iq, mean = 100, sd = 15))\n\n\n\n\n\n\n\n\nCumulative distribution function (cdf)\n\nplot(iq, pnorm(iq, mean = 100, sd = 15))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>distributions</span>"
    ]
  },
  {
    "objectID": "r_1.html",
    "href": "r_1.html",
    "title": "6  Introduction to Data Analysis in R",
    "section": "",
    "text": "6.1 14.1 Data and variables\nThis book is written using the R programming and data processing language and the Bookdown package in the RStudio data science environment.\nTo start working in it, you need to download and install the R language itself https://cran.r-project.org/ and download and install RStudio, the environment for work https://posit.co/downloads/ .\nData is information presented in a form suitable for storage and processing by humans or information systems (ISO/IEC/IEEE 24765-2010). If data is presented in a form suitable for processing by information systems, it is formalized.\nA variable is a shell that we define to store data in it and perform operations on it. A variable has a name and the data that it stores.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "r_1.html#data-and-variables",
    "href": "r_1.html#data-and-variables",
    "title": "6  Introduction to Data Analysis in R",
    "section": "",
    "text": "6.1.1 14.1.1 Basic data types\n\nNumeric (whole numbers – integer or numeric , real numbers – real , floating point numbers (fractional) – float )\nText ( character , if one character, or string – many characters)\nLogical ( logical or boolean – accepts only True / False values)\nNA – missing values ​​(not available)\nNaN – not a number, the result of performing an impossible numerical operation (for example, division by 0)\n\n\n\n6.1.2 14.1.2 Basic data structures\n\nVector – a one-dimensional array of one data type\nAn array is a multidimensional array of one data type, consisting of vectors of the same length.\nMatrix - a two-dimensional array of only numeric data\nA list is essentially a multidimensional array, but can consist of vectors of different lengths and have data of different types.\nData, dataframe - essentially a list, but all vectors are the same length\n\n\n\n\n\n\n\n\n\n\n\nhttps://practicum.yandex.ru/blog/10-osnovnyh-struktur-dannyh/\n\nhttps://practicum.yandex.ru/blog/10-osnovnyh-struktur-dannyh/",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "r_1.html#operations-with-variables-and-functions",
    "href": "r_1.html#operations-with-variables-and-functions",
    "title": "6  Introduction to Data Analysis in R",
    "section": "6.2 14.2 Operations with variables and functions",
    "text": "6.2 14.2 Operations with variables and functions\nDepending on what type of variables we are working with, we can perform different operations on them.\nAn operation is the execution of some action on data. What performs this action is called an operator or a function . The difference between them is that an operator performs atomic (single and simple actions), for example, an operator can be a sign of addition, division, greater than or less than, etc. A function does more complex actions: for example, create a vector using the function c(), read data using the function read_csv(), filter data using the function select(). Note that when calling a function, its name is always followed by parentheses.\n\n\n\n\n\n\n\nData type\nPossible atomic operations\n\n\n\n\nNumerical\n= (assignment), +, -, /, *, %\n\n\nText\n= (assignment),+ (concatenation), search for a specific character\n\n\nLogical\n= (assignment),&gt;, &lt;, == (equal), != (not equal)\n\n\n\n\n# vectors  -------------------------------------------------------\nc(1, 2, 3, 4, 5) # c() -- function for vector creation\n\n[1] 1 2 3 4 5\n\n# numeric vectors  -------------------------------------------------------\nc(1, 2, 3, 4, 5, 7, 21, 143)\n\n[1]   1   2   3   4   5   7  21 143\n\nvector1 &lt;- c(1, 2, 3, 4, 5) # vector1 -- variable name, 1, 2, 3, 4, 5 -- arguments of function с()\n\nage &lt;- c(18, 22, 25, 20, 21)\n\n1:10 # function : creates vector from 1 to 10 with step 1\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n10:1\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\nseq(2, 10, 2) # seq(): creates numeric vector from 2 tp 10 and step 2 \n\n[1]  2  4  6  8 10\n\nseq(1, 10, 2) \n\n[1] 1 3 5 7 9\n\n# Character vector -------------------------------------------------------\n\nanswers &lt;- c(\"no\", \"yes\", \"yes\", \"yes\", \"no\")\n\n#Transformation of data types -- TRUE и FALSE became 1 and 0\nc(1, 2, 3, 4, 5, TRUE, FALSE)\n\n[1] 1 2 3 4 5 1 0\n\nc(1, 2, 3, 4, 5, \"1\", \"0\")\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"1\" \"0\"\n\n# Logical vectors ------------------------------------------------------\n\ncondition &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE) #Logical vector\n\nc(1, 2, 3, 4, TRUE, FALSE)\n\n[1] 1 2 3 4 1 0\n\nc(1, 2, 3, 4, 5, 6)\n\n[1] 1 2 3 4 5 6\n\nc(1, 10, 3, -4, \"green\", \"brown\")\n\n[1] \"1\"     \"10\"    \"3\"     \"-4\"    \"green\" \"brown\"\n\n# Logical expressions ------------------------------------------------------\n\na &lt;- 5\nb &lt;- 10\n\na &gt; b\n\n[1] FALSE\n\na &gt;= b\n\n[1] FALSE\n\na == b\n\n[1] FALSE\n\na != b\n\n[1] TRUE\n\na &lt;= b\n\n[1] TRUE\n\na &lt; b\n\n[1] TRUE\n\n#function performs if else: \n# ifelse() involves 3 arguments: ( conditioin; what should it does if condition true ;\n# what should it does if condition false)\n\nifelse(a &lt; b, a+b, \"а more than b\")\n\n[1] 15\n\n# dataframes ---------------------------------------------------------------\n\neye_color &lt;- c(\"green\", \"brown\", \"grey\", \"blue\", \"red\")\n  \ndata &lt;- data.frame(vector1, age, answers, condition, eye_color) # data.frame() creates dataframe from vectors\nView(data) #  View() open this dataframe in the next tab\n\ndata$age # select column from dataframe\n\n[1] 18 22 25 20 21\n\n# описательные визуализации\nhist(data$age, breaks = c(18, 22, 27))\nlines(density(data$age)) \n\n\n\n\n\n\n\n?hist\nplot(density(data$age))\n\n\n\n\n\n\n\n# descriptive statistics\nmean(data$age)\n\n[1] 21.2\n\nsd(data$age)\n\n[1] 2.588436\n\nround(sd(data$age),1) #rounds to 1 digit\n\n[1] 2.6\n\nmin(data$age)\n\n[1] 18\n\nmax(data$age)\n\n[1] 25\n\nrange(data$age)\n\n[1] 18 25\n\nsummary(data$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   18.0    20.0    21.0    21.2    22.0    25.0",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "r_1.html#getting-started-in-the-environment",
    "href": "r_1.html#getting-started-in-the-environment",
    "title": "6  Introduction to Data Analysis in R",
    "section": "6.3 14.3 Getting Started in the Environment",
    "text": "6.3 14.3 Getting Started in the Environment\nIn this tutorial I will provide tasks for training in any data working environment and code for working with data in R.\nFor most interesting operations in R, we will need additional packages - a set of functions that someone has already written for us (the same as, for example, import numpy as npin Python). We will mainly need the package tidyverse, literally - “the universe of clean data”. Let me say right away that everything marked with the # symbol is comments, our hints that will not be executed. It is important to leave them for yourself, so as not to forget what is happening here.\nFor the package to work, it needs to be 1. installed and 2. connected\n\n# install package\ninstall.packages(\"tidyverse\")\n\n\n# switch on package\nlibrary(tidyverse)\n\nYou only need to install the package once after installing R, libraryyou always need to connect via when you open RStudio. An error like “could not find function” in 99% of cases indicates that the package from which it is used is not connected.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "r_1.html#getting-started-with-data",
    "href": "r_1.html#getting-started-with-data",
    "title": "6  Introduction to Data Analysis in R",
    "section": "6.4 14.4 Getting Started with Data",
    "text": "6.4 14.4 Getting Started with Data\nWe will work with the World Happiness Report data for 2016 https://www.kaggle.com/datasets/unsdsn/world-happiness . This is the World Happiness Report https://en.wikipedia.org/wiki/World_Happiness_Report , which shows how residents of different countries rate their level of happiness. They can also be downloaded from the link https://raw.githubusercontent.com/elenary/StatsForDA/main/2016.csv (right-click in the opened file - Save as). The 2016 data we will work with includes 157 countries. By the way, can you download data for different years and see how the number of participating countries changed? A little later we will learn how to do this using code . There are the following variables here:\n\nHappiness Rank - position in the ranking\nHappiness Score - the absolute value of the happiness score\nSE - standard error\nEconomy - GDP per capita\nFamily - Social support, feeling of family\nHealth - Life expectancy\nFreedom - Freedom\nTrust (Goverment Corruption) - perceived level of corruption\nGenerosity - how much is donated to charity\nDystopia is a dystopian country with the lowest indicators in all existing parameters.\n\nEconomy, Family, Health, Freedom, Trust, Generosity – 6 indicators by which the level of happiness is calculated.\nTo perform operations with data, they first need to be read , loaded into the data working environment. In modern traditions of R, this is easiest to do using a function read_csv()from the package tidyverse. Do not forget that any function call is accompanied by parentheses, into which arguments are passed. In the function read_csv()in parentheses, you need to pass the path to the file that we want to read. It can be copied through the file properties. But in order not to bother with the path, the easiest way is to put the data file in the same folder where our file with the code itself is located (for this, you need to save it). And the most elegant and commonly used solution is to create a project File - New Project, and store all files in the project folder, this saves from a large number of errors. Importing files is described in great detail by Ivan Pozdnyakov https://pozdniakov.github.io/tidy_stats/030-import_data.html\n\nwhr &lt;- read_csv(\"2016.csv\") # read data sheet \n\nView(whr) \n\nNow I want to explore this data. Remember how we start exploring data? With descriptive statistics\nI can select all the variables one by one and calculate the mean and standard deviation for them – this is what is in the Descriptives tab in Jamovi. Since the column does not exist in a vacuum, but inside a dataset, we need to somehow indicate that we are interested in a specific column inside a specific dataset. For now, the easiest way to do this is with the icon $:\n\nwhr$Family # select column\n\n  [1] 1.16374 1.14524 1.18326 1.12690 1.13464 1.09610 1.02912 1.17278 1.10476\n [10] 1.08764 0.99537 1.08383 1.04782 1.02152 1.08113 1.09774 1.03938 1.05249\n [19] 1.16157 1.03999 0.71460 0.86758 1.08672 0.90587 0.98912 1.06612 1.00793\n [28] 0.87114 1.03143 1.09879 1.02169 1.00508 1.04477 0.84829 0.92624 0.87964\n [37] 1.12945 0.83309 0.87119 0.77866 0.87758 0.94397 0.98569 1.03302 1.08268\n [46] 0.80975 0.88025 0.89521 1.16810 1.04167 0.85974 0.68655 1.06054 0.95544\n [55] 0.83132 1.05261 1.04685 0.72194 0.83779 1.06411 1.04993 0.81826 1.05613\n [64] 0.81255 1.03817 0.75695 0.95076 0.95025 0.70697 1.11111 0.72803 1.05163\n [73] 0.96372 0.60809 0.87021 0.33613 0.66062 0.87717 0.87625 0.86216 0.76042\n [82] 0.87877 0.79381 0.90836 0.95434 0.81329 0.64367 0.74173 0.99496 0.38595\n [91] 0.93164 0.26135 0.64184 0.94367 0.78236 0.79117 0.75862 0.43165 0.75473\n[100] 0.75602 1.08983 0.54970 0.64498 0.75596 0.38857 0.63760 0.69699 0.71629\n[109] 0.50163 0.24749 0.62800 0.59205 0.70362 0.62542 0.37932 0.96053 0.84783\n[118] 0.29247 0.69981 0.49813 0.62477 0.76240 1.01413 0.49353 0.80676 0.19249\n[127] 0.47799 0.77416 0.92542 0.84142 0.71478 0.14700 0.81928 0.72368 0.86333\n[136] 0.29561 0.89186 0.60323 0.57576 0.53750 0.66366 0.60530 0.18519 0.63178\n[145] 0.63054 0.90981 0.47493 0.46115 0.77623 0.50353 0.31090 0.61586 0.10419\n[154] 0.11037 0.00000 0.14866 0.23442\n\n\nOr we can remember that a data frame has two dimensions, like a two-dimensional array , and we can access it by index (in square brackets): row number (first number) and column number (second number). If we are not interested in a specific row, but all rows, then nothing is put in place of this index, as if we are skipping it.\n\nwhr[,8] # select column but using other way\n\n# A tibble: 157 × 1\n   Family\n    &lt;dbl&gt;\n 1   1.16\n 2   1.15\n 3   1.18\n 4   1.13\n 5   1.13\n 6   1.10\n 7   1.03\n 8   1.17\n 9   1.10\n10   1.09\n# ℹ 147 more rows\n\n\nPay attention to the output: what data structures do you think the results of the first and second methods belong to?\nLet’s calculate the mean and standard deviation for this column.\n\nmean(whr$Family) # mean\n\n[1] 0.7936211\n\n\n\nsd(whr$Family) # standart deviation\n\n[1] 0.2667057",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "r_1.html#tasks-after-seminar",
    "href": "r_1.html#tasks-after-seminar",
    "title": "6  Introduction to Data Analysis in R",
    "section": "6.5 Tasks after seminar",
    "text": "6.5 Tasks after seminar\n\nRead the 2016 WHR data in the environment you are working in (in R, use the function read_csv()from the package tidyverse).\nCalculate the mean, standard deviation, median, and range (spread from maximum to minimum value) for all 6 indicators that make up the level of happiness (Economy, Family, Health, Freedom, Trust, Generosity). What can you say about them? Where is the largest range? Where are the mean and median close to each other, and where are they not so close? (To calculate the median and range in R, you will need to learn to google a little or use other materials)\nPlot a histogram and a probability density plot for all 6 indicators that make up the level of happiness.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "normdist.html",
    "href": "normdist.html",
    "title": "5  Normal distribution and standardization",
    "section": "",
    "text": "5.1 Normal distribution and its properties\nWe have already discussed the normal distribution when we talked about distributions in general [# Distributions]. This is a probability law, where given values of a feature are matched with the probability of encountering a feature with such a value, expressed by the formula\n\\(P(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} * e^{-\\frac{(x - \\mu)^2}{2(\\sigma)^2}}\\)\nIf you look closely at the formula for the normal distribution, you will notice that almost everything in it is constant, except for the variablex x and two unknown parameters:\\(\\mu\\) и \\(\\sigma\\).. These are our already familiar friends: the mean of the general population (the mathematical expectation and standard deviation of the general population ). Thus, in order to construct a distribution for any feature that is normally distributed in the general population, we only need to know two parameters of the distribution :\nIn addition to the formula itself, you can often see the following notation denoting a normal distribution: \\(\\sim \\mathcal{N}(\\mu, \\, \\sigma^2)\\)\nThe figure shows that the mean is responsible for the position of the distribution center, and the standard deviation is responsible for its “stretching” along the axis.XX, the width of the bell.\nImage from Wikipedia\nWhy do we pay so much attention to the normal distribution?\nThe normal distribution has a number of properties :\nThe latter property gives us very great opportunities when working with a normal distribution: for example, having a representative sample of height measurements, we can calculate the probability with which, for example, we will meet a woman with a height of 158 cm. Let’s recall the visualization from the first seminars https://ourworldindata.org/human-height\nTo calculate this probability, we essentially need to enter into the normal distribution formulaxxinsertx=158x=158, and asμμAndσσuse sample means and standard deviations.\nSome percentage values ​​from this distribution are worth remembering because they will appear frequently - these are the values ​​for the amount of data that lies within one or more standard deviations of the mean.\n“Three Sigma Rule” :",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Normal distribution and standardization</span>"
    ]
  },
  {
    "objectID": "normdist.html#normal-distribution-and-its-properties",
    "href": "normdist.html#normal-distribution-and-its-properties",
    "title": "5  Normal distribution and standardization",
    "section": "",
    "text": "average\nstandard deviation\n\n\n\n\n\n\n\nThe normal distribution is interesting to us at least because most of the features we study are random variables that are affected by a large number of random factors - and therefore, according to the central limit theorem , are distributed normally. As a consequence, physical, biological and psychological features (height, weight, expression of personality traits) are usually distributed normally: average features are found in nature more often than more radical ones.\nThe normal distribution is symmetrical and unimodal, and is easier to work with in some operations.\nIn order to estimate the probability of meeting a person of a certain height, we only need to know (or estimate) the mean and standard deviation of the general population - and then we calculate this probability using the normal distribution formula.\n\n\n\nsymmetrically\nunimodal (only one mode)\ndeviations from the mean obey the law of probability: we know what percentage of the data is contained in how many standard deviations from the mean\n\n\n\n\n\n\n\n\nwithin one standard deviation of the mean (\\(\\bar x ± \\sigma\\)) 68% of the values ​​are very frequent values;\nwithin two standard deviations from the mean (\\(\\bar x ± 2\\sigma\\)) contains 95% of the values ​​- the majority of the sample;\nwithin three standard deviations from the mean (\\(\\bar x ± 3\\sigma\\)) is almost 100% of the sample - that is, the entire sample.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Normal distribution and standardization</span>"
    ]
  },
  {
    "objectID": "normdist.html#standard-normal-distribution-z-distribution",
    "href": "normdist.html#standard-normal-distribution-z-distribution",
    "title": "5  Normal distribution and standardization",
    "section": "5.2 Standard Normal Distribution (z-Distribution)",
    "text": "5.2 Standard Normal Distribution (z-Distribution)\nWhat if we need to estimate which probability is higher: to meet a woman who is 158 cm tall or a man who is 174 cm tall? As we can see from the vuzilization above, these distributions are slightly different, and we cannot place these values ​​on one. Here, the standard normal distribution or z-distribution comes to our aid.\nThe standard normal distribution is a normal distribution centered at zero (\\(\\mu=0\\)) and standart distribution equal to 1 (\\(\\sigma=1\\)).\n\n\n\n\n\n\n\n\n\nThis distribution is universal and dimensionless: on the scale we no longer haveσσ, and numbers that have no dimension. This scale is called the z-scale, and the distribution itself is also called the z-distribution. Any normal distribution can be brought to the form of a standard normal - this procedure is called the z-transformation",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Normal distribution and standardization</span>"
    ]
  },
  {
    "objectID": "normdist.html#z-transformation-and-standardization",
    "href": "normdist.html#z-transformation-and-standardization",
    "title": "5  Normal distribution and standardization",
    "section": "5.3 Z-transformation and standardization",
    "text": "5.3 Z-transformation and standardization\nStandardization is the process of transforming a normal distribution to a standard dimensionless Z-score with a mean of 0 and a standard deviation of 1.\nTo achieve standardization, two things need to be done:\n\ncenter the distribution - if it has been shifted away from zero (e.g. \\(\\mu=15\\)), then we need to return it to the center \\(\\mu=0\\)\nnormalize a distribution - get rid of the difference in deviations \\(\\sigma\\), bring our distribution to the distribution with \\(\\sigma=1\\)\n\nGiven these two operations, the z-transform formula is as follows:\n\\(Z_x = \\frac{x - \\mu}{\\sigma}\\)\nThe Z-score can be placed on the Z-distribution and see what percentage of the data lies to the left of the Z-score, i.e. is less likely . This percentage can be calculated using the normal distribution formula by substituting \\(\\mu=0\\) и \\(\\sigma=1\\), but it is easier to use Z-score tables (they are easy to google), for example https://www.z-table.com/\nStandardization does not change the nature of the distribution, it only changes the position of its center and the “flattening” along the h-axis – that is, literally what we know from the properties of the mean and variance\n\n\n\n\n\n\n\n\n\nThe Z-scale can be interpreted as a scale of typicality or frequency of occurrence of values: everything that lies in +- 1 is very common, and the further from the center, the rarer the values ​​we encounter.\nThus, the Z-transform is used to:\n\nFind out how typical (frequent) the value we encounter is\nCompare the probabilities of finding values ​​from two different samples (normally distributed)\n\nFor example, we (somehow) know that the average height of women is 165 cm with a standard deviation of 7, and the average height of men is 178 and a standard deviation of 8.\nReturning to our question:\n\\(W_{158}\\): \\(Z_{158} = \\frac{x - \\mu_w}{\\sigma_w} = \\frac{158 - 165}{7} = -1\\)\n\\(M_{174}\\): \\(Z_{174} = \\frac{x - \\mu_m}{\\sigma_m} = \\frac{174- 178}{8} = -0.5\\)\nWe have converted the height of men and women into Z-scores and can now place them on the Z-distribution. It is already clear that the number -1 is located to the left of the number -0.5, which means that the probability of meeting a woman with a height of 158 cm is less than a man with a height of 174 cm. To estimate this probability accurately, we will have to use the Z-score tables https://www.z-table.com/\n\\(Z_{w_158} = -1\\), which corresponds to approximately 16% probability \\(Z_{m_174} = -0.5\\), which corresponds to approximately 30% probability\nThus, the probability of meeting a woman with a height of 158 cm is 16%, and this is 14% less than the probability of meeting a man with a height of 174 cm.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Normal distribution and standardization</span>"
    ]
  }
]