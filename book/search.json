[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Qualitative and Quantitative Research Methods in Psychology",
    "section": "",
    "text": "1 Sample\nWe looked at two examples - about the effect of caffeine on attention and the effect of a communication work tool on productivity. In these examples, we assumed that we conducted a study - that is, we recruited a certain number of people and drew a conclusion about how it works in general, not only for these people, but for people in general. Why does this happen? Why can we extend these conclusions to all people who can theoretically drink coffee and use this communication tool? And why do we need this at all?\nImage https://towardsdatascience.com/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sample</span>"
    ]
  },
  {
    "objectID": "index.html#sample-estimates",
    "href": "index.html#sample-estimates",
    "title": "Qualitative and Quantitative Research Methods in Psychology",
    "section": "1.1 Sample estimates",
    "text": "1.1 Sample estimates\nHow can we make a generalization about all people? What do we do if we want to know the average height of people on the entire planet? Do we measure every person out of the nearly 8 billion on the planet and take the arithmetic mean?\n\n\n\n\n\n\n\n\n\n\nhttps://ourworldindata.org/human-height\n\nThe general population is the set (or totality) of all objects under study within the framework of a specific research question.\nA sample or sample population is a portion of a general population selected to study a specific research question.\nWhen we talk about a general population, we talk about its parameters , for example, the average height of people, we denote it by X. We cannot calculate it directly, so we have to resort to sample estimates , which are often denoted by the same letter as the parameter of the general population, only “with a cap” or “with a line” X̅ is the sample mean, the average height of people in a specific sample.\nSo, when we want to make a conclusion about all people from the general population, we conduct a study on a small sample (depending on the research method, the required sample size can be from 10 to 1000 people) and estimate the studied parameter of the general population based on sample estimates.\nTo avoid confusion about what we are talking about, key population and sample estimates are referred to differently:\n\n\n\n\n\n\n\n\n\nGeneral population\nSample\n\n\n\n\nMean (mathematical expectation)\n\\(\\mu\\)\nM, \\(\\overline X\\)\n\n\nStandard deviation\n\\(\\sigma\\)\ns, sd",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sample</span>"
    ]
  },
  {
    "objectID": "index.html#representativeness-of-the-sample",
    "href": "index.html#representativeness-of-the-sample",
    "title": "Qualitative and Quantitative Research Methods in Psychology",
    "section": "1.2 Representativeness of the sample",
    "text": "1.2 Representativeness of the sample\nAn important concept is the representativeness of the sample – the ability of the sample to reflect the studied parameter of the general population.\nThe same sample may be representative for one research question and not representative for another!\nRepresentativeness is achieved:\n\nBy random selection of people (we don’t take specific ones that we like more, but randomly)\nSample size (it has been proven that with a large sample size it is sufficiently representative of the general population to provide an answer about the parameter being studied)\n\nMethods for forming representative samples\n\nSimple random sample\nSystematic sample\nStratified sample\nCluster sample\n\nA simple random sample is drawn from a large population at random - all elements of the population have an equal chance of being included in the sample.\nSystematic sampling imitates random selection and is commonly used by sociologists in field research when a certain step in selecting people is set: for example, every 5th or 10th person.\nStratified sampling is needed if the feature we are studying can be affected by some parameter that is distributed unevenly in the general population: for example, we are studying the level of life satisfaction, but we know that it is affected by the level of income - and we need to ensure an equivalent distribution of income levels in the sample. Then, from these income strata, subjects are selected randomly.\nCluster sampling includes entire individual clusters, without random selection within them: for example, we are studying schoolchildren, and instead of selecting them randomly, we include several specific schools (clusters) in the sample.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sample</span>"
    ]
  },
  {
    "objectID": "index.html#dependent_samples",
    "href": "index.html#dependent_samples",
    "title": "Qualitative and Quantitative Research Methods in Psychology",
    "section": "1.3 Dependent and independent samples",
    "text": "1.3 Dependent and independent samples\nAnother important concept regarding samples is that they can be dependent or independent.\nIndependent samples are used when we compare groups of observations that belong to different people (schools/institutes/etc.). For example, in our example of the effect of caffeine on attention, we can recruit a group of people who will drink 3 cups of coffee a day, and another group who will drink decaffeinated coffee. These are different people, not related to each other in any way.\nBut if we took the same group of people and measured their attention after 3 cups of coffee in one week, and then tested the effects of a decaffeinated drink on these same people , these would be dependent samples , since these are still the same people.\n\n\n\n\n\n\n\n\n\n\n\nCase\n\n\nResearcher Nikita studied the relationship between age, teaching experience and professional burnout of university teachers. For this, he selected several universities: Moscow State University, the Higher School of Economics and RANEPA. Nikita found 10 teachers from the psychology departments of each university on the VKontakte social network and asked them to fill out the Maslach Burnout Inventory (MBI). Based on the processed data, Nikita made the following conclusions: 1) in Russia, younger teachers burn out more often; 2) teachers with less teaching experience burn out more often; 3) MSU teachers generally burn out less than HSE teachers. What sampling methods did Nikita use? Are the conclusions correct?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sample</span>"
    ]
  },
  {
    "objectID": "scales.html",
    "href": "scales.html",
    "title": "2  Data and types of scales",
    "section": "",
    "text": "2.1 Measurement and variable\nWhat does it mean to measure something? It means to bring some value on a scale into conformity with the studied feature .\nWhat could be an example of a trait? Anything that we need to measure in a study: number of cups of coffee per day, concentration level, number of errors, reaction time, degree of burnout, number of tasks completed, level of neuroticism, student rating, number of children in the family, temperature, etc.\nThe feature we are studying is also called a variable . We will encounter this concept constantly in data analysis. In fact, if we look at the observation table, any column with measurements is a variable. The rows contain observations, for example, each new person from our sample. The value in a certain column is the value of the variable of this observation.\nLet’s return to the case with Nikita, who studies burnout of university employees. Let’s make a table of data that Nikita could measure.\nteacher_number &lt;- seq(1,30,1)\nage &lt;-  sample(22:60, size = 30, replace = T)\nexp_years &lt;- sample(1:8, size = 30, replace = T)\nexp_scaled &lt;- ifelse(exp_years &gt;= 1 & exp_years &lt;= 2, \"от 1 до 2\",\n                     ifelse(exp_years &gt; 2 & exp_years &lt;= 5, \"от 3 до 5\",\n                            ifelse(exp_years &gt; 5, \"больше 5\", exp_years)))\nburnout_MBI &lt;- sample(19:70, size = 30, replace = T)\nuniver &lt;-  rep(c(\"MSU\", \"HSE\", \"MSU\", \"RANEPA\", \"HSE\", \"RANEPA\"),5)\nburnout &lt;- tibble(teacher_number, age, exp_years, exp_scaled, burnout_MBI, univer)\nkable(burnout[1:10,])\n\n\n\n\nteacher_number\nage\nexp_years\nexp_scaled\nburnout_MBI\nuniver\n\n\n\n\n1\n59\n6\nбольше 5\n25\nMSU\n\n\n2\n43\n8\nбольше 5\n40\nHSE\n\n\n3\n31\n3\nот 3 до 5\n68\nMSU\n\n\n4\n47\n5\nот 3 до 5\n40\nRANEPA\n\n\n5\n39\n6\nбольше 5\n26\nHSE\n\n\n6\n33\n7\nбольше 5\n70\nRANEPA\n\n\n7\n36\n4\nот 3 до 5\n24\nMSU\n\n\n8\n30\n8\nбольше 5\n68\nHSE\n\n\n9\n43\n7\nбольше 5\n34\nMSU\n\n\n10\n25\n8\nбольше 5\n19\nRANEPA\nWhat will be the variables here?\nIn the definition of measurement, in addition to the feature, there is a second important concept - the scale.\nA scale is a system of measurement. So that we can all use the same units of measurement and not go crazy, we, the people of planet Earth, use uniform scales. There are 4 of them, and they are metric and non-metric - that is, whether we can attach a measuring ruler to them or not. Here, a measuring ruler is any conventional device that has a division value (centimeter, gram, second, piece).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and types of scales</span>"
    ]
  },
  {
    "objectID": "scales.html#measurement-and-variable",
    "href": "scales.html#measurement-and-variable",
    "title": "2  self-contained: true",
    "section": "3.1 Measurement and variable",
    "text": "3.1 Measurement and variable\nWhat does it mean to measure something? It means to bring some value on a scale into conformity with the studied feature .\nWhat could be an example of a trait? Anything that we need to measure in a study: number of cups of coffee per day, concentration level, number of errors, reaction time, degree of burnout, number of tasks completed, level of neuroticism, student rating, number of children in the family, temperature, etc.\nThe feature we are studying is also called a variable . We will encounter this concept constantly in data analysis. In fact, if we look at the observation table, any column with measurements is a variable. The rows contain observations, for example, each new person from our sample. The value in a certain column is the value of the variable of this observation.\nLet’s return to the case with Nikita, who studies burnout of university employees. Let’s make a table of data that Nikita could measure.\n\nteacher_number &lt;- seq(1,30,1)\nage &lt;-  sample(22:60, size = 30, replace = T)\nexp_years &lt;- sample(1:8, size = 30, replace = T)\nexp_scaled &lt;- ifelse(exp_years &gt;= 1 & exp_years &lt;= 2, \"от 1 до 2\",\n                     ifelse(exp_years &gt; 2 & exp_years &lt;= 5, \"от 3 до 5\",\n                            ifelse(exp_years &gt; 5, \"больше 5\", exp_years)))\nburnout_MBI &lt;- sample(19:70, size = 30, replace = T)\nuniver &lt;-  rep(c(\"MSU\", \"HSE\", \"MSU\", \"RANEPA\", \"HSE\", \"RANEPA\"),5)\nburnout &lt;- tibble(teacher_number, age, exp_years, exp_scaled, burnout_MBI, univer)\nkable(burnout[1:10,])\n\n\n\n\nteacher_number\nage\nexp_years\nexp_scaled\nburnout_MBI\nuniver\n\n\n\n\n1\n48\n1\nот 1 до 2\n25\nMSU\n\n\n2\n39\n3\nот 3 до 5\n52\nHSE\n\n\n3\n45\n4\nот 3 до 5\n30\nMSU\n\n\n4\n33\n2\nот 1 до 2\n33\nRANEPA\n\n\n5\n50\n6\nбольше 5\n22\nHSE\n\n\n6\n56\n8\nбольше 5\n26\nRANEPA\n\n\n7\n44\n1\nот 1 до 2\n54\nMSU\n\n\n8\n48\n6\nбольше 5\n58\nHSE\n\n\n9\n28\n2\nот 1 до 2\n39\nMSU\n\n\n10\n38\n6\nбольше 5\n20\nRANEPA\n\n\n\n\n\n\n\nWhat will be the variables here?\nIn the definition of measurement, in addition to the feature, there is a second important concept - the scale.\nA scale is a system of measurement. So that we can all use the same units of measurement and not go crazy, we, the people of planet Earth, use uniform scales. There are 4 of them, and they are metric and non-metric - that is, whether we can attach a measuring ruler to them or not. Here, a measuring ruler is any conventional device that has a division value (centimeter, gram, second, piece).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>self-contained: true</span>"
    ]
  },
  {
    "objectID": "scales.html#quantitative-and-non-quantitative-data",
    "href": "scales.html#quantitative-and-non-quantitative-data",
    "title": "2  Data and types of scales",
    "section": "2.2 3.2 Quantitative and non-quantitative data",
    "text": "2.2 3.2 Quantitative and non-quantitative data\nData measured by metric scales are quantitative data (e.g. height, weight, number of cases, temperature). What cannot be measured by metric scales (e.g. eye color, well-being, level of neuroticism, level of education) are non-quantitative data, which can have different names: categorical , sometimes qualitative .\nSometimes data is called qualitative as opposed to quantitative, but this is not entirely correct: the distinction between quantitative and qualitative is usually applied to types of research, where qualitative research is, for example, interviews or analysis of blocks of text. But as a result of this analysis, we may well end up with quantitative variables, for example, the number of times a particular word was used, so I recommend not using the word “qualitative” in relation to data, rather than types of research.\nWith quantitative data it’s simpler – this is everything that can be measured with a metric scale, a conventional ruler.\nAmong non-quantitative data, there are two types: categorical and rank (ordinal)\nWe will examine this data in detail on the scales.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and types of scales</span>"
    ]
  },
  {
    "objectID": "scales.html#types-of-scales",
    "href": "scales.html#types-of-scales",
    "title": "2  Data and types of scales",
    "section": "2.3 3.3 Types of scales",
    "text": "2.3 3.3 Types of scales\nAs we have already understood, different data refer to different scales.\nDifferent scales have different measuring power – the precision with which we measure a feature. The same feature can be measured with different precision: for example, depending on the research question, height can be expressed quantitatively in centimeters on the interval {0; ∞}, or it can be coded as {“less than 150 cm”; 150 cm and more} if we are only interested in prevalence over a certain feature.\nThere are 4 scales in total, if you arrange them from bottom to top according to measuring power: names, ordinal, interval, ratios.\n\n\n\n\n\n\n\n\n\nScale\nDescription\nPossible operations\nExamples\n\n\n\n\nRation\nQuantitative, there is absolute zero, you can calculate how much more or less, and how many times\n=, ≠≠, &gt;, &lt;, +, -, ×, ÷\nHeight, weight, number of cases\n\n\nInterval\nQuantitative, but there is no absolute zero, you can calculate how much more or less, but you can’t calculate how many times\n=,≠≠, &gt;, &lt;, +, -\nTemperature in degrees Celsius, timekeeping according to different calendars\n\n\nOrdinal (rank)\nCategorical (qualitative), you can set “more” or “less”, but you can’t calculate quantitatively how much more or less\n=, ≠≠, &gt;, &lt;\nEducation level, neuroticism level, sports rating\n\n\nNominal\nCategorical (qualitative), cannot be set to “more” or “less”\n=,≠≠\nGender, color, place of residence, name of university",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and types of scales</span>"
    ]
  },
  {
    "objectID": "scales.html#continuous-and-discrete-data",
    "href": "scales.html#continuous-and-discrete-data",
    "title": "2  Data and types of scales",
    "section": "2.4 3.4 Continuous and discrete data",
    "text": "2.4 3.4 Continuous and discrete data\nWe see that our richest possibilities for measurement lie in quantitative scales – on the ratio scale (it is the steepest) and the interval scale (it is worse and, in fact, is rarely encountered in research in our field).\nQuantitative data can be discrete, when the variable takes strictly defined values, and continuous, when it can take any values, to infinity or over a given interval.\nFor example, in our example with the burnout study, the variable age can take any value: teachers can be 25 years old, 27.5 years old, or 31,666.. years old - these are all values ​​from the range of acceptable values ​​for this variable . But if we consider the number of people infected with coronavirus, there is no way it can be 27.5 or 31,666.. - the number of people infected is not expressed as fractional shares of one person.\nAn important concept here is the range of admissible values . For a continuous variable, this is always an interval, for example {0;+∞}, for discrete variables, it is strictly defined values, which, nevertheless, can also tend to infinity, for example, {0;1;2;3;4;5;6;7;8…}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and types of scales</span>"
    ]
  },
  {
    "objectID": "scales.html#how-can-i-determine-what-scale-the-data-is-on",
    "href": "scales.html#how-can-i-determine-what-scale-the-data-is-on",
    "title": "2  Data and types of scales",
    "section": "2.5 3.5 How can I determine what scale the data is on?",
    "text": "2.5 3.5 How can I determine what scale the data is on?\nThis is quite a difficult question, and at first it is very difficult to answer it ( and this is normal ). The most difficult thing is to distinguish an ordinal scale from an interval scale, and from the point of view of measurement theory this establishment is not an easy task at all. We rarely encounter a classical interval scale like temperature in degrees Celsius or a scale of chronology in our studies, but the data of the sum scores of questionnaires, most often, also belong to the interval scale, although they have more ordinal properties (it is difficult to say that between 20 and 30 points on the Beck anxiety scale there is exactly the same anxiety interval as between 60 and 70 points). We would rather classify it as an ordinal scale due to the fact that there are a lot of divisions on this scale (more than when we evaluate something on a scale from 1 to 5), and we do not want to limit ourselves to statistical methods suitable for working only with an ordinal scale and not higher. In other cases, defining the scale is a little easier. I suggest using the following simplified algorithm for determining the scale for the first time:\n\nAre there any letters in the variable values? If so, it is either a nominative scale or an ordinal scale.\nCan we arrange these letter values ​​in a certain order on the x-scale? Will anything change if we swap adjacent values? If yes, it is an ordinal scale, if no, it is a nominative scale (for example, I can arrange the names of universities in any order, but the places taken in competitions, only in the order 1-2-3, the change 1-3-2 ruins the meaning)\nIf the answer to (1) is no, and there are no letter values, can the numbers be replaced with letters without changing anything? If yes, then this is also a nominative scale (e.g., subject ID)\nIf there are no letters and the numbers in the data cannot be replaced by letters, it can be either an ordinal scale, an interval scale, or a ratio scale.\nAre the intervals on the scale equal at different places? Will the interval, for example, 20 to 30 include exactly the same number of values ​​as the interval from 50 to 60? If not, it is an ordinal scale; if yes, it is either an interval or a ratio scale.\nIs there an absolute zero on the scale? Doesn’t it sound absurd to say that “value 1” is so many times greater or less than “value 2”? If so, it’s a ratio scale; if not, it’s an interval scale. Done!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and types of scales</span>"
    ]
  },
  {
    "objectID": "descriptives.html",
    "href": "descriptives.html",
    "title": "4  Descriptive statistics",
    "section": "",
    "text": "4.1 Descriptive Visualizations\nWe now turn to a discussion of the first of the two types of statistics, descriptive statistics . When we talk about descriptive statistics, we are always talking about empirically obtained data, not about the population. To say anything about the population, we need to deal with inferential statistics, but for now we will stick with empirically obtained data and sample distributions.\nWhy do we need descriptive statistics? We discussed that descriptive statistics:\nIf it so happens that our collected data represents the entire general population, then we do not even need to engage in inferential statistics - we can draw some conclusions about this data from empirically collected data (for example, compare the performance in class A and B of just one school)\nWe have already discussed two important graphs – the histogram and the probability density plot – when we talked about distributions. These graphs are worth making whenever we want to explore empirically obtained data using descriptive statistics. There seems to be no official term for this, but I also call these graphs descriptive.\nFor example, we built them for data on emotional burnout:\nTo visualize the ordinal variable Teaching Experience exp_scaled\nburnout %&gt;% \n  ggplot(aes(x=exp_scaled)) +\n  geom_bar() +\n  theme_minimal()\nTo visualize the quantitative variable Age (ratio scale)\nburnout %&gt;% \n  ggplot(aes(x=age)) +\n  geom_density() +\n  theme_minimal()\nDistribution graphs, in fact, already give us a lot - at least an understanding of what family of distributions a variable may belong to, and what the distribution properties of these data are. When talking about the properties of graphs, there are usually two.\nThe asymmetry coefficient, or “skewness”, is an indicator of the symmetry of the graph: if the graph is symmetrical6, this indicator is equal to 0; if not, it has a value different from zero in the direction in which the graph is skewed.\nExcess, “stretching” (kurtosis) is an indicator of how much the graph is stretched upward or “flattened” along the abscissa axis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#descriptive-visualizations",
    "href": "descriptives.html#descriptive-visualizations",
    "title": "4  Descriptive statistics",
    "section": "",
    "text": "If the asymmetry coefficient is positive,S&gt;0S&gt;0– they talk about right-sided assimilation (the right side is more elongated than the left).\nIf the asymmetry coefficient is negative,S&lt;0S&lt;0– they talk about left-sided assimilation (the left side is more elongated than the right).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#ctm",
    "href": "descriptives.html#ctm",
    "title": "4  Descriptive statistics",
    "section": "4.2 Measures of Central Tendency",
    "text": "4.2 Measures of Central Tendency\nMeasures of central tendency are descriptive statistics that tell you something about the “center of mass” of a distribution: where is its center, where is the most data in this distribution? This can be very useful in applied questions.\n\nLet’s imagine that we are interested in the question: what is the well-being of Russians? Has it changed from 2008 to 2022, and if so, how? How can we answer this question? ( let’s imagine that we have access to real data of all Russians collected on this issue, and there is no need to make a statistical conclusion on a small sample )\n\nLet’s say we evaluate well-being by annual per capita income. The first thing that comes to mind is to calculate the average income in 2008 and in 2022 and see how they differ. Will this be the right calculation? Most likely, not.\nThe point is that per capita income is a very uneven indicator (variable). https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%94%D0%B6%D0%B8%D0%BD%D0%B8\n\n4.2.1 Mean, median, mode\nThe arithmetic mean (mean) is the sum of all values divided by the number of observations. It is calculated using the formula\n\\(\\bar X = \\dfrac{\\sum_{i=1}^{n}x_i}{n}\\)\nFor example, we have a data set:\n\n\n [1] 49 37  1 25 10 36 18 24  7 45 47\n\n\nIts average\n\nmean(sample_data1)\n\n[1] 27.18182\n\n\nProperties of the average\nIt is useful to remember the properties of the average:\n\nIf you add (or subtract) the same number to each sample value, the average will also increase (or decrease) by the same number:¯\\(\\bar X_(x_i+c) = \\bar X + c\\)\nIf each sample value is multiplied (or divided) by the same number, then the average will also increase (or decrease) by the same number of times.¯\\(\\bar X_(x_i \\times c) = \\bar X \\times c\\)\nThe sum of all deviations from the mean is zero. \\(\\sum_{i=1}^{n}(\\bar X - x_i) = 0\\)\n\nThe median is the boundary that divides an ordered set of data into two. To calculate the median, we need to: 1) arrange all the available values in ascending order; 2) find the middle: this will be either the value corresponding to the place \\(\\frac{n}{2}+1\\), if n is odd, or the average of two central values \\((\\frac{1}{2}(X_{n/2}+X_{n+1})/2\\), if n is even.\n\nsort(sample_data1)\n\n [1]  1  7 10 18 24 25 36 37 45 47 49\n\nmedian(sample_data1)\n\n[1] 25\n\n\n\n\n [1] 37 46 20 26  3 41 25 27 36 50\n\n\n\nsort(sample_data2)\n\n [1]  3 20 25 26 27 36 37 41 46 50\n\nmedian(sample_data2)\n\n[1] 31.5\n\n\nIn practice, we rarely encounter data where each value is presented only once – we have already discussed at length that in statistics we work with probabilities and frequencies. Therefore, in reality, we must remember that we are not halving the values ​​themselves, as in the case of the average – but the distribution of values. In a histogram or probability density graph, the median is a line that divides the graph into two equal parts: the same number of data (values ​​taken with their frequencies) should remain on the left and right.\n\nburnout %&gt;% \n  ggplot(aes(x=exp_years)) +\n  geom_histogram(binwidth = 1) +\n  geom_vline(xintercept = median(exp_years)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nMode is the value of a feature that occurs more often than others.\nOn the histogram, the mode will always be on the highest bar, and on the probability density graph – at its peak (or very, very close to it – remember that the probability density graph is a trick, where we draw each point as if “taking” its neighborhood with us, therefore, depending on the size of the neighborhood, the median may drift a little from the peak, but this is a tiny deviation)\nWeighted average.\nSquare mean.\n\n\n4.2.2 Features of use\nIn the welfare valuation example, we found that the mean is a poor choice for describing these data using measures of central tendency. So which measures of central tendency should we choose?\n\n\n\n\n\n\n\n\nMeasure of CT\nData and scale\nFeatures of use\n\n\n\n\nAverage\nQuantitative only: ratio or interval scale\na fairly large sample, the distribution is symmetrical, there are no noticeable outliers\n\n\nMedian\nQuantitative or ordinal: ratio scale, interval or ordinal (rank) scale\nwe can use it when the distribution is not symmetrical, there are outliers, we cannot use it for the nominative scale\n\n\nFashion\nAny scale, Nominal, Ordinal, Quantitative (ratio or interval)\nmost often used where we cannot perform metric operations, but cannot accurately calculate the mode for continuous quantities\n\n\n\nWhere is the mean, median and mode on the graphs?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have come close to the concepts that can be (and we will be) directly applied to the data. To make these concepts close to reality, we will take a real dataset (collected data) on which we will work further. As an example, we will take a dataset from kaggle https://www.kaggle.com/datasets/uciml/student-alcohol-consumption . This is data from two Portuguese schools (more likely colleges) with detailed socio-demographic information about students, including how they study in mathematics and Portuguese and how often they drink alcohol. I took this dataset, because it contains variables of different types of data on different scales (both a relationship scale, such as age, and an ordinal scale, such as ratings of mother’s or father’s education or support in the family).\n\nkable(students[1:10,]) %&gt;% scroll_box(width = \"100%\") \n\n\n\n\n\nstudent\nschool\nsex\nage\naddress\nfamsize\nPstatus\nMedu\nFedu\nMjob\nFjob\nreason\nguardian\ntraveltime\nstudytime\nfailures\nschoolsup\nfamsup\npaid_mat\nactivities\nnursery\nhigher\ninternet\nromantic\nfamrel\nfreetime\ngoout\nDalc\nWalc\nhealth\nabsences_mat\nG1_mat\nG2_mat\nG3_mat\npaid_por\nabsences_por\nG1_por\nG2_por\nG3_por\nG_mat\nG_por\nansences_mat_groups\nansences_por_groups\n\n\n\n\nid1\nGP\nF\n18\nU\nGT3\nA\n4\n4\nat_home\nteacher\ncourse\nmother\n2\n2\n0\nyes\nno\nno\nno\nyes\nyes\nno\nno\n4\n3\n4\n1\n1\n3\n6\n5\n6\n6\nno\n4\n0\n11\n11\n5.666667\n7.333333\nmiddle\nless\n\n\nid2\nGP\nF\n17\nU\nGT3\nT\n1\n1\nat_home\nother\ncourse\nfather\n1\n2\n0\nno\nyes\nno\nno\nno\nyes\nyes\nno\n5\n3\n3\n1\n1\n3\n4\n5\n5\n6\nno\n2\n9\n11\n11\n5.333333\n10.333333\nless\nless\n\n\nid4\nGP\nF\n15\nU\nGT3\nT\n4\n2\nhealth\nservices\nhome\nmother\n1\n3\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nyes\n3\n2\n2\n1\n1\n5\n2\n15\n14\n15\nno\n0\n14\n14\n14\n14.666667\n14.000000\nless\nless\n\n\nid5\nGP\nF\n16\nU\nGT3\nT\n3\n3\nother\nother\nhome\nfather\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nno\nno\n4\n3\n2\n1\n2\n5\n4\n6\n10\n10\nno\n0\n11\n13\n13\n8.666667\n12.333333\nless\nless\n\n\nid6\nGP\nM\n16\nU\nLE3\nT\n4\n3\nservices\nother\nreputation\nmother\n1\n2\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nno\n5\n4\n2\n1\n2\n5\n10\n15\n15\n15\nno\n6\n12\n12\n13\n15.000000\n12.333333\nmiddle\nmiddle\n\n\nid7\nGP\nM\n16\nU\nLE3\nT\n2\n2\nother\nother\nhome\nmother\n1\n2\n0\nno\nno\nno\nno\nyes\nyes\nyes\nno\n4\n4\n4\n1\n1\n3\n0\n12\n12\n11\nno\n0\n13\n12\n13\n11.666667\n12.666667\nless\nless\n\n\nid8\nGP\nF\n17\nU\nGT3\nA\n4\n4\nother\nteacher\nhome\nmother\n2\n2\n0\nyes\nyes\nno\nno\nyes\nyes\nno\nno\n4\n1\n4\n1\n1\n1\n6\n6\n5\n6\nno\n2\n10\n13\n13\n5.666667\n12.000000\nmiddle\nless\n\n\nid9\nGP\nM\n15\nU\nLE3\nA\n3\n2\nservices\nother\nhome\nmother\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nyes\nno\n4\n2\n2\n1\n1\n1\n0\n16\n18\n19\nno\n0\n15\n16\n17\n17.666667\n16.000000\nless\nless\n\n\nid10\nGP\nM\n15\nU\nGT3\nT\n3\n4\nother\nother\nhome\nmother\n1\n2\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nno\n5\n5\n1\n1\n1\n5\n0\n14\n15\n15\nno\n0\n12\n12\n13\n14.666667\n12.333333\nless\nless\n\n\nid11\nGP\nF\n15\nU\nGT3\nT\n4\n4\nteacher\nhealth\nreputation\nmother\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nyes\nno\n3\n3\n3\n1\n2\n2\n0\n10\n8\n9\nno\n2\n14\n14\n14\n9.000000\n14.000000\nless\nless\n\n\n\n\n\n\n\n\nstudents %&gt;% \n  ggplot(aes(x=age)) +\n  geom_histogram(binwidth = 1) +\n  geom_density(aes(x = age)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ntable(students$age)\n\n\n15 16 17 18 19 22 \n71 91 87 64  6  1 \n\nprop.table(table(students$age))\n\n\n      15       16       17       18       19       22 \n0.221875 0.284375 0.271875 0.200000 0.018750 0.003125 \n\n# round(prop.table(table(students$age, students$Walc), 1), 2)\n\n\nmean(students$age)\n\n[1] 16.525\n\n\n\nmedian(students$age)\n\n[1] 16\n\n\n\nmode(students$age)\n\n[1] 16\n\n\n\nstudents %&gt;% \n  ggplot(aes(x=G_mat)) +\n  geom_density() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nmean(students$G_mat)\n\n[1] 11.16979\n\n\n\nmedian(students$G_mat)\n\n[1] 11\n\n\n\ntable(students$Mjob)\n\n\n at_home   health    other services  teacher \n      44       30      116       75       55 \n\nprop.table(table(students$Mjob))\n\n\n at_home   health    other services  teacher \n0.137500 0.093750 0.362500 0.234375 0.171875 \n\nmode(students$Mjob)\n\n[1] \"other\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#quantiles-quartiles-and-percentiles",
    "href": "descriptives.html#quantiles-quartiles-and-percentiles",
    "title": "4  Descriptive statistics",
    "section": "4.3 Quantiles, quartiles and percentiles",
    "text": "4.3 Quantiles, quartiles and percentiles\nWe have found out that the median divides the distribution in half. This is convenient. For example, to estimate below / above what value the first or second half of the data is.\nThese questions come up quite often, and not just about half of the data, but about specific chunks of data. Quantiles are a general term for points that divide a distribution into equal chunks of data—say, ten percent chunks.\nDeciles are special cases of quantiles, 9 points that divide the entire distribution into 10 equal parts. Thus, each part contains 10% of the data.\nPercentiles are special cases of quantiles, 99 points that divide the entire distribution into 100 equal parts, so that each part contains 1% of the data.\nQuartiles are special cases of quantiles, they divide the entire distribution into four parts, 25% of the data each. That is, there are 4 quartiles in total, and they are designated by the letter Q: Q1, Q2, Q3, Q4.\nIt turns out that:\n\nto the left of the first (lower) quartileQ1Q125% of observations are\nto the left of the second (middle) quartile (median)Q2Q250% of observations lie\nto the left of the third (upper) quartileQ3Q375% of observations are\n\nknitr::include_graphics(\"docs/images/quartiles-2.png\")\n\nknitr::include_graphics(\"images/quartiles-2.png\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#measures-of-variability-variability",
    "href": "descriptives.html#measures-of-variability-variability",
    "title": "4  Descriptive statistics",
    "section": "4.4 Measures of variability (variability)",
    "text": "4.4 Measures of variability (variability)\nIn addition to measures of central tendency, there are measures of variability, also known as measures of changeability or dispersion. Variability is the variability of values, how much the data differs from observation to observation within the sample.\n\nLet’s say you find that your red blood cell count is 3.8*10, while the norm is 4*10. How big of a deviation is this?\n\nWhy do we need them? Isn’t it enough to use measures of central tendency to describe a distribution? No, it’s not enough. While measures of central tendency show where our data tends to be, where their “center of mass” is, that is, what single number can characterize our data in general - measures of variability show how much spread there is in our data, how close each of our observations lies to the mean?\nMeasures of central tendency may coincide although measures of variability differ significantly, for example\n\n\n\n\n\n\n\n\n\nImages from Anton Angelhardt’s textbook\nThese distributions are very different, butthey are together after allif you look at their averages, they will be the same\n\n\n\n\n\n\n\n\n\n\n4.4.1 Range, standard deviation and variance\nRange is the difference between the maximum and minimum values in a sample \\(x_{max}-x_{min}\\)\n\nsample_data1\n\n [1] 49 37  1 25 10 36 18 24  7 45 47\n\nrange(sample_data1)\n\n[1]  1 49\n\n\nHere are the minimum and maximum values, to calculate the range, you need to subtract the minimum from the maximum. And as you can see, the range is very sensitive to outliers - if you add the value “3000” to this sample, the range will change dramatically.\nWe have already heard about standard deviation (at least, it appears in the formula of normal distribution). Now let’s get to know it better and try to derive it.\nSince if we calculate the deviation of each observation from the mean and try to average them, we will get zero, we need a different approach to estimating the deviation. If we want to get rid of the problem of getting zero, we usually have two options: take the values ​​by module or square them. To obtain an estimate of the deviation, we used the second option and introduced another important measure of variability for - variance .\nVariance is a measure of variability, which is the square of the standard deviation and is calculated using the formula \\(D = \\dfrac{\\sum_{i=1}^{n}(x_i - \\bar x)^2}{n}\\)\nThe greater the variability in the data, the greater (proportional to the square of the deviations) the dispersion.\nAn important point is the formula for the dispersion for the general population. For sample dispersion, the number of degrees of freedom is reduced by one, so the formula will take the following form \\(D = \\dfrac{\\sum_{i=1}^{n}(x_i - \\bar x)^2}{n-1}\\)\nStandard deviation, also known as root mean square deviation , is the average deviation of observations from their mean. It is calculated as the square root of the variance: \\(sd = \\sqrt{\\dfrac{\\sum_{i=1}^{n}(x_i - \\bar x)^2}{n-1}}\\)\nProperties of variance and standard deviation\nJust like the mean, there are important properties for the variance/standard deviation\n\nIf the same number (constant) is added (or subtracted) from each sample value \\(c\\), then the dispersion and standard deviation will not change: \\(D_(x_i+c) = D\\), \\(sd_(x_i+c) = sd\\)\nIf each sample value is multiplied (or divided) by the same numbercc, then the dispersion will increase (decrease) in \\(c^2\\) times \\(D_(x_i \\times c) = D \\times c^2\\), and the average will increase (or decrease) in \\(c\\) times: \\(sd_(x_i \\times c) = sd \\times c\\)\nIf the values ​​of two samples do not differ from each other, then they have equal variance and standard deviation\n\nUsing student data as an example:\n\nrange(students$age)\n\n[1] 15 22\n\nrange(students$G_mat)\n\n[1]  2.00000 19.33333\n\nrange(students$Walc)\n\n[1] 1 5\n\nrange(students$Mjob)\n\n[1] \"at_home\" \"teacher\"\n\n\n\nsd(students$age)\n\n[1] 1.141687\n\nsd(students$G_mat)\n\n[1] 3.559905\n\nsd(students$Walc)\n\n[1] 1.264167\n\n\nInterquartile range",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#boxplot-and-violinplot",
    "href": "descriptives.html#boxplot-and-violinplot",
    "title": "4  Descriptive statistics",
    "section": "4.5 Boxplot and Violinplot",
    "text": "4.5 Boxplot and Violinplot\n\n\n# knitr::include_graphics(\"docs/images/Boxplot_vs_PDF.png\")\n\n\n\n\n# knitr::include_graphics(\"docs/images/boxplots_hist.png\")\n\n\n\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nstudents %&gt;% \n  ggplot(aes(x=ansences_mat_groups, y = G_mat)) +\n  geom_violin(aes(fill = ansences_mat_groups)) +\n  geom_boxplot(aes(fill = ansences_mat_groups), width=.1) +\n  scale_fill_viridis(discrete=TRUE) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "distributions.html",
    "href": "distributions.html",
    "title": "3  Distributions",
    "section": "",
    "text": "3.1 5.1 Types of distributions\nWhat is distribution? We use this word all the time.\nWhen speaking about distributions, we mean the law of distribution of a random variable – the correspondence between the possible values ​​of this variable from the range of its admissible values ​​and: the probabilities of these values ​​– for a discrete variable, or the probability density – for a continuous variable.\nThe laws of probability distributions can not be derived for all quantities! But it so happened that we can describe some patterns, probability distributions, with formulas, as in physics: the force of gravity (the force with which the Earth attracts all bodies) is directly proportional to the mass of the object, taken with the coefficient of acceleration of free fall \\(F = m*g\\).. We noticed this in relation to the surrounding world and derived a law (not we personally, but representatives of the planet Earth in general). The same is with distributions: we noticed that the probabilities of the distribution of some random variables obey certain laws, and wrote them down. For example, it has been proven that continuous random variables, which are affected by many random factors (for example, height, weight, etc.), are distributed in accordance with the Gaussian distribution , also known as the normal distribution . Its formula is:\n\\(P(x) = \\frac{e^{-(x - \\mu)^{2}/(2\\sigma^{2}) }} {\\sigma\\sqrt{2\\pi}}\\)\nplot(dnorm(1:1000, mean = 500, sd = 100))\nOr, for example, the exponential distribution:\n\\(P(x)= \\lambda \\times e^{-\\lambda x}\\)\nplot(dexp(x = 1:100, rate = 0.1))\n\\(P(x)= \\lambda \\times e^{-\\lambda x}\\)\nplot(dchisq(1:1000, df=100))\nOr, for example, the binomial distribution:\n\\(P(k | n, p) = \\frac{n!}{k!(n-k)!} \\times p^k \\times (1-p)^{n-k} =  {n \\choose k} \\times p^k \\times (1-p)^{n-k}\\)\nplot(dbinom(1:100, size=100, prob=0.5))\nYou can look and be horrified here http://www.math.wm.edu/~leemis/chart/UDR/UDR.html . Fortunately, we won’t need any of this.\nLess scary version https://www.johndcook.com/blog/distribution_chart/#normal",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>distributions</span>"
    ]
  },
  {
    "objectID": "distributions.html#types-of-distributions",
    "href": "distributions.html#types-of-distributions",
    "title": "3  distributions",
    "section": "4.1 5.1 Types of distributions",
    "text": "4.1 5.1 Types of distributions\nThe laws of probability distributions can not be derived for all quantities! But it so happened that we can describe some patterns, probability distributions, with formulas, as in physics: the force of gravity (the force with which the Earth attracts all bodies) is directly proportional to the mass of the object, taken with the coefficient of acceleration of free fall \\(F = m*g\\).. We noticed this in relation to the surrounding world and derived a law (not we personally, but representatives of the planet Earth in general). The same is with distributions: we noticed that the probabilities of the distribution of some random variables obey certain laws, and wrote them down. For example, it has been proven that continuous random variables, which are affected by many random factors (for example, height, weight, etc.), are distributed in accordance with the Gaussian distribution , also known as the normal distribution . Its formula is:\n\\(P(x) = \\frac{e^{-(x - \\mu)^{2}/(2\\sigma^{2}) }} {\\sigma\\sqrt{2\\pi}}\\)\n\nplot(dnorm(1:1000, mean = 500, sd = 100))\n\n\n\n\n\n\n\n\nOr, for example, the exponential distribution:\n\\(P(x)= \\lambda \\times e^{-\\lambda x}\\)\n\nplot(dexp(x = 1:100, rate = 0.1))\n\n\n\n\n\n\n\n\n\\(P(x)= \\lambda \\times e^{-\\lambda x}\\)\n\nplot(dchisq(1:1000, df=100))\n\n\n\n\n\n\n\n\nOr, for example, the binomial distribution:\n\\(P(k | n, p) = \\frac{n!}{k!(n-k)!} \\times p^k \\times (1-p)^{n-k} =  {n \\choose k} \\times p^k \\times (1-p)^{n-k}\\)\n\nplot(dbinom(1:100, size=100, prob=0.5))\n\n\n\n\n\n\n\n\nYou can look and be horrified here http://www.math.wm.edu/~leemis/chart/UDR/UDR.html . Fortunately, we won’t need any of this.\nLess scary version https://www.johndcook.com/blog/distribution_chart/#normal",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>distributions</span>"
    ]
  },
  {
    "objectID": "distributions.html#distribution-functions",
    "href": "distributions.html#distribution-functions",
    "title": "3  Distributions",
    "section": "3.2 5.2 Distribution functions",
    "text": "3.2 5.2 Distribution functions\nAbove we got acquainted with probability density, let’s record all the functions applicable to probability that are useful:\n\nprobability density function for continuous variables (e.g. height, weight) and probability mass function for discrete variables (e.g. number of cases) – the simplest basic function, often denoted by the letterd*\ncumulative distribution function / probability density function for continuous variables, often denoted by the letterp*\nquantile function , also known as the inverse cumulative distribution density function (more on that later), is often denoted by the letter q* probability density function (cumulative distribution function; cdf)\n\nWhy are they needed?\nLet’s look at the example of IQ tests.\nProbability density function\n\niq &lt;- seq(50,150, 0.1)\nplot(iq, dnorm(iq, mean = 100, sd = 15))\n\n\n\n\n\n\n\n\nCumulative distribution function (cdf)\n\nplot(iq, pnorm(iq, mean = 100, sd = 15))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>distributions</span>"
    ]
  },
  {
    "objectID": "r_1.html",
    "href": "r_1.html",
    "title": "6  Introduction to Data Analysis in R",
    "section": "",
    "text": "6.1 14.1 Data and variables\nThis book is written using the R programming and data processing language and the Bookdown package in the RStudio data science environment.\nTo start working in it, you need to download and install the R language itself https://cran.r-project.org/ and download and install RStudio, the environment for work https://posit.co/downloads/ .\nData is information presented in a form suitable for storage and processing by humans or information systems (ISO/IEC/IEEE 24765-2010). If data is presented in a form suitable for processing by information systems, it is formalized.\nA variable is a shell that we define to store data in it and perform operations on it. A variable has a name and the data that it stores.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "r_1.html#data-and-variables",
    "href": "r_1.html#data-and-variables",
    "title": "6  Introduction to Data Analysis in R",
    "section": "",
    "text": "6.1.1 14.1.1 Basic data types\n\nNumeric (whole numbers – integer or numeric , real numbers – real , floating point numbers (fractional) – float )\nText ( character , if one character, or string – many characters)\nLogical ( logical or boolean – accepts only True / False values)\nNA – missing values ​​(not available)\nNaN – not a number, the result of performing an impossible numerical operation (for example, division by 0)\n\n\n\n6.1.2 14.1.2 Basic data structures\n\nVector – a one-dimensional array of one data type\nAn array is a multidimensional array of one data type, consisting of vectors of the same length.\nMatrix - a two-dimensional array of only numeric data\nA list is essentially a multidimensional array, but can consist of vectors of different lengths and have data of different types.\nData, dataframe - essentially a list, but all vectors are the same length\n\n\n\n\n\n\n\n\n\n\n\nhttps://practicum.yandex.ru/blog/10-osnovnyh-struktur-dannyh/\n\nhttps://practicum.yandex.ru/blog/10-osnovnyh-struktur-dannyh/",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "r_1.html#operations-with-variables-and-functions",
    "href": "r_1.html#operations-with-variables-and-functions",
    "title": "6  Introduction to Data Analysis in R",
    "section": "6.2 14.2 Operations with variables and functions",
    "text": "6.2 14.2 Operations with variables and functions\nDepending on what type of variables we are working with, we can perform different operations on them.\nAn operation is the execution of some action on data. What performs this action is called an operator or a function . The difference between them is that an operator performs atomic (single and simple actions), for example, an operator can be a sign of addition, division, greater than or less than, etc. A function does more complex actions: for example, create a vector using the function c(), read data using the function read_csv(), filter data using the function select(). Note that when calling a function, its name is always followed by parentheses.\n\n\n\n\n\n\n\nData type\nPossible atomic operations\n\n\n\n\nNumerical\n= (assignment), +, -, /, *, %\n\n\nText\n= (assignment),+ (concatenation), search for a specific character\n\n\nLogical\n= (assignment),&gt;, &lt;, == (equal), != (not equal)\n\n\n\n\n# vectors  -------------------------------------------------------\nc(1, 2, 3, 4, 5) # c() -- function for vector creation\n\n[1] 1 2 3 4 5\n\n# numeric vectors  -------------------------------------------------------\nc(1, 2, 3, 4, 5, 7, 21, 143)\n\n[1]   1   2   3   4   5   7  21 143\n\nvector1 &lt;- c(1, 2, 3, 4, 5) # vector1 -- variable name, 1, 2, 3, 4, 5 -- arguments of function с()\n\nage &lt;- c(18, 22, 25, 20, 21)\n\n1:10 # function : creates vector from 1 to 10 with step 1\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n10:1\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\nseq(2, 10, 2) # seq(): creates numeric vector from 2 tp 10 and step 2 \n\n[1]  2  4  6  8 10\n\nseq(1, 10, 2) \n\n[1] 1 3 5 7 9\n\n# Character vector -------------------------------------------------------\n\nanswers &lt;- c(\"no\", \"yes\", \"yes\", \"yes\", \"no\")\n\n#Transformation of data types -- TRUE и FALSE became 1 and 0\nc(1, 2, 3, 4, 5, TRUE, FALSE)\n\n[1] 1 2 3 4 5 1 0\n\nc(1, 2, 3, 4, 5, \"1\", \"0\")\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"1\" \"0\"\n\n# Logical vectors ------------------------------------------------------\n\ncondition &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE) #Logical vector\n\nc(1, 2, 3, 4, TRUE, FALSE)\n\n[1] 1 2 3 4 1 0\n\nc(1, 2, 3, 4, 5, 6)\n\n[1] 1 2 3 4 5 6\n\nc(1, 10, 3, -4, \"green\", \"brown\")\n\n[1] \"1\"     \"10\"    \"3\"     \"-4\"    \"green\" \"brown\"\n\n# Logical expressions ------------------------------------------------------\n\na &lt;- 5\nb &lt;- 10\n\na &gt; b\n\n[1] FALSE\n\na &gt;= b\n\n[1] FALSE\n\na == b\n\n[1] FALSE\n\na != b\n\n[1] TRUE\n\na &lt;= b\n\n[1] TRUE\n\na &lt; b\n\n[1] TRUE\n\n#function performs if else: \n# ifelse() involves 3 arguments: ( conditioin; what should it does if condition true ;\n# what should it does if condition false)\n\nifelse(a &lt; b, a+b, \"а more than b\")\n\n[1] 15\n\n# dataframes ---------------------------------------------------------------\n\neye_color &lt;- c(\"green\", \"brown\", \"grey\", \"blue\", \"red\")\n  \ndata &lt;- data.frame(vector1, age, answers, condition, eye_color) # data.frame() creates dataframe from vectors\nView(data) #  View() open this dataframe in the next tab\n\ndata$age # select column from dataframe\n\n[1] 18 22 25 20 21\n\n# описательные визуализации\nhist(data$age, breaks = c(18, 22, 27))\nlines(density(data$age)) \n\n\n\n\n\n\n\n?hist\nplot(density(data$age))\n\n\n\n\n\n\n\n# descriptive statistics\nmean(data$age)\n\n[1] 21.2\n\nsd(data$age)\n\n[1] 2.588436\n\nround(sd(data$age),1) #rounds to 1 digit\n\n[1] 2.6\n\nmin(data$age)\n\n[1] 18\n\nmax(data$age)\n\n[1] 25\n\nrange(data$age)\n\n[1] 18 25\n\nsummary(data$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   18.0    20.0    21.0    21.2    22.0    25.0",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "r_1.html#getting-started-in-the-environment",
    "href": "r_1.html#getting-started-in-the-environment",
    "title": "6  Introduction to Data Analysis in R",
    "section": "6.3 14.3 Getting Started in the Environment",
    "text": "6.3 14.3 Getting Started in the Environment\nIn this tutorial I will provide tasks for training in any data working environment and code for working with data in R.\nFor most interesting operations in R, we will need additional packages - a set of functions that someone has already written for us (the same as, for example, import numpy as npin Python). We will mainly need the package tidyverse, literally - “the universe of clean data”. Let me say right away that everything marked with the # symbol is comments, our hints that will not be executed. It is important to leave them for yourself, so as not to forget what is happening here.\nFor the package to work, it needs to be 1. installed and 2. connected\n\n# install package\ninstall.packages(\"tidyverse\")\n\n\n# switch on package\nlibrary(tidyverse)\n\nYou only need to install the package once after installing R, libraryyou always need to connect via when you open RStudio. An error like “could not find function” in 99% of cases indicates that the package from which it is used is not connected.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "r_1.html#getting-started-with-data",
    "href": "r_1.html#getting-started-with-data",
    "title": "6  Introduction to Data Analysis in R",
    "section": "6.4 14.4 Getting Started with Data",
    "text": "6.4 14.4 Getting Started with Data\nWe will work with the World Happiness Report data for 2016 https://www.kaggle.com/datasets/unsdsn/world-happiness . This is the World Happiness Report https://en.wikipedia.org/wiki/World_Happiness_Report , which shows how residents of different countries rate their level of happiness. They can also be downloaded from the link https://raw.githubusercontent.com/elenary/StatsForDA/main/2016.csv (right-click in the opened file - Save as). The 2016 data we will work with includes 157 countries. By the way, can you download data for different years and see how the number of participating countries changed? A little later we will learn how to do this using code . There are the following variables here:\n\nHappiness Rank - position in the ranking\nHappiness Score - the absolute value of the happiness score\nSE - standard error\nEconomy - GDP per capita\nFamily - Social support, feeling of family\nHealth - Life expectancy\nFreedom - Freedom\nTrust (Goverment Corruption) - perceived level of corruption\nGenerosity - how much is donated to charity\nDystopia is a dystopian country with the lowest indicators in all existing parameters.\n\nEconomy, Family, Health, Freedom, Trust, Generosity – 6 indicators by which the level of happiness is calculated.\nTo perform operations with data, they first need to be read , loaded into the data working environment. In modern traditions of R, this is easiest to do using a function read_csv()from the package tidyverse. Do not forget that any function call is accompanied by parentheses, into which arguments are passed. In the function read_csv()in parentheses, you need to pass the path to the file that we want to read. It can be copied through the file properties. But in order not to bother with the path, the easiest way is to put the data file in the same folder where our file with the code itself is located (for this, you need to save it). And the most elegant and commonly used solution is to create a project File - New Project, and store all files in the project folder, this saves from a large number of errors. Importing files is described in great detail by Ivan Pozdnyakov https://pozdniakov.github.io/tidy_stats/030-import_data.html\n\nwhr &lt;- read_csv(\"2016.csv\") # read data sheet \n\n# View(whr) \n\nNow I want to explore this data. Remember how we start exploring data? With descriptive statistics\nI can select all the variables one by one and calculate the mean and standard deviation for them – this is what is in the Descriptives tab in Jamovi. Since the column does not exist in a vacuum, but inside a dataset, we need to somehow indicate that we are interested in a specific column inside a specific dataset. For now, the easiest way to do this is with the icon $:\n\nwhr$Family # select column\n\n  [1] 1.16374 1.14524 1.18326 1.12690 1.13464 1.09610 1.02912 1.17278 1.10476\n [10] 1.08764 0.99537 1.08383 1.04782 1.02152 1.08113 1.09774 1.03938 1.05249\n [19] 1.16157 1.03999 0.71460 0.86758 1.08672 0.90587 0.98912 1.06612 1.00793\n [28] 0.87114 1.03143 1.09879 1.02169 1.00508 1.04477 0.84829 0.92624 0.87964\n [37] 1.12945 0.83309 0.87119 0.77866 0.87758 0.94397 0.98569 1.03302 1.08268\n [46] 0.80975 0.88025 0.89521 1.16810 1.04167 0.85974 0.68655 1.06054 0.95544\n [55] 0.83132 1.05261 1.04685 0.72194 0.83779 1.06411 1.04993 0.81826 1.05613\n [64] 0.81255 1.03817 0.75695 0.95076 0.95025 0.70697 1.11111 0.72803 1.05163\n [73] 0.96372 0.60809 0.87021 0.33613 0.66062 0.87717 0.87625 0.86216 0.76042\n [82] 0.87877 0.79381 0.90836 0.95434 0.81329 0.64367 0.74173 0.99496 0.38595\n [91] 0.93164 0.26135 0.64184 0.94367 0.78236 0.79117 0.75862 0.43165 0.75473\n[100] 0.75602 1.08983 0.54970 0.64498 0.75596 0.38857 0.63760 0.69699 0.71629\n[109] 0.50163 0.24749 0.62800 0.59205 0.70362 0.62542 0.37932 0.96053 0.84783\n[118] 0.29247 0.69981 0.49813 0.62477 0.76240 1.01413 0.49353 0.80676 0.19249\n[127] 0.47799 0.77416 0.92542 0.84142 0.71478 0.14700 0.81928 0.72368 0.86333\n[136] 0.29561 0.89186 0.60323 0.57576 0.53750 0.66366 0.60530 0.18519 0.63178\n[145] 0.63054 0.90981 0.47493 0.46115 0.77623 0.50353 0.31090 0.61586 0.10419\n[154] 0.11037 0.00000 0.14866 0.23442\n\n\nOr we can remember that a data frame has two dimensions, like a two-dimensional array , and we can access it by index (in square brackets): row number (first number) and column number (second number). If we are not interested in a specific row, but all rows, then nothing is put in place of this index, as if we are skipping it.\n\nwhr[,8] # select column but using other way\n\n# A tibble: 157 × 1\n   Family\n    &lt;dbl&gt;\n 1   1.16\n 2   1.15\n 3   1.18\n 4   1.13\n 5   1.13\n 6   1.10\n 7   1.03\n 8   1.17\n 9   1.10\n10   1.09\n# ℹ 147 more rows\n\n\nPay attention to the output: what data structures do you think the results of the first and second methods belong to?\nLet’s calculate the mean and standard deviation for this column.\n\nmean(whr$Family) # mean\n\n[1] 0.7936211\n\n\n\nsd(whr$Family) # standart deviation\n\n[1] 0.2667057",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "r_1.html#tasks-after-seminar",
    "href": "r_1.html#tasks-after-seminar",
    "title": "6  Introduction to Data Analysis in R",
    "section": "6.5 Tasks after seminar",
    "text": "6.5 Tasks after seminar\n\nRead the 2016 WHR data in the environment you are working in (in R, use the function read_csv()from the package tidyverse).\nCalculate the mean, standard deviation, median, and range (spread from maximum to minimum value) for all 6 indicators that make up the level of happiness (Economy, Family, Health, Freedom, Trust, Generosity). What can you say about them? Where is the largest range? Where are the mean and median close to each other, and where are they not so close? (To calculate the median and range in R, you will need to learn to google a little or use other materials)\nPlot a histogram and a probability density plot for all 6 indicators that make up the level of happiness.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Data Analysis in R</span>"
    ]
  },
  {
    "objectID": "normdist.html",
    "href": "normdist.html",
    "title": "5  Normal distribution and standardization",
    "section": "",
    "text": "5.1 Normal distribution and its properties\nWe have already discussed the normal distribution when we talked about distributions in general [# Distributions]. This is a probability law, where given values of a feature are matched with the probability of encountering a feature with such a value, expressed by the formula\n\\(P(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} * e^{-\\frac{(x - \\mu)^2}{2(\\sigma)^2}}\\)\nIf you look closely at the formula for the normal distribution, you will notice that almost everything in it is constant, except for the variablex x and two unknown parameters:\\(\\mu\\) и \\(\\sigma\\).. These are our already familiar friends: the mean of the general population (the mathematical expectation and standard deviation of the general population ). Thus, in order to construct a distribution for any feature that is normally distributed in the general population, we only need to know two parameters of the distribution :\nIn addition to the formula itself, you can often see the following notation denoting a normal distribution: \\(\\sim \\mathcal{N}(\\mu, \\, \\sigma^2)\\)\nThe figure shows that the mean is responsible for the position of the distribution center, and the standard deviation is responsible for its “stretching” along the axis.XX, the width of the bell.\nImage from Wikipedia\nWhy do we pay so much attention to the normal distribution?\nThe normal distribution has a number of properties :\nThe latter property gives us very great opportunities when working with a normal distribution: for example, having a representative sample of height measurements, we can calculate the probability with which, for example, we will meet a woman with a height of 158 cm. Let’s recall the visualization from the first seminars https://ourworldindata.org/human-height\nTo calculate this probability, we essentially need to enter into the normal distribution formulaxxinsertx=158x=158, and asμμAndσσuse sample means and standard deviations.\nSome percentage values ​​from this distribution are worth remembering because they will appear frequently - these are the values ​​for the amount of data that lies within one or more standard deviations of the mean.\n“Three Sigma Rule” :",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Normal distribution and standardization</span>"
    ]
  },
  {
    "objectID": "normdist.html#normal-distribution-and-its-properties",
    "href": "normdist.html#normal-distribution-and-its-properties",
    "title": "5  Normal distribution and standardization",
    "section": "",
    "text": "average\nstandard deviation\n\n\n\n\n\n\n\nThe normal distribution is interesting to us at least because most of the features we study are random variables that are affected by a large number of random factors - and therefore, according to the central limit theorem , are distributed normally. As a consequence, physical, biological and psychological features (height, weight, expression of personality traits) are usually distributed normally: average features are found in nature more often than more radical ones.\nThe normal distribution is symmetrical and unimodal, and is easier to work with in some operations.\nIn order to estimate the probability of meeting a person of a certain height, we only need to know (or estimate) the mean and standard deviation of the general population - and then we calculate this probability using the normal distribution formula.\n\n\n\nsymmetrically\nunimodal (only one mode)\ndeviations from the mean obey the law of probability: we know what percentage of the data is contained in how many standard deviations from the mean\n\n\n\n\n\n\n\n\nwithin one standard deviation of the mean (\\(\\bar x ± \\sigma\\)) 68% of the values ​​are very frequent values;\nwithin two standard deviations from the mean (\\(\\bar x ± 2\\sigma\\)) contains 95% of the values ​​- the majority of the sample;\nwithin three standard deviations from the mean (\\(\\bar x ± 3\\sigma\\)) is almost 100% of the sample - that is, the entire sample.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Normal distribution and standardization</span>"
    ]
  },
  {
    "objectID": "normdist.html#standard-normal-distribution-z-distribution",
    "href": "normdist.html#standard-normal-distribution-z-distribution",
    "title": "5  Normal distribution and standardization",
    "section": "5.2 Standard Normal Distribution (z-Distribution)",
    "text": "5.2 Standard Normal Distribution (z-Distribution)\nWhat if we need to estimate which probability is higher: to meet a woman who is 158 cm tall or a man who is 174 cm tall? As we can see from the vuzilization above, these distributions are slightly different, and we cannot place these values ​​on one. Here, the standard normal distribution or z-distribution comes to our aid.\nThe standard normal distribution is a normal distribution centered at zero (\\(\\mu=0\\)) and standart distribution equal to 1 (\\(\\sigma=1\\)).\n\n\n\n\n\n\n\n\n\nThis distribution is universal and dimensionless: on the scale we no longer haveσσ, and numbers that have no dimension. This scale is called the z-scale, and the distribution itself is also called the z-distribution. Any normal distribution can be brought to the form of a standard normal - this procedure is called the z-transformation",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Normal distribution and standardization</span>"
    ]
  },
  {
    "objectID": "normdist.html#z-transformation-and-standardization",
    "href": "normdist.html#z-transformation-and-standardization",
    "title": "5  Normal distribution and standardization",
    "section": "5.3 Z-transformation and standardization",
    "text": "5.3 Z-transformation and standardization\nStandardization is the process of transforming a normal distribution to a standard dimensionless Z-score with a mean of 0 and a standard deviation of 1.\nTo achieve standardization, two things need to be done:\n\ncenter the distribution - if it has been shifted away from zero (e.g. \\(\\mu=15\\)), then we need to return it to the center \\(\\mu=0\\)\nnormalize a distribution - get rid of the difference in deviations \\(\\sigma\\), bring our distribution to the distribution with \\(\\sigma=1\\)\n\nGiven these two operations, the z-transform formula is as follows:\n\\(Z_x = \\frac{x - \\mu}{\\sigma}\\)\nThe Z-score can be placed on the Z-distribution and see what percentage of the data lies to the left of the Z-score, i.e. is less likely . This percentage can be calculated using the normal distribution formula by substituting \\(\\mu=0\\) и \\(\\sigma=1\\), but it is easier to use Z-score tables (they are easy to google), for example https://www.z-table.com/\nStandardization does not change the nature of the distribution, it only changes the position of its center and the “flattening” along the h-axis – that is, literally what we know from the properties of the mean and variance\n\n\n\n\n\n\n\n\n\nThe Z-scale can be interpreted as a scale of typicality or frequency of occurrence of values: everything that lies in +- 1 is very common, and the further from the center, the rarer the values ​​we encounter.\nThus, the Z-transform is used to:\n\nFind out how typical (frequent) the value we encounter is\nCompare the probabilities of finding values ​​from two different samples (normally distributed)\n\nFor example, we (somehow) know that the average height of women is 165 cm with a standard deviation of 7, and the average height of men is 178 and a standard deviation of 8.\nReturning to our question:\n\\(W_{158}\\): \\(Z_{158} = \\frac{x - \\mu_w}{\\sigma_w} = \\frac{158 - 165}{7} = -1\\)\n\\(M_{174}\\): \\(Z_{174} = \\frac{x - \\mu_m}{\\sigma_m} = \\frac{174- 178}{8} = -0.5\\)\nWe have converted the height of men and women into Z-scores and can now place them on the Z-distribution. It is already clear that the number -1 is located to the left of the number -0.5, which means that the probability of meeting a woman with a height of 158 cm is less than a man with a height of 174 cm. To estimate this probability accurately, we will have to use the Z-score tables https://www.z-table.com/\n\\(Z_{w_158} = -1\\), which corresponds to approximately 16% probability \\(Z_{m_174} = -0.5\\), which corresponds to approximately 30% probability\nThus, the probability of meeting a woman with a height of 158 cm is 16%, and this is 14% less than the probability of meeting a man with a height of 174 cm.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Normal distribution and standardization</span>"
    ]
  },
  {
    "objectID": "r_2.html",
    "href": "r_2.html",
    "title": "7  Data preprocessing",
    "section": "",
    "text": "7.0.1 Column selection\nAfter we have read the data into a variable (we used a function for this), we often need to preprocessread_csv() the data first . First, the data itself may not be of very good quality, and it needs to be cleaned. Second, we never work with the entire table at once - we select data, for example, a certain column, and often we do not need all the data, but only those that meet certain conditions (for example, we need to select reaction times in a group where subjects consumed caffeine, not a placebo).\nData preprocessing most often includes:\nLet’s go in order.\nAlmost anything can be done in different ways. Most often, there is no right way; if it works, it is right. But some solutions are more optimal in different contexts. Let’s look at different ways of selecting columns. Notice how they differ?\nA comment right away - when we do the assignment operation &lt;-, nothing is output to the console. To see what we assigned to the variable, you can output it by name or, if we are talking about data, look using the View() function. If you are just trying to write an operation - do not rush to assign it to a variable! This way we will immediately see the result in the console, and if it is wrong, it will be clear.\nSelection in basic R by column name\nvar1 &lt;- whr$`Happiness Score`\nvar1\n\n  [1] 7.526 7.509 7.501 7.498 7.413 7.404 7.339 7.334 7.313 7.291 7.267 7.119\n [13] 7.104 7.087 7.039 6.994 6.952 6.929 6.907 6.871 6.778 6.739 6.725 6.705\n [25] 6.701 6.650 6.596 6.573 6.545 6.488 6.481 6.478 6.474 6.379 6.379 6.375\n [37] 6.361 6.355 6.324 6.269 6.239 6.218 6.168 6.084 6.078 6.068 6.005 5.992\n [49] 5.987 5.977 5.976 5.956 5.921 5.919 5.897 5.856 5.835 5.835 5.822 5.813\n [61] 5.802 5.771 5.768 5.743 5.658 5.648 5.615 5.560 5.546 5.538 5.528 5.517\n [73] 5.510 5.488 5.458 5.440 5.401 5.389 5.314 5.303 5.291 5.279 5.245 5.196\n [85] 5.185 5.177 5.163 5.161 5.155 5.151 5.145 5.132 5.129 5.123 5.121 5.061\n [97] 5.057 5.045 5.033 4.996 4.907 4.876 4.875 4.871 4.813 4.795 4.793 4.754\n[109] 4.655 4.643 4.635 4.575 4.574 4.513 4.508 4.459 4.415 4.404 4.395 4.362\n[121] 4.360 4.356 4.324 4.276 4.272 4.252 4.236 4.219 4.217 4.201 4.193 4.156\n[133] 4.139 4.121 4.073 4.028 3.974 3.956 3.916 3.907 3.866 3.856 3.832 3.763\n[145] 3.739 3.739 3.724 3.695 3.666 3.622 3.607 3.515 3.484 3.360 3.303 3.069\n[157] 2.905\n\nstr(var1)\n\n num [1:157] 7.53 7.51 7.5 7.5 7.41 ...\nSelection in base R by index\nAn index is a number of an element in a data structure. We talked about them when we discussed multidimensional arrays: in a one-dimensional structure, for example, a vector, the index will consist of one number, in a two-dimensional structure (for example, a matrix or a data frame) the index consists of two numbers separated by a comma, in a three-dimensional structure it consists of three numbers, and so on. An index in R is always written in square brackets, for example, to find out what is in the second row of the third column, the index of the element will be [2,3]. First comes the row number, then the column number. If we want to output all the rows or all the columns, nothing is put in place of this index. For example, if I want to output all the rows from the third column, I will write[,3]\nvar2 &lt;- whr[,4]\nvar2\n\n# A tibble: 157 × 1\n   `Happiness Score`\n               &lt;dbl&gt;\n 1              7.53\n 2              7.51\n 3              7.50\n 4              7.50\n 5              7.41\n 6              7.40\n 7              7.34\n 8              7.33\n 9              7.31\n10              7.29\n# ℹ 147 more rows\n\nstr(var2)\n\ntibble [157 × 1] (S3: tbl_df/tbl/data.frame)\n $ Happiness Score: num [1:157] 7.53 7.51 7.5 7.5 7.41 ...\nFiltering by name using the tidyverse package\nFirst, a couple of important points about working with the package and the code writing culture tidyverse. The sequence of operations within one task is written line by line with a transfer to the next line in the form of a pipe %&gt;% - a symbol that allows you to use as an argument to the function of the next line what was obtained as a result of executing the previous one. On the first line, the data itself is transferred to the pipe, that is, the name of the variable to which we wrote them. Then, on each subsequent line, the result of executing the previous one will be used as the first argument of the function in brackets. More details about tidyverse https://pozdniakov.github.io/tidy_stats/110-tidyverse_basic.html and about pipes https://pozdniakov.github.io/tidy_stats/110-tidyverse_basic.html\nInternally, tidyversea data structure such as tibble( https://tibble.tidyverse.org/ ) is used. Tibble is a modified dataframe, as we already discussed when discussing data structures.\nOutput to dataframe (tibble):\nvar3 &lt;- whr %&gt;%\n  select(`Happiness Score`)\nvar3\n\n# A tibble: 157 × 1\n   `Happiness Score`\n               &lt;dbl&gt;\n 1              7.53\n 2              7.51\n 3              7.50\n 4              7.50\n 5              7.41\n 6              7.40\n 7              7.34\n 8              7.33\n 9              7.31\n10              7.29\n# ℹ 147 more rows\n\n#смотрим структуру данных\nstr(var3)\n\ntibble [157 × 1] (S3: tbl_df/tbl/data.frame)\n $ Happiness Score: num [1:157] 7.53 7.51 7.5 7.5 7.41 ...\nUsing this function, select()we can extract not just one column, but several:\nwhr %&gt;%\n  select(Country,`Happiness Score`)\n\n# A tibble: 157 × 2\n   Country     `Happiness Score`\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 Denmark                  7.53\n 2 Switzerland              7.51\n 3 Iceland                  7.50\n 4 Norway                   7.50\n 5 Finland                  7.41\n 6 Canada                   7.40\n 7 Netherlands              7.34\n 8 New Zealand              7.33\n 9 Australia                7.31\n10 Sweden                   7.29\n# ℹ 147 more rows\nFiltering by name using the tidyverse package\nNote that the previous output is also a tibble, not a vector. To output to a vector, you need to perform one more step using the function pull(), which sort of “pulls” the values ​​out of the tibble:\nvar4 &lt;- whr %&gt;%\n  select(`Happiness Score`) %&gt;%\n  pull() #вытягивает числовые значения и превращает в числовой вектор\nvar4\n\n  [1] 7.526 7.509 7.501 7.498 7.413 7.404 7.339 7.334 7.313 7.291 7.267 7.119\n [13] 7.104 7.087 7.039 6.994 6.952 6.929 6.907 6.871 6.778 6.739 6.725 6.705\n [25] 6.701 6.650 6.596 6.573 6.545 6.488 6.481 6.478 6.474 6.379 6.379 6.375\n [37] 6.361 6.355 6.324 6.269 6.239 6.218 6.168 6.084 6.078 6.068 6.005 5.992\n [49] 5.987 5.977 5.976 5.956 5.921 5.919 5.897 5.856 5.835 5.835 5.822 5.813\n [61] 5.802 5.771 5.768 5.743 5.658 5.648 5.615 5.560 5.546 5.538 5.528 5.517\n [73] 5.510 5.488 5.458 5.440 5.401 5.389 5.314 5.303 5.291 5.279 5.245 5.196\n [85] 5.185 5.177 5.163 5.161 5.155 5.151 5.145 5.132 5.129 5.123 5.121 5.061\n [97] 5.057 5.045 5.033 4.996 4.907 4.876 4.875 4.871 4.813 4.795 4.793 4.754\n[109] 4.655 4.643 4.635 4.575 4.574 4.513 4.508 4.459 4.415 4.404 4.395 4.362\n[121] 4.360 4.356 4.324 4.276 4.272 4.252 4.236 4.219 4.217 4.201 4.193 4.156\n[133] 4.139 4.121 4.073 4.028 3.974 3.956 3.916 3.907 3.866 3.856 3.832 3.763\n[145] 3.739 3.739 3.724 3.695 3.666 3.622 3.607 3.515 3.484 3.360 3.303 3.069\n[157] 2.905\n\n#смотрим структуру данных\nstr(var4)\n\n num [1:157] 7.53 7.51 7.5 7.5 7.41 ...\nIn this case, we can even calculate the median or average in the same pipe:\nwhr %&gt;%\n  select(`Happiness Score`) %&gt;% #результат -- тиббл\n  pull() %&gt;% #результат -- вектор\n  median()\n\n[1] 5.314\nOtherwise there will be a mistake\nwhr %&gt;%\n  select(`Happiness Score`) %&gt;% #результат -- тиббл\n  median()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "r_2.html#descriptive-statistics-and-aggregation",
    "href": "r_2.html#descriptive-statistics-and-aggregation",
    "title": "7  Data preprocessing",
    "section": "7.1 Descriptive Statistics and Aggregation",
    "text": "7.1 Descriptive Statistics and Aggregation\n\n7.1.1 Descriptive statistics\nWe’ve already calculated some descriptive statistics at the very beginning of working with the data, to make things more fun. Now let’s look at them in more detail.\nThe most common descriptive statistics of the central tendency measures are the mean, median, and mode. We have already constructed the mean and median, but how do we construct the mode?\n\nmean(whr_new$`Happiness Score`, na.rm = TRUE)\n\n[1] 5.321539\n\nmedian(whr_new$`Happiness Score`, na.rm = TRUE)\n\n[1] 5.285\n\nsummary(whr_new$`Happiness Score`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  2.905   4.387   5.285   5.322   6.105   7.526       5 \n\n# mode(whr_new$`Happiness Score`, na.rm = TRUE) #выдаст ошибку\n# mode(whr_new$`Happiness Score`)\n\nIt so happens that R does not have a special function for mode. There is a logical reason for this - the data we mostly work with is quantitative continuous, and we discussed that for this kind of data the concept of mode is a bit artificial. But we still need to calculate the mode sometimes. So you can either write a function to calculate it yourself, or use a ready-made function from the package. I googled what can be done, and I recommend you always do this in such situations - and chose the function mlv()(most likely value) from the packagemodeest\n\n# install.packages(\"modeest\")\nlibrary(modeest)\nmlv(whr_new$`Happiness Score`, na.rm = TRUE)\n\nWarning: argument 'method' is missing. Data are supposed to be continuous. \n            Default method 'shorth' is used\n\n\n[1] 5.239092\n\nmlv(whr_new$Region, na.rm = TRUE)\n\n[1] \"Sub-Saharan Africa\"\n\n\nOr, you can use a simple function table()that counts the number of values ​​of a specified variable.\n\ntable(whr_new$Region)\n\n\n      Australia and New Zealand      Central and Eastern Europe \n                              2                              29 \n                   Eastern Asia     Latin America and Caribbean \n                              6                              24 \nMiddle East and Northern Africa                   North America \n                             19                               2 \n              Southeastern Asia                   Southern Asia \n                              9                               7 \n             Sub-Saharan Africa                  Western Europe \n                             38                              21 \n\n\nIn addition to measures of central tendency, we can calculate measures of variability – minimum and maximum, range, standard deviation, dispersion, interquartile range.\n\nmin(whr_new$`Happiness Score`, na.rm = TRUE)\n\n[1] 2.905\n\nmax(whr_new$`Happiness Score`, na.rm = TRUE) \n\n[1] 7.526\n\nrange(whr$`Happiness Score`) #размах\n\n[1] 2.905 7.526\n\nsd(whr_new$`Happiness Score`, na.rm = TRUE) #стандартное отклонение\n\n[1] 1.108958\n\nvar(whr_new$`Happiness Score`, na.rm = TRUE) #дисперсия\n\n[1] 1.229787\n\nIQR(whr_new$`Happiness Score`, na.rm = TRUE) #межквартильный размах\n\n[1] 1.71825\n\n\nWe also calculated descriptive statistics using the function summary(). There are more advanced functions that calculate a large number of descriptive statistics at once. For example, the skim() function from the skimr package is often used - in addition to measures of central tendency and all four quartiles, it calculates the standard deviation and even builds a small histogram. summary(whr_new$ Happiness Score)\n\nsummary(whr_new$`Happiness Score`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  2.905   4.387   5.285   5.322   6.105   7.526       5 \n\n# install.packages(\"skimr\")\nlibrary(skimr)\nskim(whr_new$`Happiness Score`)\n\n\n\nData summary\n\n\n\n\nName\n\n\nwhr_new$Happiness Score\n\n\n\n\nNumber of rows\n\n\n157\n\n\n\n\nNumber of columns\n\n\n1\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n1\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\nVariable type: numeric\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\ndata\n\n\n5\n\n\n0.97\n\n\n5.32\n\n\n1.11\n\n\n2.9\n\n\n4.39\n\n\n5.28\n\n\n6.1\n\n\n7.53\n\n\n▂▆▇▇▃",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "r_2.html#assignments-after-the-seminar-2",
    "href": "r_2.html#assignments-after-the-seminar-2",
    "title": "7  Data preprocessing",
    "section": "7.2 Assignments after the seminar 2",
    "text": "7.2 Assignments after the seminar 2\n\nFollow all the examples in this section to make sure everything works correctly.\nSee which countries are in places 1-10 and 147-157 (the output should have two columns: Regionand Happiness Rank)\nCompare the average economic performance Economy (GDP per Capita)and happiness levels Happiness Scorein Central and Eastern Europe and Western Europe. What can you say about them? Where is the economic performance higher? Where are people happier?\nCompare the average economic performance Economy (GDP per Capita)and happiness level Happiness Scorein Western Europe and Southern Asia. What can you say about them? Where is the economic performance higher? Where are people happier?\n(For those working in R only) Calculate the average Happiness Scorefor all countries and print the countries that are within ± 1 of the average Happiness Score(the output should have two columns: Regionand Happiness Score) Hint: it might be easier to calculate the average separately and save it in a variable first\nCreate a column that will contain information about whether the current country Happiness Scoreis above or below the average (the column can be filled with, for example, upper and lower values ​​or any other designations)\nCalculate how many NAs are in columns Economy (GDP per Capita), Family, Health (Life Expectancy), Freedom, Trust (Government Corruption), Generosity (the task can be done in different ways, any one that gives the correct answer will do)\nCalculate descriptive statistics for the , , columns Economy (GDP per Capita): Familymean Freedom, median, mode, standard deviation, variance, and interquartile range, and (for those working in R only) calculate values ​​for quartiles 1 and 3. Answers should be different from NA (one way to find out values ​​by quartile is, for example, using separate common functions for descriptive statistics) .",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "6  Statistical inference",
    "section": "",
    "text": "6.1 The idea of ​​statistical inference\nWe finished the introduction to statistics, discussed random variables and probability, and learned how to calculate descriptive statistics: measures of central tendency, where the center of mass of our data tends to be (mean, median, mode), and measures of variability, showing how much our data varies from observation to observation (variance, standard deviation, interquartile range). We needed descriptive statistics to describe our data, to replace a big, incomprehensible table of raw data with specific values, and to make some assumptions about whether there are differences between them if we are talking about a sample from the general population, or some conclusions if our data represents the entire general population.\nIn addition to describing the data, we talked about sample estimates - parameters calculated on our sample in an attempt to estimate these parameters in the general population. As a rule, we are interested in parameters that define the distribution of a feature in a population. Recall that in psychological research, our features are most often random variables, and therefore, according to the central limit theorem, are normally distributed.\nThe ratings we received:\nLet us remember that population and sample estimates are written differently on purpose, so as not to get confused about whether we are talking about a sample or the general population:\nLet’s recall the calculation of the confidence interval:\nHow to make the right conclusion and not screw up?\nGarbage in, garbage out (GIGO).\nWe find it within the framework of frequentist (frequency) statistics - that is, we talk about frequencies and probabilities. We could say little based on the sample estimates of one study: let’s say we studied 100 married couples and calculated that the average time that parents spend with their children in Russia is 15 hours, but how can we understand whether the average in the general population is also 15, and not, for example, the same 14?\nThe idea of ​​statistical hypothesis testing (null hypothesis statistical testing) is based on the fact that we make some assumption about the general population and its parameters: the mean (mathematical expectation), standard deviation or some other parameters, and statistically test this assumption. If we select many samples and calculate their averages, what is the probability of obtaining such or more different from the population sample estimates purely by chance, if in fact this is not the case?\nLet’s return to our example about time spent with children, let’s recall the dataset about Portuguese students\nstudent\nschool\nsex\nage\naddress\nfamsize\nPstatus\nMedu\nFedu\nMjob\nFjob\nreason\nguardian\ntraveltime\nstudytime\nfailures\nschoolsup\nfamsup\npaid_mat\nactivities\nnursery\nhigher\ninternet\nromantic\nfamrel\nfreetime\ngoout\nDalc\nWalc\nhealth\nabsences_mat\nG1_mat\nG2_mat\nG3_mat\npaid_por\nabsences_por\nG1_por\nG2_por\nG3_por\nG_mat\nG_por\nansences_mat_groups\nansences_por_groups\n\n\n\n\nid1\nGP\nF\n18\nU\nGT3\nA\n4\n4\nat_home\nteacher\ncourse\nmother\n2\n2\n0\nyes\nno\nno\nno\nyes\nyes\nno\nno\n4\n3\n4\n1\n1\n3\n6\n5\n6\n6\nno\n4\n0\n11\n11\n5.666667\n7.333333\nmiddle\nless\n\n\nid2\nGP\nF\n17\nU\nGT3\nT\n1\n1\nat_home\nother\ncourse\nfather\n1\n2\n0\nno\nyes\nno\nno\nno\nyes\nyes\nno\n5\n3\n3\n1\n1\n3\n4\n5\n5\n6\nno\n2\n9\n11\n11\n5.333333\n10.333333\nless\nless\n\n\nid4\nGP\nF\n15\nU\nGT3\nT\n4\n2\nhealth\nservices\nhome\nmother\n1\n3\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nyes\n3\n2\n2\n1\n1\n5\n2\n15\n14\n15\nno\n0\n14\n14\n14\n14.666667\n14.000000\nless\nless\n\n\nid5\nGP\nF\n16\nU\nGT3\nT\n3\n3\nother\nother\nhome\nfather\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nno\nno\n4\n3\n2\n1\n2\n5\n4\n6\n10\n10\nno\n0\n11\n13\n13\n8.666667\n12.333333\nless\nless\n\n\nid6\nGP\nM\n16\nU\nLE3\nT\n4\n3\nservices\nother\nreputation\nmother\n1\n2\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nno\n5\n4\n2\n1\n2\n5\n10\n15\n15\n15\nno\n6\n12\n12\n13\n15.000000\n12.333333\nmiddle\nmiddle\n\n\nid7\nGP\nM\n16\nU\nLE3\nT\n2\n2\nother\nother\nhome\nmother\n1\n2\n0\nno\nno\nno\nno\nyes\nyes\nyes\nno\n4\n4\n4\n1\n1\n3\n0\n12\n12\n11\nno\n0\n13\n12\n13\n11.666667\n12.666667\nless\nless\n\n\nid8\nGP\nF\n17\nU\nGT3\nA\n4\n4\nother\nteacher\nhome\nmother\n2\n2\n0\nyes\nyes\nno\nno\nyes\nyes\nno\nno\n4\n1\n4\n1\n1\n1\n6\n6\n5\n6\nno\n2\n10\n13\n13\n5.666667\n12.000000\nmiddle\nless\n\n\nid9\nGP\nM\n15\nU\nLE3\nA\n3\n2\nservices\nother\nhome\nmother\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nyes\nno\n4\n2\n2\n1\n1\n1\n0\n16\n18\n19\nno\n0\n15\n16\n17\n17.666667\n16.000000\nless\nless\n\n\nid10\nGP\nM\n15\nU\nGT3\nT\n3\n4\nother\nother\nhome\nmother\n1\n2\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nno\n5\n5\n1\n1\n1\n5\n0\n14\n15\n15\nno\n0\n12\n12\n13\n14.666667\n12.333333\nless\nless\n\n\nid11\nGP\nF\n15\nU\nGT3\nT\n4\n4\nteacher\nhealth\nreputation\nmother\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nyes\nno\n3\n3\n3\n1\n2\n2\n0\n10\n8\n9\nno\n2\n14\n14\n14\n9.000000\n14.000000\nless\nless\nLet’s ask three questions:\nWe saw the question of statistical significance. Statistical significance is the probability level that our values ​​found in the sample must exceed in order for us to draw a conclusion about the general population . Why can’t we just say that 14 and 15 are obviously different numbers, which means that the values ​​are different? Because we measured these values ​​only in one sample, and the conclusions we draw are based on the probabilities of obtaining these or those values ​​or the frequencies of their occurrence (frequentist statistics!). And we know that if we repeat the study many, many times, the means in them will not be the same, according to the central limit theorem, they will be distributed normally, where the mean will be the mean of the general population (the mathematical expectation).\nIn order to answer the question of whether there are statistically significant differences, it is necessary to test the hypothesis about the differences using the NHST algorithm (null hypothesis statistical testing)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "inference.html#the-idea-of-statistical-inference",
    "href": "inference.html#the-idea-of-statistical-inference",
    "title": "6  Statistical inference",
    "section": "",
    "text": "Formulate a meaningful testable hypothesis\nSelect a representative sample\nCollect quality (good) data on it\nProcess the data according to the quality processing algorithm\n\n\n\n\n\n\n\n\n\nLet’s say we studied families with teenagers in large (GP school) and small cities (MS school) and calculated on our sample that in large cities parents spend an average of 14 hours a week with their children, and in small cities - 15 hours (from the data of the confidence interval task). 1. Is there a statistically significant difference in the time that parents spend with their children in Russia in large and small cities? 2. Is there a statistically significant difference in the frequency of alcohol consumption in families with less supportive relationships and more supportive ones? 3. Is there a statistically significant difference in the average score in mathematics for those who skip classes more or less often?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "inference.html#nhst-hypothesis-testing-and-data-analysis-algorithm",
    "href": "inference.html#nhst-hypothesis-testing-and-data-analysis-algorithm",
    "title": "6  Statistical inference",
    "section": "6.2 8.2 NHST: Hypothesis Testing and Data Analysis Algorithm",
    "text": "6.2 8.2 NHST: Hypothesis Testing and Data Analysis Algorithm\n\nWe formulate an empirical hypothesis (a hypothesis in the language of the possibilities of the study that will be conducted), identify the dependent and independent variables and the nature of the studied relationship between them. We understand what the collected data will look like.\nWe select the conditions under which we will calculate the statistical criterion – the significance level \\(\\alpha\\) (probability of a false positive finding, most often α=0.05) and statistical power \\(power = 1-\\beta\\) (the probability of detecting an effect, if there is one, is most often power=0.8).\nWe select a statistical criterion for testing the hypothesis (for example, t-test, Mann-Whitney test, ANOVA, Kruskal-Wallis test, correlation test, linear regression, etc.).\nWe formulate the null hypothesis \\(H_0\\) (about the absence and differences or connections) and an alternative hypothesis \\(H_1\\) (about the presence of differences or connections)\nBased on α, power, and the effect size of previous studies for the selected statistical criterion, we calculate the required sample size .\n\n– collecting data –\n\nWe select the data analysis environment and pre-process the data.\nFor the selected variables, we calculate descriptive statistics separately ; the variables are visualized using a histogram or a barplot or a density plot.\nWe calculate the statistical criterion : we calculate the key statistics (t-value, F-value, R, etc.) and p-value, we calculate the effect size (Cohen’s d, eta squared, etc.).\nWe draw conclusions and interpret the statistics obtained during the statistical test: we compare the obtained p-value with the selected level \\(\\alpha\\), if p-value &lt; \\(\\alpha\\) – we believe that we have enough evidence to reject the null hypothesis \\(H_0\\), and we decide on the correctness of the alternative hypothesis \\(H_1\\).\nWe draw a conclusion regarding the empirical hypothesis .\nWe visualize the data with a graph illustrating the results (scatterplot with trend line, boxplot, violet plot, etc.).\n\nLet’s look at each step.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "inference.html#empirical-hypotheses-and-variables",
    "href": "inference.html#empirical-hypotheses-and-variables",
    "title": "6  Statistical inference",
    "section": "6.3 Empirical hypotheses and variables",
    "text": "6.3 Empirical hypotheses and variables\nEmpirical hypotheses are those that we can test empirically. To formulate them correctly, we also need to decide on the type of empirical research that is possible to test our theoretical hypotheses. In psychological research, this is most often an experiment, a quasi-experiment, a correlational study (and a longitudinal study as its subtype). Sometimes there is a case study , a study of one specific case, more often in cognitive psychology and neuroscience.\nEmpirical hypotheses are formulated in a form that can be tested for our study, that is, translated into the language of the study, based on our assumed variables and their values.\nLet’s look at some examples. What methods can be used to study these issues?\nCase 1\n\nSuppose a group of scientists wants to study the effect of smoking on the development of mental disorders. What studies could be conducted to investigate this issue?\n\nCase 2\n\nLet’s assume that before the elections to the State Duma, a group of sociologists conducts a public opinion poll to identify preferences for candidates. To do this, call center operators called phone numbers selected randomly from the Moscow phone directory, according to the principle of “every tenth number.” The calls continued for a week during the call center operators’ working hours from 10 to 18. What methods are used in such research? What can be said about them, do they have any shortcomings?\n\nMethods\nAnd let’s return to our situation with the data on Portuguese students.\n\nLet’s say we studied families with teenagers in large (GP school) and small cities (MS school) and calculated on our sample that in large cities parents spend an average of 14 hours a week with their children, and in small cities - 15 hours (from the data of the confidence interval task). 1. Is there a statistically significant difference in the time that parents spend with their children in Russia in large and small cities? 2. Is there a statistically significant difference in the frequency of alcohol consumption in families with less supportive relationships and more supportive ones? 3. Is there a statistically significant difference in the average score in mathematics for those who skip classes more or less often?\n\nThe method used here is a correlational study. If there were any other manipulations in this study with different schools, it would be a quasi-experiment (since we can’t take identical families and tell some of them “spend 15 hours with the kids” and others “spend 14 hours with the kids”, we already have these groups, we don’t do the manipulations ourselves).\nEmpirical hypothesis 1 might sound like this:families in cities spend less time comparatively with families in rural areas\nEmpirical hypothesis 2:studens evaluating their relationship with parents as less supportive (variable famrel, less supportive coded as 1-2), drink more alcohol (variable Walc, values 4-5)\nEmpirical hypothesis 3Studens with more absences will have less grade (avarage amoung G1_mat, G2_mat, G3_mat или G1_por, G2_por, G3_por -- small values)\nLet’s first think about what these hypotheses are, what dependent and independent variables, the presence or absence of what relationship between the dependent and independent variables we want to test?\n\n6.3.1 Dependent and independent variables\nIndependent variables are everything we manipulate during the course of the study, what differences we create in order to obtain a different result. The dependent variable is the target variable, the differences in which we want to obtain by manipulating the independent variables, what we measure. When describing a study, it is always important to write down the PP and NP, along with the empirical (if the method is an experiment, then the experimental) hypothesis. The empirical hypothesis and variables are the heart of our study, the most basic of the language in which the study is described, without understanding what we want to test, nothing will work.\nWhen describing salary and wages, is it important to indicate what scale they belong to? It is important to understand this in order to later choose how to analyze the data.\nFor hypothesis 1\nWe assume that the average time parents spend with their children per week is correlated with school, meaning that the average time will vary across schools:\n\n\\(Time \\sim School\\)\n\n\nDV – average time spent by parents with children per week, quantitative continuous – relationship scale\nIV – school location (urban / rural) – categorical nominative\n\nFor hypothesis 2\n\n\\(Walc \\sim famrel\\)\n\n\nDV – Walc, the amount of alcohol consumed by students on weekends, expressed by a questionnaire with a scale from 1 to 5. DV is an ordinal scale.\nIV – famrel, an assessment of how supportive the relationships in the family are, expressed by a questionnaire with a scale from 1 to 5. IV – ordinal scale.\n\nFor hypothesis 3\n\n\\(?? \\sim ??\\)\n\n\nDV – ?\nIP – ?\n\n\n\n6.3.2 Levels in categorical variables\nLevels in an independent variable (IV) are spoken of in categorical – ordinal or nominative – variables. This is a list of possible values ​​that a categorical IV can take. For example, the conditions “often” and “rarely”, the conditions “well-off family” or “poorly-off”. In the examples above, for the hypothesis, families in cities spend less time comparatively with families in rural areas we are going to compare two groups, and here we can speak of IV levels город– urbanand `rural``.\nDepending on what conclusions we want to draw, we may need a different number of levels in the data. Compare hypotheses: studens evaluatiin their relationship with parents as less supportive (variable famrel, less supportive coded as 1-2), drink more alcohol (variable Walc, values 4-5) and Less assesment of support ( famrel), more alcohol consumption (Walc). How are they different?\nDifferences\nThe statistical method we choose will depend on what conclusion we want to make, whether we want to discover the nature of the relationship between the levels of a categorical variable (usually linear, but it can also be something else, exponential, for example) or whether it is enough for us to compare two groups with each other. We will return to this a little later.\n\n\n6.3.3 Relationship between variables\nWhat kind of connection can we conclude?\nThere are two main types of relationships between variables:\n\nAssociative or correlational\nCause and effect\n\nThree necessary conditions for establishing a cause-and-effect relationship :\n\nThe change in IV occurred before we observed a change in the DV\nChanges in the IV have an associative connection with changes in the DV\nThere are no alternative explanations for the changes in the DV, other than the changes in the IV\n\nAt the moment, we only have survey data, in which students answered questions simultaneously. It turns out that in such a research design, we do not go through the necessary conditions for establishing a cause-and-effect relationship, therefore, we cannot draw a conclusion about a cause-and-effect relationship. We can establish a cause-and-effect relationship only in the course of an experiment or quasi-experiment (it differs from an experiment in that the subjects are not randomly distributed into groups, but groups that already exist in the population are used, for example, different countries). In all other types of research, especially when the relationship of variables in one self-report or questionnaire is studied, we can only talk about an associative or correlational relationship.\nIt often happens that it is quite possible that bad relationships in the family lead to alcoholism in children. But in our sample we only have such data - from the questionnaire, and from the data of that study that we have, we can only judge the presence or absence of a correlation.\nWhen formulating conclusions, these conclusions may differ as follows, compare: - Conclusion 1: “Less supportive family relationships cause alcoholism.” - Conclusion 2: “Less supportive family relationships are associated with a high risk of alcoholism.”\nIn order to test whether there are statistically significant differences between these groups, it is necessary to formulate a null and alternative hypotheses.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "inference.html#null-and-alternative-statistical-hypotheses",
    "href": "inference.html#null-and-alternative-statistical-hypotheses",
    "title": "6  Statistical inference",
    "section": "6.4 Null and Alternative Statistical Hypotheses",
    "text": "6.4 Null and Alternative Statistical Hypotheses\nIn order to conduct a statistical test, it is necessary to define statistical hypotheses - artificially introduced testable statements regarding the general population. They are introduced precisely in order to be able to somehow calculate the data and statistically draw a conclusion based on them about the general population for our small sample. There are two statistical hypotheses, they are mutually opposite: the null (main) and alternative hypotheses.\nNull (main) hypothesis \\(H_0\\) – this is always a hypothesis about the absence of differences in the general population. The hypothesis about the absence of differences between groups (if the hypothesis implies a comparison of groups) or about the absence of a relationship between variables (if the hypothesis is about the relationship of quantitative continuous variables). We try to refute it in statistical testing (yes, exactly refute, not confirm – the conclusions that we can make regarding the null hypothesis are only to reject or not to reject, we cannot confirm it and accept it).\nAlternative hypothesis \\(H_1\\)- a hypothesis opposite toH0H0, that is, the hypothesis about the presence of differences in the general population. \\(P(H_0) + P(H_1) = 1\\)\nWhat about \\(H_0\\) и \\(H_1\\) for our hypotheses?\nFor hypothesis 1\n\n\\(Time \\sim School\\)\n\nFor hypothesis 2\n\n\\(Walc \\sim famrel\\)\n\n\nDV – Walc, the amount of alcohol consumed by students on weekends, expressed by a questionnaire with a scale from 1 to 5. DV is an ordinal scale.\nIV – famrel, an assessment of how supportive the relationships in the family are, expressed by a questionnaire with a scale from 1 to 5. IV – ordinal scale.\n\nFor hypothesis 3\n\n\\(?? \\sim ??\\)\n\n\nDV – ?\nIV – ?\n\n\n6.4.1 Directional and non-directional hypotheses\nAt this stage, it is important for us how the empirical hypothesis is formulated: towards an increase/decrease in the value of the salary in one of the groups or, in general, that the values ​​of the salary in the groups differ in a non-directional manner?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "inference.html#significance-level-and-statistical-power",
    "href": "inference.html#significance-level-and-statistical-power",
    "title": "6  Statistical inference",
    "section": "6.5 Significance level and statistical power",
    "text": "6.5 Significance level and statistical power\nOnce we have sorted out the hypotheses, empirical and statistical, we need to set the criteria on the basis of which we will make decisions about the conclusions we draw from our sample to the general population.\nLevel of significance \\(\\alpha\\) and statistical \\(power = 1-\\beta\\) – these are two of the most important parameters in hypothesis testing. These concepts set the probability framework in which we will conduct the test. The first framework is the probability of obtaining a significant result (significant differences between groups or a relationship between variables) if it is not actually in the general population – a false positive, aka a type I error \\(\\alpha\\).\n\n\n\n\n\n\n\n\n\n\n\nThe second frame is the probability of obtaining an insignificant result, if it actually exists in the general population - a false negative result, also known as a type II error \\(\\beta\\).\n\n\n\n\n\n\n\n\n\n\n\nLevel of significance \\(\\alpha\\), it is also a type I error - to make a false positive conclusion, that is, when there is no effect in the general population, but we conclude that it is there - we set it ourselves (!). In psychology, there is a conventional agreement to consider the level of significance \\(\\alpha = 0.05\\) That is, we simply agreed to consider the 5% probability of making a false positive conclusion as a compromise between being able to make any conclusions at all about the general population, and the fact that 5% of our studies will contain incorrect conclusions (in fact, much more, we will talk about this later).\nBut this is an agreement, and a controversial one at that! In elementary particle physics \\(\\alpha = 0.000003\\)!\nArticle “Justify your alpha” in Nature Human Behaviour https://www.nature.com/articles/s41562-018-0311-x\nThe probability that in the population there actually is a difference between groups or a relationship, but we were unable to detect it in our data \\(\\beta\\), it is also a type II error - this is a type II error, the probability of making a false negative conclusion. And if a type I error \\(\\alpha\\) explicitly appears in hypothesis testing - this is the value with which we compare our resulting p-value, then \\(\\beta\\) does not participate in this testing. That is, we can easily get a false negative conclusion, the absence of results, although in fact they are. Therefore, we need to enter \\(\\beta\\) in testing the hypothesis and minimize such probability. For this purpose, the concept of statistical power of the test was introduced \\(power = 1 - \\beta\\).\nThe level of statistical power is a positive metric, the level of probability at which we are guaranteed that if differences between groups or a relationship between variables exist in the general population, we will be able to find it in our data using our statistical test. If you look at a table of type I and type II errors, statistical power is the inverse of the probability of a type II error, not finding a significant effect if it exists. In psychology, it has become conventional that statistical power is most often taken at the level \\(power = 1 - \\beta\\). Another agreement!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "inference.html#selection-of-statistical-criterion",
    "href": "inference.html#selection-of-statistical-criterion",
    "title": "6  Statistical inference",
    "section": "6.6 Selection of statistical criterion",
    "text": "6.6 Selection of statistical criterion\nHow do you know which test to choose? This is probably one of the most difficult questions in statistics. It is influenced by a large number of nuances, which we will consider in the next section",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "inference.html#sample-size-and-effect-size",
    "href": "inference.html#sample-size-and-effect-size",
    "title": "6  Statistical inference",
    "section": "6.7 Sample size and effect size",
    "text": "6.7 Sample size and effect size\nCalculating the sample size is a critical step because it is responsible for leveling out the type II error and making a false negative conclusion. It may well turn out that due to the small sample, we simply were unable to record differences that actually exist in the general population!\nThis point was often overlooked before: it seemed that the sample size did not require a specific calculation, and it was enough to rely on previous research. It turned out that this is not the case, we will talk about the consequences of this approach in more detail below.\nTo calculate the required sample, we need to know the approximate size of the effect that we can detect, the level of significance \\(\\alpha\\), and statistical power \\(power = 1 - \\beta\\).\nThe effect size is the magnitude of the observed differences. The degree of difference is determined not by the statistics obtained after the statistical criterion is applied (e.g., t-value, F-value), and not by the p-value, but by a separate metric . This metric is calculated using formulas individually for each statistical test. For example, for a t-test, the effect size is \\(Cohen's \\ d\\) or its normalized version \\(Hedges`\\  g\\) (used less frequently in practice). The only exception is the correlation coefficient \\(r\\) – it will be both a statistic and an effect size. For a linear regression with one predictor (factor, also an independent variable), the effect size can also be \\(R^2\\), and for multiple, when there are many predictors (factors or NP), it is better to use the metric \\(Cohen’s \\ f^2\\) or \\(partial \\ \\eta^2 2\\). In psychological research, unless it involves psychophysiology, the effect size is rarely large - usually in the small to medium range. And the smaller the expected effect size, the more observations we need to collect to be able to draw an accurate conclusion about the presence or absence of differences in the general population!\nThere is a good visualization of the effect size for Cohen’s d t-test https://rpsychologist.com/d3/cohend/\nEffect size is typically involved in hypothesis testing at two stages of this algorithm :\n\nTo calculate the sample size at the planning stage of the study - in this case, the effect size from similar studies already conducted is used in order to roughly estimate the possibility of catching the population effect in our sample and collecting the required number (step 5 of this algorithm).\nWhen interpreting a statistical test performed on data. The effect size is one of the key numbers for understanding the results of statistical tests (step 10 of this algorithm)\n\nIf the articles you rely on when planning your study do not indicate the effect size, you can calculate it yourself based on the data provided in the articles: the sample size in the study, the adopted level of statistical power (if not specified, then it is usually taken as 0.8) and the selected level of statistical significance \\(\\alpha\\) (if not specified, then it is usually taken as 0.5).\nTable with effect size metrics and interpretation of their magnitudes on the Cambridge University website and https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize\n\n\n\n\n\n\n\n\n\n\nStat criterion\nEffect size metric\nSmall effect\nAverage effect\nStrong effect\n\n\n\n\nt-test\nCohen′s d,Hedges′ gCohen′s d,Hedges′ g\n0.2\n0.5\n0.8\n\n\nANOVA\nη2,ω2η2,ω2\n0.01\n0.06\n0.14\n\n\nANOVA\nCohen′ fCohen′ f\n0.1\n0.25\n0.4\n\n\nlinear regression (one factor)\nCohen′ fCohen′ f\n0.1\n0.25\n0.4\n\n\nlinear regression (multiple factors)\npartial η2partial η2\n0.02\n0.13\n0.26\n\n\nlinear regression (multiple factors)\nCohen′ fCohen′ f\n0.14\n0.39\n0.59\n\n\ncorrelation test\nrxyrxy\n0.1\n0.3\n0.7\n\n\n\n“Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs” https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full\nEffect size, significance level \\(\\alpha\\), statistical power and sample size are related parameters. Knowing 3 of them, you can always calculate the fourth! https://rpsychologist.com/d3/nhst/\nThis can be done in G*Power https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower or in R, for example, using the package pwr https://cran.r-project.org/web/packages/pwr/pwr.pdf .\nLet’s say we decided to test the hypothesis that students whose parents received a master’s degree do better than those whose parents graduated from college. We chose the level of statistical significance \\(\\alpha = 0.05\\), with which we will compare the p-values ​​that we obtain. We chose the level of statistical \\(power = 0.8\\) (that is, a type II error \\(\\beta = 0.2\\)). We chose to test this hypothesis using a t-test. From a previous similar study, we learned that the effect size in a similar study was Cohen’s d = 0.37. And now we can estimate how much data we need to collect so that if there was a difference between such students in the general population, we could detect it in our data. These 4 values ​​are mathematically related, so knowing any 3, we can calculate the fourth.\n\n\n\n\n\n\n\n\n\nUnderpowered studies are a huge problem for many sciences. “Power failure: why small sample size undermines the reliability of neuroscience” https://www.nature.com/articles/nrn3475\n\nlibrary(pwr)\npwr::pwr.t.test(d=0.37,power=0.8,sig.level=0.05,type=\"two.sample\", alternative=\"greater\")\n\n\n     Two-sample t test power calculation \n\n              n = 91.00624\n              d = 0.37\n      sig.level = 0.05\n          power = 0.8\n    alternative = greater\n\nNOTE: n is number in *each* group\n\n\nSimilarly, calculating the sample size to test the hypothesis that parental education and preparatory course completion are somehow related to student achievement was done by fitting a linear model.\n\nlibrary(WebPower)\nWebPower::wp.regression(n = NULL, p1 = 2, f2 = 0.24, alpha = 0.05, power = 0.8)\n\nPower for multiple regression\n\n           n p1 p2   f2 alpha power\n    43.28562  2  0 0.24  0.05   0.8\n\nURL: http://psychstat.org/regression\n\n\nThe package has a web application https://webpower.psychstat.org/models/reg01/\nAn important point about statistical power: it is precisely this that shows how often the errors will occur \\(p\\)-values &lt; \\(\\alpha\\) if the alternative hypothesis is true (that there are differences or a connection!)\n\n\n\n\n\n\n\n\n\nTo get a detailed understanding of the nuances of effect size and sampling, you can take Lakens’ course https://www.coursera.org/learn/statistical-inferences (it seems that now only with a VPN)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "inference.html#calculation-of-the-static-criterion-and-significance-testing",
    "href": "inference.html#calculation-of-the-static-criterion-and-significance-testing",
    "title": "6  Statistical inference",
    "section": "6.8 Calculation of the static criterion and significance testing",
    "text": "6.8 Calculation of the static criterion and significance testing\nLet’s return to our questions again.\n\nLet’s say we studied families with teenagers in large (GP school) and small cities (MS school) and calculated on our sample that in large cities parents spend an average of 14 hours a week with their children, and in small cities - 15 hours (from the data of the confidence interval task). 1. Is there a statistically significant difference in the time that parents spend with their children in large and small cities? 2. Is there a statistically significant difference in the frequency of alcohol consumption in families with less supportive relationships and more supportive ones? 3. Is there a statistically significant difference in the average score in mathematics for those who skip classes more or less often?\n\nUsing the first question as an example, we will analyze the process of testing the null hypothesis, how any statistical criterion works.\n\\(\\mu\\) = 14, M = 15, n = 100, sd = 4.5\nLet us formulate the null and alternative hypotheses. The null hypothesis is always a hypothesis of no differences, i.e. that the average time parents spend with their children per week does not differ in large and small cities.\n\n\\(H_0\\): \\(\\mu_{urban} = \\mu_{rural}\\)\n\n\n\\(H_1\\): \\(\\mu_{urban} \\neq \\mu_{rural}\\)\n\nAnother important point is to choose the significance level, the boundary value, upon crossing which we will make a decision that we can reject the null hypothesis. Usually\\(\\alpha = 0.05\\)\nNext we begin to test our null assumption, we will try to disprove the null hypothesis.\nSo, we assume that the null hypothesis is true. Further, if we assume that it is true, then \\(\\mu_{urban}\\) и \\(\\mu_{rural}\\) will be at one point. Since we are talking about two averages of different samples, we need a distribution of sample averages on which we can place \\(\\mu_{urban}\\) and \\(\\mu_{rural}\\).\n\n\n\n\n\n\n\n\n\nNext, we need to somehow estimate how much \\(\\mu_{urban}\\) far from from \\(\\mu_{rural}\\). What do we always do when we need to compare which of the values ​​taken from different samples is greater or less than the other? We switch to the standard normal distribution - the Z-score distribution! \\(Z = \\frac{X - \\bar X}{sd}\\)\nMoreover, the standard deviation of this distribution will be the standard error of the mean– the value of the standard deviation for the distribution of sample means.\nLet’s take as the middle of the distribution \\(\\mu_{urban}\\) (arbitrary, we can take any of the averages). To translate \\(\\mu_{rural}\\) in Z-score, first we calculate the standard error of the mean \\(\\mathrm{se} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{sd}{\\sqrt{n}}\\). \\(se = \\frac{4.5}{\\sqrt{10}} = 0.45\\) (we have already calculated it to calculate the confidence interval - the mathematics of these processes are almost identical!)\n\\(Z_{Mrural} = \\frac{\\mu_{rural} - \\mu_{urban}}{se}\\).\n\\(Z_{Mrural} = \\frac{15-14}{0.45} = 2.2\\)\nNow we can arrange \\(\\mu_{rural}\\) on the Z-distribution with the mean at \\(\\mu_{urban}\\) and see how far apart they are.\n\n\n\n\n\n\n\n\n\nWe know that the number 2 in the Z-score distribution corresponds to approximately 96% of the data (we know the data distribution for any normal distribution). And remember that we constructed this distribution based on the assumption that the null hypothesis is equal, that \\(\\mu_{urban} = \\mu_{rural}\\) How then can we interpret the obtained estimates?\nIt turned out that the probability of getting the value \\(\\mu_{rural} = 15\\) or even further from \\(\\mu_{urban}= 14\\) if the assumption is true that \\(\\mu_{urban} = \\mu_{rural}\\) составляет \\(P(M_{rural} | \\mu_{urban} = \\mu_{rural}) = 1 - 0.96 = 0.04\\).This is our p-value , the target statistic we are trying to obtain.\nP-value is the probability (area of ​​the graph under the curve) of obtaining such and even more radical differences (going into the tails) under the condition that the null hypothesis is true (that in fact there are no differences in the general population, and we obtain this result by chance).\nThere is a lot of confusion in the interpretation of p-value, to unravel the confusion you can read these articles:\n\nArticle “A Dirty Dozen: Twelve P-Value Misconceptions” https://sixsigmadsi.com/wp-content/uploads/2020/10/A-Dirty-Dozen-Twelve-P-Value-Misconceptions.pdf\nMisuse of p-values ​​- Misuse of p- values ​​- https://ru.abcdef.wiki/wiki/Misuse_of_p-values ​​https://ru.abcdef.wiki/wiki/Misuse_of_p-values\n\nNext, after we have obtained the p-value, we compare this value with our chosen level \\(\\alpha\\)– the probability that we can accept of making a false positive conclusion, of making a type I error.\n\nIf \\(p \\ value &lt; \\alpha\\):\n\nthe location of our means looks atypical for the null hypothesis - we assumed the means would be at one point, but they moved apart quite a bit\nwe believe that the probability of obtaining such or even stronger differences between the means, given the validity of the null hypothesis, is statistically significantly small\ntherefore we can reject the null hypothesis of no difference\nwe accept the alternative hypothesis that there are differences\n\nIf \\(p \\ value \\ge \\alpha\\):\n\nthe location of our means looks typical for the null hypothesis - the means are not spread out enough, so our data looks like the null hypothesis is true\nwe cannot reject the null hypothesis\n\n\nThe p-value is related, we have already discussed this above in sample size diiscussioin:\n\nSample size: the larger the sample size, the greater the statistical power and the more often and confidently p-values &lt; 0.05 will appear\nVariance: the smaller the variance and the more homogeneous the data, the lower the p-value will be\n\nIn our case: \\(p-value = 0.04 &lt; \\alpha = 0.05\\).\nIn this case, since the p-value is less than alpha, we can say that the probability of obtaining such or even stronger differences between the means is sufficiently small, provided that the null hypothesis is true, so we can reject the null hypothesis of no differences and accept the alternative hypothesis that there are differences between the means. That is, we have tested the statistical hypothesis and concluded that in the general population \\(\\mu_{urban}\\) и \\(\\mu_{rural}\\) are different!\nWhat did we do just now?\n\nWe put forward a null hypothesis about the absence of differences between the groups, which we will reject - that is, that the means of the two groups are at the same point\nFormulated an opposite alternative hypothesis\nWe have stated that we will test the null hypothesis at the significance level \\(\\alpha = 0.05\\)\nPlaced the first mean on the distribution of sample means in the middle and agreed to estimate the actual (in the data) location of the second mean from it\nTo estimate how far the second mean was from the first in our data, we converted everything into Z-scores and calculated the Z-score for the second mean.\nBased on the calculated Z-score, we placed the second mean on the distribution of sample means and calculated what percentage of the data lies beyond this value (p-value)\nComparedp-value \\(p-value\\) with \\(\\alpha\\): it turned out to be less than 0.05, so we rejected the null hypothesis of no differences and concluded that the means in the general population differ.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "criteria.html",
    "href": "criteria.html",
    "title": "7  Statistical criteria",
    "section": "",
    "text": "7.1 Degrees of Freedom\nA statistical criterion is a rule by which we will try to reject the null hypothesis. Different criteria are appropriate for different hypotheses and data.\nThe statistical criterion includes:\nThe number of degrees of freedom (df) is the number of directions for changing a feature. The formula for calculating the degrees of freedom is specific to each statistical criterion (we do not need to calculate it ourselves) and depends, in fact, on the type of statistical criterion, the design of our study (how many comparisons we conduct) and the number of observations in the sample.\nFor the t-criterion, the number of degrees of freedom is calculated very simply: \\(df = n_1 -1 + n_2 - 1 = n_1 + n_2 - 2\\)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical criteria</span>"
    ]
  },
  {
    "objectID": "criteria.html#key-distributions-and-criteria-statistics-z-t-f",
    "href": "criteria.html#key-distributions-and-criteria-statistics-z-t-f",
    "title": "7  Statistical criteria",
    "section": "7.2 Key distributions and criteria statistics (Z, T, F)",
    "text": "7.2 Key distributions and criteria statistics (Z, T, F)\nZ-distribution\nLast time we looked at an example where we compared the average time parents spend with their children in large and small cities, and compared these values ​​using the [Z-distribution] {stat_test_example] example: we got the Z-statistics and found the percentage of data for the point found that corresponded to the Z-score distribution.\n\n\n\n\n\n\n\n\n\n\\(Z = \\frac{M - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} = \\frac{M - \\mu}{\\frac{sd}{\\sqrt{n}}}\\)\nThis is the simplest version of the criterion for testing statistical hypotheses – but not the only one. In fact, there may be others as a distribution on which we place the points (averages) and compare them with each other, depending on the nature of our data. Let’s consider the main ones.\nhttps://gallery.shinyapps.io/dist_calc/\nStudent’s t-distribution\n\n\n\n\n\n\n\n\n\nAs we can see, it is very similar to the Z-distribution, but it has slightly higher tails.\nThe formula that defines the probability law of Student’s T-distribution:\n\n\n\n\n\n\n\n\n\nIt is tacitly assumed that with a sample size of n=30, the t-distribution is considered close to the Z-distribution, which is normal, and we can use it. But in reality, this is not quite true.\nThe formula for calculating the T-value based on our data is:\n\n\\(t = \\frac{M_1 - M_2}{\\frac{sd_{1;2}}{\\sqrt{n_{1;2}}}} = \\frac{M_1 - M_2}{\\sqrt{\\frac{sd_1^2}{n_1} + \\frac{sd_2^2}{n_2}}}\\)\n\nF-distribution\n\n\n\n\n\n\n\n\n\n\\(\\chi^2\\)-distribution",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical criteria</span>"
    ]
  },
  {
    "objectID": "criteria.html#selection-of-statistical-criterion",
    "href": "criteria.html#selection-of-statistical-criterion",
    "title": "7  Statistical criteria",
    "section": "7.3 Selection of statistical criterion",
    "text": "7.3 Selection of statistical criterion\nQuestions that influence the choice of statistical test:\n\nDependent variable: quantitative (interval or ratio scale) or categorical (nominative or ordinal scale)?\nIf the DV is quantitative, is it described by a parametric (usually normal) distribution? (usually determined “by eye” using a probability density graph or QQ graphs, normality tests will almost always give a negative result due to sensitivity and therefore are rarely used in modern data analysis\nHow many independent variables are there?\nAre the IV quantitative (interval and ratio scale) or categorical (nominative and ordinal)?\nIf the IV are categorical and we compare groups, how many groups do we compare?\nIf the IV are categorical and we compare groups, are the data in the groups dependent or not? If not, how much do the data in the groups differ, can we say that the variance of the DV is approximately the same in the groups or not? ( the equality of variances is called Homogeneity of Variance, it is tested using Levene’s test )\n\nThere are a lot of schemes, but they are usually very overloaded and difficult to use, more confusing than helpful.\nBut for some reason I still decided to try to draw my own, and here is the result: https://miro.com/app/board/uXjVOxmKhr8=/?share_link_id=245423331470 (will be updated)\n\n7.3.1 Parametric and nonparametric tests\nIn the list of questions that influence the choice of statistical criterion, the second item is the question of parametrics. What does this mean?\nBy parametric distribution we mean any distribution that can be described by a law. Let’s remember: the distribution law is a formula by which we can give any number the probability of encountering such a value in nature (we discussed this in the topic about distributions). This formula has parameters - unknown variables that we are trying to find out. For a normal distribution, this is the mean of the general population (the mathematical expectation) and the standard deviation of the general population. Let’s look again at the normal distribution formula and find them: \\(P(x) = \\frac{e^{-(x - \\mu)^{2}/(2\\sigma^{2}) }} {\\sigma\\sqrt{2\\pi}}\\)\nLet us also remember that the features that we study in psychological research are mainly random variables , and with a large volume of data, random variables, according to the central limit theorem, are normally distributed. Therefore, in the study of psychological features, the parametric distribution usually means one specific distribution - normal .\nIf the variable that expresses the studied feature is normally distributed, then we can use parametric statistical criteria for its study, which assume normal data distribution: t-test, ANOVA. If the variable is not normally distributed, then we need to use their nonparametric analogues: Mann-Whitney or Wilcoxon test, Kruskal-Wallis test.\nTesting for normality is a fairly well-established routine in the hypothesis testing algorithm. But there is one nuance.\n\nWhen the amount of data is small, it is difficult to understand whether the data is distributed normally or not.\nWith a large amount of data, any deviation from the normal distribution, even the slightest one, will be significant.\n\nTherefore, in the modern research culture, normality testing is no longer given such a sacred meaning. It is more important, for example, to operate with the sample size: if it is large enough (we can return to the empirical rule of n &gt;= 30), then it is enough to look at the data distribution and QQ-plot, and if they do not raise suspicions, feel free to use parametric methods. If the sample is small or if the data with a large sample clearly differ greatly from the bell-shaped Gaussian, then it is better to use a) nonparametric analogues b) generalized linear models (we will talk about them later).\nSo, I would not recommend doing a thorough normality test like the Kolmogorov-Smirnov or Shapiro-Ulik tests (although you can count them, it is not forbidden), but to assess the belonging to the parametric (in this case, normal) distribution as follows:\n\nProbability density plot or histogram: symmetry and kurtosis which were discussed in descriptives\nQQ-plot\n\n\n\n\n\n\n\n\n\n\nGraphs from Andy Field’s book “Discovering Statistics Using R”\nThe top row of graphs shows what graphs might look like in the case of a distribution close to normal: the histogram or probability density plot of the dependent variable looks like a bell-shaped Gaussian, on the QQ plot the theoretical quartiles almost correspond to the actual ones in the data - the more this graph looks like a straight line y = x, the better.\nThe bottom line of the graphs shows the opposite story, what visualizations of the salary distribution might look like, which we will NOT consider normally distributed: the density and probability graph and the histogram are strongly skewed to the left, the QQ plot no longer resembles a straight line.\n\n\n7.3.2 Dependent and independent samples\nAnother point when choosing a statistical criterion is whether our observations are dependent or not. We discussed dependent and independent samples at the beginning. Now this knowledge will be useful to us in order to correctly select a statistical method.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical criteria</span>"
    ]
  },
  {
    "objectID": "group_comparison.html",
    "href": "group_comparison.html",
    "title": "8  Comparison of group averages",
    "section": "",
    "text": "8.1 T-test\nThe first of the statistical tests we will consider is one of the simplest options – the t-test.\nThis is a statistical test whose distribution belongs to the T-distribution family - very similar to the normal distribution, but with higher tails. It is used to compare the means of two groups measured on a metric (quantitative) scale. For other scales, the t-test is not suitable (although features measured on a Likert scale, for example, on a scale from 1 to 5, can sometimes be attributed to quantitative measurements, but we will not touch on this in this course)\nNull and alternative hypotheses:\n\\(H_0\\): \\(\\mu_1 = \\mu_2\\)\n\\(H_1\\): \\(\\mu_1 \\neq \\mu_2\\)\nLet’s return to the data about students and our questions and now consider the following question:\nstudent\nschool\nsex\nage\naddress\nfamsize\nPstatus\nMedu\nFedu\nMjob\nFjob\nreason\nguardian\ntraveltime\nstudytime\nfailures\nschoolsup\nfamsup\npaid_mat\nactivities\nnursery\nhigher\ninternet\nromantic\nfamrel\nfreetime\ngoout\nDalc\nWalc\nhealth\nabsences_mat\nG1_mat\nG2_mat\nG3_mat\npaid_por\nabsences_por\nG1_por\nG2_por\nG3_por\nG_mat\nG_por\nabsences_mat_groups\nabsences_por_groups\n\n\n\n\nid1\nGP\nF\n18\nU\nGT3\nA\n4\n4\nat_home\nteacher\ncourse\nmother\n2\n2\n0\nyes\nno\nno\nno\nyes\nyes\nno\nno\n4\n3\n4\n1\n1\n3\n6\n5\n6\n6\nno\n4\n0\n11\n11\n5.666667\n7.333333\nmiddle\nless\n\n\nid2\nGP\nF\n17\nU\nGT3\nT\n1\n1\nat_home\nother\ncourse\nfather\n1\n2\n0\nno\nyes\nno\nno\nno\nyes\nyes\nno\n5\n3\n3\n1\n1\n3\n4\n5\n5\n6\nno\n2\n9\n11\n11\n5.333333\n10.333333\nless\nless\n\n\nid4\nGP\nF\n15\nU\nGT3\nT\n4\n2\nhealth\nservices\nhome\nmother\n1\n3\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nyes\n3\n2\n2\n1\n1\n5\n2\n15\n14\n15\nno\n0\n14\n14\n14\n14.666667\n14.000000\nless\nless\n\n\nid5\nGP\nF\n16\nU\nGT3\nT\n3\n3\nother\nother\nhome\nfather\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nno\nno\n4\n3\n2\n1\n2\n5\n4\n6\n10\n10\nno\n0\n11\n13\n13\n8.666667\n12.333333\nless\nless\n\n\nid6\nGP\nM\n16\nU\nLE3\nT\n4\n3\nservices\nother\nreputation\nmother\n1\n2\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nno\n5\n4\n2\n1\n2\n5\n10\n15\n15\n15\nno\n6\n12\n12\n13\n15.000000\n12.333333\nmiddle\nmiddle\n\n\nid7\nGP\nM\n16\nU\nLE3\nT\n2\n2\nother\nother\nhome\nmother\n1\n2\n0\nno\nno\nno\nno\nyes\nyes\nyes\nno\n4\n4\n4\n1\n1\n3\n0\n12\n12\n11\nno\n0\n13\n12\n13\n11.666667\n12.666667\nless\nless\n\n\nid8\nGP\nF\n17\nU\nGT3\nA\n4\n4\nother\nteacher\nhome\nmother\n2\n2\n0\nyes\nyes\nno\nno\nyes\nyes\nno\nno\n4\n1\n4\n1\n1\n1\n6\n6\n5\n6\nno\n2\n10\n13\n13\n5.666667\n12.000000\nmiddle\nless\n\n\nid9\nGP\nM\n15\nU\nLE3\nA\n3\n2\nservices\nother\nhome\nmother\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nyes\nno\n4\n2\n2\n1\n1\n1\n0\n16\n18\n19\nno\n0\n15\n16\n17\n17.666667\n16.000000\nless\nless\n\n\nid10\nGP\nM\n15\nU\nGT3\nT\n3\n4\nother\nother\nhome\nmother\n1\n2\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nno\n5\n5\n1\n1\n1\n5\n0\n14\n15\n15\nno\n0\n12\n12\n13\n14.666667\n12.333333\nless\nless\n\n\nid11\nGP\nF\n15\nU\nGT3\nT\n4\n4\nteacher\nhealth\nreputation\nmother\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nyes\nno\n3\n3\n3\n1\n2\n2\n0\n10\n8\n9\nno\n2\n14\n14\n14\n9.000000\n14.000000\nless\nless\nWe have already discussed in the previous chapter that DV here is the average grade in mathematics (column G_mat), IV is the group by amount of absences ( absences_mat). DV is quantitative encoded in the ratio scale, IV we should transform from quantitative to categorical. This means that we can use the branch of statistical tests that is suitable for quantitative DV.\nLet’s go through the algorithm for choosing a statistical test: https://miro.com/app/board/uXjVOxmKhr8=/?share_link_id=245423331470\nIn addition to the main reflected branches, we see that for each criterion there are a number of assumptions.\nAssumptions are statements about the nature of our data, without which parametric tests will not work correctly (that’s why no one likes them).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Comparison of group averages</span>"
    ]
  },
  {
    "objectID": "group_comparison.html#t-test",
    "href": "group_comparison.html#t-test",
    "title": "8  Comparison of group averages",
    "section": "",
    "text": "Is there a statistically significant difference in the average math score between those who skip class more often or less often?\n\n\n\n\n\n\n\n\n8.1.1 Assumptions for the t-test\n\nThe data are normally distributed (or, if the measured variable is a random variable, there are more than 30 observations in groups) – this test was discussed in topic about parametric and non parametric distributiioins.\nThe variances are homogeneous – this is checked using the Levene’s test (Homogeneity of Variance test, also known as Levene’s test).\n\n\n\n8.1.2 1Nonparametric analogues\nIf the assumptions of normality and homogeneity of variances are violated, we cannot use the t-test.\n\n\n\n\n\n\n\n\n\n\n\n8.1.3 Independent and paired tests\nHere, too, everything is quite simple:\n\nIf we have independent samples, then we use the independent t-test or its nonparametric analogue – the Mann-Whitney test.\nIf the samples are dependent, then we use the paired t-test or its nonparametric analogue, the Wilcoxon test.\n\nThe abundance of names can be intimidating, but in essence, it is the same test with minor adjustments - it’s just that more statisticians are immortalized under different names!\n\n\n8.1.4 Computing the t-test and nonparametric analogues\nThe mean score and standard deviation in the groups of those who skip the least (in class we called them parishioners):\n\n\n# A tibble: 1 × 3\n   mean    sd     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1  11.2  3.70   207\n\n\nAverage score in truant groups:\n\nstudents %&gt;% \n  filter(absences_mat_groups != \"middle\") -&gt; students_2\n\nt.test(students_2$G_mat ~ students_2$absences_mat_groups, paired = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  students_2$G_mat by students_2$absences_mat_groups\nt = 1.2839, df = 26.204, p-value = 0.2104\nalternative hypothesis: true difference in means between group less and group more is not equal to 0\n95 percent confidence interval:\n -0.6112609  2.6475660\nsample estimates:\nmean in group less mean in group more \n          11.23027           10.21212",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Comparison of group averages</span>"
    ]
  },
  {
    "objectID": "group_comparison.html#anova-analysis-of-variance",
    "href": "group_comparison.html#anova-analysis-of-variance",
    "title": "8  Comparison of group averages",
    "section": "8.2 ANOVA (analysis of variance)",
    "text": "8.2 ANOVA (analysis of variance)\nANOVA (ANalysis Of VAriance) is a statistical criterion whose distribution and key statistics belong to the F-distribution family. This is a slightly skewed distribution to the left, which is already very different from the normal one. On this distribution, we (or rather specially trained programs) will place the F-value, which will be calculated. This criterion is used when the dependent variable (DV) is measured on a metric (quantitative) scale, and the independent variables (IV) are categorical (ordinal or nominative). ANOVA is used when the number of comparisons is greater than two - that is, either it is one IV, which can take three values ​​(levels), or the number of IV is greater than or equal to two.\nANOVA is used when the number of comparisons we need to make becomes greater than two . It is possible to make two comparisons with ANOVA, but this would be pointless, since it is essentially a good old t-test.\nNull and alternative hypotheses for ANOVA:\n\n\\(H_0\\): \\(\\mu_1 = \\mu_2 = ... =\\mu_n\\)\n\n\n\\(H_1\\): There is at least one inequality: \\(\\mu_1 \\neq \\mu_2 \\neq ... \\neq \\mu_n\\)\n\nHow is it that we need to make more than two comparisons?\n\n8.2.1 Factors and Levels\nA factor or predictor in linear models and ANOVA is an independent variable.\nTypically, we use ANOVA to compare groups, so the independent variable is categorical, it takes on a finite number of values. The NP values ​​are the groups that we compare with each other, they are called IV levels or factor levels .\n\n\n\n\n\n\n\n\n\nDepending on the number of IVs and IV’s levels, the experimental design and the design for ANOVA can be specified:\n\n\n\n\n\n\n\n\n\n\n\n8.2.2 Why is it variance?\nTo proceed directly to the analysis, we need to understand a little about what kind of method this is and why it is called variance, although we make assumptions, as in most statistical criteria, regarding the average values ​​by groups. The fact is that this method really takes into account variance, or more precisely, the difference in variances The mathematics of ANOVA is based on the fact that when combining several samples with approximately the same variance, but different averages, the variance increases proportionally to the average of these values. This is due to the fact that all variance can be divided into between-group and within-group. If it turns out that the variance between groups is greater than within, then we can draw a conclusion in favor of differences between the groups. In order to record differences between groups, the intra-group variance here should be as small as possible: the smaller it is within groups and the larger it is between groups, the more serious the conclusion about the differences in the groups we can make.\nThe F-value is essentially calculated as the ratio of the variances of the two groups.\nAs a variance for ANOVA mathematics, usually only its numerator is taken, without dividing by the number of elements in the group (and we calculated the variance itself). The numerator of the variance without the denominator is the sum of squares (Sum of Squares)\n\n\\(D = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})^2}{n}\\)\n\nLet’s also remember that the standard deviation is the root of the variance:\n\n\\(\\sigma = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})}{\\sqrt{n}}\\)\n\nAnd the sum of squares is the numerator of the variance, SST (Sum of Squares Total):\n\n\\(SST = \\sum_{i=1}^{n} (x_i - \\overline{x})^2\\)\n\nThe total sum of squares SST consists of the between-group sum of squares (SSE, Sum of Squares Explained or SSB, Sum of Squares Between groups) and the within-group sum of squares (SSR, Sum of Squares Random or SSW, Sum of Squares Within groups)\n\n\n\n\n\n\n\n\n\n\n\\(SST = SSE + SSR\\)\n\nSuppose we have m groups with n observations in each (for simplicity, let’s take equal-sized groups). Then the total sum of squares (variance without denominator) is:\n\n\\(SST = \\sum_{i=1}^{m \\times n} (x_i - \\bar x)^2\\)\n\nHow to calculate between-group and within-group sum of squares?\n\n\\(SSE = \\sum_{j=1}^m (\\bar x_j - \\bar x)^2\\)\n\n\n\\(df_{SSE} = m-1\\), m – количество групп\n\n\n\\(SSR = \\sum_{j=1}^m\\sum_{i=1}^n (x_{ij} - \\bar x_j)^2\\)\n\n\n\\(df_{SSR} = m \\times n - m\\), n – number of elements in a group, m – number of groups\nThe desired result here is the lowest SSR (within-group) and the highest SSE (between-group).\nFrom this, one of the important indicators for interpreting ANOVA results logically follows:\n\\(R^2 = \\frac{SSE}{SST} = 1 - \\frac{SSR}{SST}\\)– the percentage of variance explained by factors ( the coefficient of determination ), that is, how well our factors explain the variability of the data.\nAnd there is one step left to calculate the F-value - calculate the average sums of squares, that is, divide them by the number of degrees of freedom for each sum of squares (just as we divided the variance by the number of observations! Let’s return to its meaning)\n\n\\(MSE = \\frac{SSE}{df_{SSE}} = \\frac{SSE}{m-1}\\)\n\n\n\\(MSR = \\frac{SSR}{df_{SSR}} = \\frac{SSR}{m \\times n - m}\\)\n\nAnd we can calculate the F-value, which we will place on the F-distribution (let me remind you that specially trained machines usually do this for us)\n\\(F = \\frac{MSE}{MSR} = \\frac{SSE \\times (m \\times n - m)}{(m-1) \\times SSR}\\)\n\n\n\n8.2.3 F-distribution (Fisher)\n\n\n\n\n\n\n\n\n\nF-distribution or Fisher distribution is a distribution of a value that is calculated as the ratio of the mean squares of the intra- and inter-group variability (very similar to variance) and includes the calculated number of degrees of freedom for the intra- and inter-group variability. With a very large sample, a large value of degrees of freedom, it will also resemble normal!\nWith a small number of degrees of freedom (small sample), the F-distribution is visually different from the normal one - but for testing the assumption this does not matter at all, since this is the distribution of the F-statistics, not the dependent variable - the assumption of normality or large sample size still applies to them.\n\n\n8.2.4 Assumptions for ANOVA\nLet’s go through the algorithm for choosing a statistical test: https://miro.com/app/board/uXjVOxmKhr8=/?share_link_id=245423331470\nIn addition to the main reflected branches, there are a number of assumptions for each criterion.\nAssumptions are statements about the nature of our data, without which parametric tests will not work correctly (that’s why no one likes them).\n\nThe data are normally distributed (or, if the measured variable is a random variable, there are more than 30 observations in groups) – this test was discussed\nIf independent, the variances are homogeneous, checked using Levene’s Test of Homogeneity of Variance; if dependent, the groups are spheric, checked using the Sphericity test (Mauchly test).\n\n\n\n8.2.5 Nonparametric analogues\nIn case the assumptions of normality (remember that this is more about a large number of observations in the sample and a visually significant difference from the Gaussian) and homogeneity of variances are violated, we cannot use ANOVA, and we need to use nonparametric analogs.\n\n\n\n\n\n\n\n\n\n\n\n8.2.6 Simple ANOVA and Repeated Measures\nHere, too, everything is quite simple:\n\nIf we have independent samples, then we use ANOVA or its nonparametric analogue – the Kruskal-Wallis test.\nIf the samples are dependent, then we use ANOVA with repeated measures or its nonparametric analogue – the Friedman test.\n\nThe abundance of names can be intimidating, but in essence, it is the same test with minor adjustments - it’s just that more statisticians are immortalized under different names!\n\n\n8.2.7 Nonlinear effects\nA major advantage of comparing three or more groups is that such comparisons allow one to identify nonlinear effects.\n\n\n\n\n\n\n\n\n\n\n\n8.2.8 Multivariate ANOVA and Factor Interactions\nMultivariate ANOVA is the same as ANOVA, only when we have more than one independent variable (factor) . Multivariate ANOVA is usually written something like this:\\(ANOVA \\ 2 \\times 3\\) – this means that we have two factors, 2 levels in the first factor and 3 levels in the second.\nMultifactorial ANOVA is interesting because it allows us to study the interaction of these NPs. The nonlinearity of one factor may overlap with the nonlinearity of another, or the effect of a factor may only manifest itself under certain special conditions - this requires a multifactorial design. For example, we study the same concentration of caffeine on attention, give the subjects 1 or 2 cups of coffee to drink, and also want to study this effect depending on the time of day when the subjects drink coffee. It may turn out that the effect of caffeine on attention increases with increasing coffee concentration, but this only happens in the morning hours - and in the evening no difference in attention is found, 1 and 2 cups of coffee act absolutely the same. This is called an interaction factor and manifests itself when we have 2 or more NPs in the study.\nVisually, the interaction manifests itself as follows: if you draw a graph of the dependent variable from one of the factors and place a line on this graph corresponding to another factor, then if the lines are not parallel, then we can talk about the presence of interaction.\n\n\n\n\n\n\n\n\n\nIn the case of several factors (independent variables), the calculation and logic of using ANOVA is exactly the same, only one more factor is added and the interaction of the first factor with the second:\n\n\\(SST = SSE_{factor1} + SSE_{factor2} + SSE_{factor1} \\times SSE_{factor2} +SSR\\)\n\n\n\n8.2.9 Post-hocs and multiple comparisons\nLet’s say we performed ANOVA (any of its types), compared the resulting p-value withαα, and it turned out that the p-value &lt; \\(\\alpha\\), and we can reject the null hypothesis \\(H_0\\) in favor of an alternative \\(H_1\\). Does this mean that the means are different in all groups? Or could it be that \\(\\mu_1 = \\mu_2\\) и \\(\\mu_1 \\neq \\mu_3\\)? Maybe.\nANOVA says that in some groups there are significant differences between the means, but it does not say which ones. And to find out in which groups there are significant differences, you need to conduct a series of post hoc, a posteriori tests. These are simply paired t-tests ( [we analyzed them ]) to compare each IV level with each other, but already with an adjustment for multiple comparisons .\nWhat are these corrections? We need them to avoid increasing the error of the first kind. It turns out that if we test several hypotheses on the same data, then the probability of accidentally obtaining statistically significant differences will increase proportionally to the number of hypotheses tested, that is, comparisons made! This happens because an important assumption about the independence of our conclusions is violated.\nAnd the probability of making a false positive conclusion will no longer be 0.05, but much higher. To avoid this, corrections are introduced for multiple comparisons, which adjust the level \\(\\alpha\\) and underestimate it.\nThe most popular amendments:\n\nBonferronni\nTukey\nFDR\n\nPairwise post hoc tests are needed when the ANOVA results are significant – this is the next step to understand which levels of factors or their interactions contributed to the significance. If the ANOVA is not significant, then subsequent pairwise comparisons are not needed – we have nothing significant at all (remember that the null hypothesis for ANOVA is the presence of at least one different mean), so there is nothing to compare.\n\n\n8.2.10 Computing ANOVA and Nonparametric Analogues\nLet’s look at the data again and examine the relationship between the math score (variable G_mat) and the mother’s work (variable Mjob).\n\n\n\n\n\n\nstudent\nschool\nsex\nage\naddress\nfamsize\nPstatus\nMedu\nFedu\nMjob\nFjob\nreason\nguardian\ntraveltime\nstudytime\nfailures\nschoolsup\nfamsup\npaid_mat\nactivities\nnursery\nhigher\ninternet\nromantic\nfamrel\nfreetime\ngoout\nDalc\nWalc\nhealth\nabsences_mat\nG1_mat\nG2_mat\nG3_mat\npaid_por\nabsences_por\nG1_por\nG2_por\nG3_por\nG_mat\nG_por\nabsences_mat_groups\nabsences_por_groups\n\n\n\n\nid1\nGP\nF\n18\nU\nGT3\nA\n4\n4\nat_home\nteacher\ncourse\nmother\n2\n2\n0\nyes\nno\nno\nno\nyes\nyes\nno\nno\n4\n3\n4\n1\n1\n3\n6\n5\n6\n6\nno\n4\n0\n11\n11\n5.666667\n7.333333\nmiddle\nless\n\n\nid2\nGP\nF\n17\nU\nGT3\nT\n1\n1\nat_home\nother\ncourse\nfather\n1\n2\n0\nno\nyes\nno\nno\nno\nyes\nyes\nno\n5\n3\n3\n1\n1\n3\n4\n5\n5\n6\nno\n2\n9\n11\n11\n5.333333\n10.333333\nless\nless\n\n\nid4\nGP\nF\n15\nU\nGT3\nT\n4\n2\nhealth\nservices\nhome\nmother\n1\n3\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nyes\n3\n2\n2\n1\n1\n5\n2\n15\n14\n15\nno\n0\n14\n14\n14\n14.666667\n14.000000\nless\nless\n\n\nid5\nGP\nF\n16\nU\nGT3\nT\n3\n3\nother\nother\nhome\nfather\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nno\nno\n4\n3\n2\n1\n2\n5\n4\n6\n10\n10\nno\n0\n11\n13\n13\n8.666667\n12.333333\nless\nless\n\n\nid6\nGP\nM\n16\nU\nLE3\nT\n4\n3\nservices\nother\nreputation\nmother\n1\n2\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nno\n5\n4\n2\n1\n2\n5\n10\n15\n15\n15\nno\n6\n12\n12\n13\n15.000000\n12.333333\nmiddle\nmiddle\n\n\nid7\nGP\nM\n16\nU\nLE3\nT\n2\n2\nother\nother\nhome\nmother\n1\n2\n0\nno\nno\nno\nno\nyes\nyes\nyes\nno\n4\n4\n4\n1\n1\n3\n0\n12\n12\n11\nno\n0\n13\n12\n13\n11.666667\n12.666667\nless\nless\n\n\nid8\nGP\nF\n17\nU\nGT3\nA\n4\n4\nother\nteacher\nhome\nmother\n2\n2\n0\nyes\nyes\nno\nno\nyes\nyes\nno\nno\n4\n1\n4\n1\n1\n1\n6\n6\n5\n6\nno\n2\n10\n13\n13\n5.666667\n12.000000\nmiddle\nless\n\n\nid9\nGP\nM\n15\nU\nLE3\nA\n3\n2\nservices\nother\nhome\nmother\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nyes\nno\n4\n2\n2\n1\n1\n1\n0\n16\n18\n19\nno\n0\n15\n16\n17\n17.666667\n16.000000\nless\nless\n\n\nid10\nGP\nM\n15\nU\nGT3\nT\n3\n4\nother\nother\nhome\nmother\n1\n2\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nno\n5\n5\n1\n1\n1\n5\n0\n14\n15\n15\nno\n0\n12\n12\n13\n14.666667\n12.333333\nless\nless\n\n\nid11\nGP\nF\n15\nU\nGT3\nT\n4\n4\nteacher\nhealth\nreputation\nmother\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nyes\nno\n3\n3\n3\n1\n2\n2\n0\n10\n8\n9\nno\n2\n14\n14\n14\n9.000000\n14.000000\nless\nless\n\n\n\n\n\n\n\n\nWe formulate an empirical hypothesis. We have already discussed that the CP here is G_mat, quantitative continuous. NP is Mjob, categorical nominative, includes 5 levels.\n\n\nG_mat ~ Mjob\n\n\nWe formulate the null hypothesis:\n\n\n\\(H_0\\): \\(\\mu_r = \\mu_u\\)\n\n\n\\(H_1\\): \\(\\mu_r \\neq \\mu_u\\)\n\n\nLet’s fix that we will test the hypothesis at the level \\(\\alpha = 0.05\\)\nLet’s choose a statistical criterion for testing. Let’s test the normality of the PP and compare the variances of the two samples.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn our case, the criterion is a one-way ANOVA with independent samples. Let’s calculate it\n\n\nlibrary(\"ez\")\nmodel_abs_mat_ez &lt;-  ezANOVA(data = students, dv = G_mat, wid = student,\n                           between = Mjob)\nmodel_abs_mat_ez\n\n$ANOVA\n  Effect DFn DFd        F          p p&lt;.05        ges\n1   Mjob   4 315 4.132617 0.00280909     * 0.04986108\n\n$`Levene's Test for Homogeneity of Variance`\n  DFn DFd      SSn      SSd         F         p p&lt;.05\n1   4 315 12.53941 1434.182 0.6885308 0.6003484      \n\n\n\nHow to interpret the results? The key statistic for us is the F-value\n\nAnother good and more detailed chapter (even two) about ANOVA was written by my friend and colleague Anton Angelgardt, if you are interested in going deeper, you can read his material. https://angelgardt.github.io/SFDA2022/book/oneway-anova.html\n\nmodel_abs_mat_ez &lt;-  ezANOVA(data = students, dv = G_mat, wid = student,\n                           between = Mjob)\nmodel_abs_mat_ez\n\n$ANOVA\n  Effect DFn DFd        F          p p&lt;.05        ges\n1   Mjob   4 315 4.132617 0.00280909     * 0.04986108\n\n$`Levene's Test for Homogeneity of Variance`\n  DFn DFd      SSn      SSd         F         p p&lt;.05\n1   4 315 12.53941 1434.182 0.6885308 0.6003484      \n\nsummary(model_abs_mat_ez)\n\n                                          Length Class      Mode\nANOVA                                     7      data.frame list\nLevene's Test for Homogeneity of Variance 7      data.frame list",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Comparison of group averages</span>"
    ]
  },
  {
    "objectID": "linear_comparison.html",
    "href": "linear_comparison.html",
    "title": "9  Checking the linear link",
    "section": "",
    "text": "9.1 Correlation analysis\nBefore this, we considered types of statistical analysis when it was necessary to compare average values ​​in several groups. The dependent variable was always quantitative (we compared its average value across groups), and the independent variable was categorical, took a finite number of values, and each of its values was a separate IV level, a separate group.\nNow we turn to statistical tests that are used when both variables, DV and IV, are quantitative.\nCorrelation is a relationship between variables. Although it has the same name as one of the two types of relationships between variables, correlation can be identified using basically any type of analysis – after all, when we get the results of statistical tests, we only understand that two variables are related (or not), but we cannot conclude whether this is a cause-and-effect relationship or a correlation.\nHere we will talk specifically about correlation analysis – a special type of analysis for determining the significance of a linear relationship between only two quantitative or ordinal variables.\nTo derive the formula and meaning of correlation, let’s get acquainted with the concept of covariance.\nCo-variance is a measure of the co-variability of data, an indicator of how observations on two quantitative variables vary relative to each other.\npic from here\n\\(\\text{cov}(x,y)=\\frac{\\sum_{i=1}^n (x_i - \\bar x) (y_i - \\bar y )}{n-1}\\)\nShock content: try to calculate the covariance of a variable with itself and look at the resulting formula: does it remind you of anything?\nThe correlation coefficient is an indicator of the strength and direction of the relationship between variables. The magnitude of the number is responsible for the strength of the relationship, and the sign of the correlation is responsible for the direction. In essence, this is the covariance of variables, but weighted by the standard deviations of these variables. This is done in order to standardize the coefficient, move from absolute values ​​to relative ones, and place this coefficient within the limits of [-1;1]. For the Pearson correlation coefficient (correlation of two quantitative variables):\n\\(\\text{corr}(x,y) = r_{xy} = \\frac{\\text{cov(x, y)}}{sd_x sd_y} = \\frac{\\sum_{i=1}^n (x_i - \\bar x) (y_i - \\bar y )}{(n-1)sd_x sd_y}\\)\nThe coefficient of determination is an indicator of the extent to which the variability of the data is explained by this selected independent variable. If we have only one NP, then the coefficient of determination is practically the same as the correlation, only taken squared:\n\\(R^2 = r_{xy}^2 = \\frac{\\text{cov(x, y)}}{sd_x sd_y} = \\frac{\\sum_{i=1}^n (x_i - \\bar x) (y_i - \\bar y )}{(n-1)sd_x sd_y}\\)\nExample from the site https://rpsychologist.com/correlation/\nGuess the Correlation Game: http://guessthecorrelation.com/",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Checking the linear link</span>"
    ]
  },
  {
    "objectID": "linear_comparison.html#correlation-analysis",
    "href": "linear_comparison.html#correlation-analysis",
    "title": "9  Checking the linear link",
    "section": "",
    "text": "Covariance with itself\n\n\n\\(\\text{cov}(x,x)=\\frac{\\sum_{i=1}^n (x_i - \\bar x) (x_i - \\bar x )}{n-1} = \\frac{\\sum_{i=1}^n (x_i - \\bar x )^2}{n-1}\\)\n\nThis is variance!\n\n\n\n\n\n\n\n\n\n\n9.1.1 Correlation test\nHypotheses about the presence of a linear relationship between variables are tested using a correlation test. This is exactly the same statistical criterion as those we have already discussed. In essence, it is exactly the same as linear regression with one variable. The correlation test is used when both the PP and NP are quantitative variables or expressed in an ordinal scale (but not a nominative one). For a quantitative scale, the Pearson correlation coefficient is usually used, for an ordinal or quantitative variable with a small number of observations - the Spearman correlation coefficient.\nThe correlation test uses – you won’t believe it – the already familiar Student’s T-distribution! ( that is, we only need to remember two distributions: the T-distribution and the F-distribution )\nThe number of degrees of freedom is calculated using the formula\n\n\\(df = n - 2\\), n – number of observations\n\n,\nNull and alternative hypotheses for the correlation test:\n\n\\(H_0\\): \\(r_{xy} = 0\\)\n\n\n\\(H_1\\): \\(r_{xy} \\neq 0\\)\n\nLike other criteria, it has assumptions.\n\n\n9.1.2 Assumptions for the Correlation Test\n(DV and IV are measured on a quantitative or ordinal scale)\n\nThe distribution of NP by PP is linear - there is no picture of non-linear relationships or clusters of data in different places.\nThe salary is normally distributed (not necessarily strictly consistent) and there are no noticeable outliers - this check was discussed here\n\nExamples of what a nonlinear distribution might look like:\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.3 Nonparametric analogues\nIf the PP deviates greatly from the normal distribution, or the sample is small, or the PP is coded on an ordinal scale, the correlation test uses the Spearman correlation coefficient instead of the Pearson, and this is the only difference.\nThere is also Kendall’s tau, which is almost the same as Spearman’s correlation, but we will not consider it, since it is used extremely rarely.\n\n\n\n\n\n\n\n\n\n\n\n9.1.4 Calculating the Correlation Test\nLet’s test the following hypothesis.\n\nstudens evaluating their relationship with parents as less supportive (variable famrel, less supportive coded as 1-2), drink more alcohol (variable Walc, values 4-5)\n\n\n\n\n\n\n\nstudent\nschool\nsex\nage\naddress\nfamsize\nPstatus\nMedu\nFedu\nMjob\nFjob\nreason\nguardian\ntraveltime\nstudytime\nfailures\nschoolsup\nfamsup\npaid_mat\nactivities\nnursery\nhigher\ninternet\nromantic\nfamrel\nfreetime\ngoout\nDalc\nWalc\nhealth\nabsences_mat\nG1_mat\nG2_mat\nG3_mat\npaid_por\nabsences_por\nG1_por\nG2_por\nG3_por\nG_mat\nG_por\nabsences_mat_groups\nabsences_por_groups\n\n\n\n\nid1\nGP\nF\n18\nU\nGT3\nA\n4\n4\nat_home\nteacher\ncourse\nmother\n2\n2\n0\nyes\nno\nno\nno\nyes\nyes\nno\nno\n4\n3\n4\n1\n1\n3\n6\n5\n6\n6\nno\n4\n0\n11\n11\n5.666667\n7.333333\nmiddle\nless\n\n\nid2\nGP\nF\n17\nU\nGT3\nT\n1\n1\nat_home\nother\ncourse\nfather\n1\n2\n0\nno\nyes\nno\nno\nno\nyes\nyes\nno\n5\n3\n3\n1\n1\n3\n4\n5\n5\n6\nno\n2\n9\n11\n11\n5.333333\n10.333333\nless\nless\n\n\nid4\nGP\nF\n15\nU\nGT3\nT\n4\n2\nhealth\nservices\nhome\nmother\n1\n3\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nyes\n3\n2\n2\n1\n1\n5\n2\n15\n14\n15\nno\n0\n14\n14\n14\n14.666667\n14.000000\nless\nless\n\n\nid5\nGP\nF\n16\nU\nGT3\nT\n3\n3\nother\nother\nhome\nfather\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nno\nno\n4\n3\n2\n1\n2\n5\n4\n6\n10\n10\nno\n0\n11\n13\n13\n8.666667\n12.333333\nless\nless\n\n\nid6\nGP\nM\n16\nU\nLE3\nT\n4\n3\nservices\nother\nreputation\nmother\n1\n2\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nno\n5\n4\n2\n1\n2\n5\n10\n15\n15\n15\nno\n6\n12\n12\n13\n15.000000\n12.333333\nmiddle\nmiddle\n\n\nid7\nGP\nM\n16\nU\nLE3\nT\n2\n2\nother\nother\nhome\nmother\n1\n2\n0\nno\nno\nno\nno\nyes\nyes\nyes\nno\n4\n4\n4\n1\n1\n3\n0\n12\n12\n11\nno\n0\n13\n12\n13\n11.666667\n12.666667\nless\nless\n\n\nid8\nGP\nF\n17\nU\nGT3\nA\n4\n4\nother\nteacher\nhome\nmother\n2\n2\n0\nyes\nyes\nno\nno\nyes\nyes\nno\nno\n4\n1\n4\n1\n1\n1\n6\n6\n5\n6\nno\n2\n10\n13\n13\n5.666667\n12.000000\nmiddle\nless\n\n\nid9\nGP\nM\n15\nU\nLE3\nA\n3\n2\nservices\nother\nhome\nmother\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nyes\nno\n4\n2\n2\n1\n1\n1\n0\n16\n18\n19\nno\n0\n15\n16\n17\n17.666667\n16.000000\nless\nless\n\n\nid10\nGP\nM\n15\nU\nGT3\nT\n3\n4\nother\nother\nhome\nmother\n1\n2\n0\nno\nyes\nyes\nyes\nyes\nyes\nyes\nno\n5\n5\n1\n1\n1\n5\n0\n14\n15\n15\nno\n0\n12\n12\n13\n14.666667\n12.333333\nless\nless\n\n\nid11\nGP\nF\n15\nU\nGT3\nT\n4\n4\nteacher\nhealth\nreputation\nmother\n1\n2\n0\nno\nyes\nyes\nno\nyes\nyes\nyes\nno\n3\n3\n3\n1\n2\n2\n0\n10\n8\n9\nno\n2\n14\n14\n14\n9.000000\n14.000000\nless\nless\n\n\n\n\n\n\n\nLet’s also follow the algorithm.\nDV – ordinal, IV – ordinal. Actualy, in this design they are not depedent and independent anymore – because this design is not experimental and do not assume causal relatioinhip. Our hypothesis is not about comparing groups with each other, but that these variables correlate, there is a linear relationship between them.\nSince the both variables are ordinal, I need to use a nonparametric analogue of Pearson correlation - Spearman rank correlation (or ordinal logistic regression (if I want the relationship to have predictive power), but that’s not the topic of this article).\n\ncor.test(students$famrel, students$Walc, method = 'spearman')\n\n\n    Spearman's rank correlation rho\n\ndata:  students$famrel and students$Walc\nS = 6173557, p-value = 0.0196\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n-0.130423 \n\n\nIf we had two quantitative variables, we would simply visualize them with a scatterplot with the now familiar line in the middle of the dots. For example, like this:\n\n\n\n\n\n\n\n\n\n\nfrom here\nBut we have two ordinal variables, so the scatterplot will give an unclear variant. Therefore, we will use a mosaic plot: the size of the tile reflects the frequency of coincidence of such values ​​of two variables.\n\n\n\n\n\n\n\n\n\nAnother option is hitmap, where the dimensions are fixed and the color is responsible for the frequency of matches.\n\n\n\n\n\n\n\n\n\n\n\n9.1.5 Interpretation of results\nWhen we interpret the results of a correlation test, we are usually interested in the value of the statistic (the t-value or F-value), the p-value, and the effect size. For a correlation test, the value of the statistic is the t-value, but it is usually not the t-value that is used, but the correlation coefficient between the variables x and y \\(r_xy\\)– it is also the effect size, an indicator of the magnitude of differences. The correlation test is the only test where we do not need an additional metric about the effect size (for example, Cohen’s d), and we judge the strength of the differences by the coefficient itself.\nIn the example above, we got r = -0.13. In the correlation coefficient, we look at two parameters: the sign and the modulus of the number. Here we have a negative correlation, that is, the relationship will be inverse: with an increase in one variable (for example, the assessment of the quality of family relationships famrel), the second variable (frequency of alcohol consumption Walc) will decrease. 0.13 in modulus is a small number, this is a fairly weak correlation (you can check the breakdown by size in the section on effect sizes ).\nIt is important that with a very large sample, even a very weak correlation will reach statistical significance! Therefore, do not get carried away with correlation tests to find connections between everything and everything: you will definitely find it, and it will even be significant. As you can see, even r=0.1 can reach the threshold of statistical significance.\nIt is worth looking for a correlation between meaningful variables: since it can be significant with large samples, it may turn out that the number of films starring Nicolas Cage and the number of suicides by drowning are correlated - obviously, these values ​​are not related to each other, and the correlation here is random. You can look at strange correlations on the website https://tylervigen.com/view_correlation?id=12692\nAnother important point is that in a correlation test, even with a perfectly designed experiment, we will not be able to conclude a cause-and-effect relationship. But it is not the theoretical possibility of concluding a cause-and-effect relationship and the methods of statistical analysis that are important: like warm and red, they refer to different things. The possibility of concluding is determined by the design of the study, not the statistical test. If we have a well-conducted controlled experiment, and the 3 conditions for establishing a cause-and-effect relationship are met ( we discussed this here ), then we can conclude it. At the same time, the use of the same ANOVA may not be related to the experiment, and we will still conclude about the correlation (associative) relationship.\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.6 Correlation matrices\nCorrelation matrix analysis is often used – when correlations are calculated pairwise for each matrix of variables. This can be found, for example, in the correlation of questionnaires: let’s say there is questionnaire O1 and O2. Questionnaire O1 has subscales C11, C12, C13, C14, C15, and O2 has correspondingly C21, C22, C23, C24, C25. Then we can construct a correlation matrix for the subscales of these questionnaires.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Checking the linear link</span>"
    ]
  },
  {
    "objectID": "linear_comparison.html#linear-regression-analysis",
    "href": "linear_comparison.html#linear-regression-analysis",
    "title": "9  Checking the linear link",
    "section": "9.2 Linear Regression Analysis",
    "text": "9.2 Linear Regression Analysis\nLinear regression analysis is exactly the same as the ANOVA (analysis of variance) we know, only if we replace categorical NP with quantitative ones!\nLinear regression itself is a straight line that we try to draw through all our points in such a way that it captures the largest number of them. In essence, this is the same as correlation, only a more powerful tool - here we can enter several NPs.\n\n\n\n\n\n\n\n\n\n\n\nRegression analysis is a pretty powerful thing, because here for the first time we start talking about the predictive function of the analysis. It turns out that regression analysis can be used:\n\nTo test hypotheses about the presence of a linear relationship between quantitative or ordinal variables\nTo predict salary values ​​beyond the available data\n\nFor now, we are interested in the first of these functions, although very often linear regression analysis is interesting precisely from the point of view of the second.\nRegression analysis is based on the construction of a regression line: any line has the form \\(y = kx + b\\), in regression analysis this equation is often written as \\(y = b_0 + b_1x\\). And the task of regression analysis is to determine and test the coefficients \\(b_0\\) and \\(b_1\\) linear regression.\n\n9.2.1 Regression coefficients\nThe equation of the regression line we have drawn is:\n\n\\(\\hat y = b_o + b_1x\\)\n\nWe see that most of the points do not fit the line perfectly - there is still some distance along the y-axis to the point itself. Therefore, if we write down the equation for each point using the regression line equation, it will look like this:\n\n\\(y = b_o + b_1x + e\\)\n\nThe distance along the y-axis that remains to the points after we have drawn a straight line through them is called residuals – that is, these are the differences between the original data and those described by our model (line), what “remains”:\n\n\\(e = y - \\hat y\\)\n\nNote that when we talk about the equation of a straight line, we denote y as \\(\\hat y\\), and when we talk about actual points, we will simply denote it \\(y\\).\nThe regression line is often also called a model . The equation of the regression line with each new coefficient is a new model.\n\n\n\n\n\n\n\n\n\n\nCoefficient \\(b_1\\) answers the slope of the line\nCoefficient \\(b_0\\) is responsible for the displacement of the line along the y-axis (intercept)\n\nThe coefficients are calculated in such a way that the sum of the squares of the residuals is minimal. This is called the least squares method .\nWhen constructing a regression line, we need to strive to reduce the sum of the residuals:\n\n\\(\\sum_{i=1}^{n} e^2 = \\sum_{i=1}^{n}(y - \\hat y)^2\\)\n\nThe formulas for the coefficients using the least squares method are equal to:\n\n\\(b_{1_{xy}} = \\frac{sd_y}{sd_x} r_{xy}\\)\n\n\n\\(b_o = \\bar y - b_{1_{xy}}\\bar x\\)\n\nWhen calculating the coefficients, the first one to be calculated is \\(b_1\\), and it, as can be seen from the formula, depends on the magnitude of the variability of the data for the variables x and y (standard deviations or dispersions). In the case of equal variability \\(b_1\\) is the correlation coefficient \\(r_{xy}\\)\n\n\n9.2.2 Coefficient of determination and proportion of explained variability\nIn linear regression, as in ANOVA, the coefficient of determination tells us the percentage of variability explained , that is, how well our regression model explains the variability in the dependent variable.\nAs in ANOVA, the sum of squares SST consists of the between-group sum of squares (SSE, Sum of Squares Explained or SSB, Sum of Squares Between groups) and the within-group sum of squares (SSR, Sum of Squares Random or SSW, Sum of Squares Within groups).\n\n\\(SST = SSE + SSR\\)\n\n\n\n\n\n\n\n\n\n\n\n\nThe total variability is calculated from a line with a mean value of y.\n\n\\(SST = \\sum_{i=1}^n (\\bar y - y_i)^2\\)\n\nFrom the picture you can see that\n\n\\(SSE = \\sum_{i=1}^n (\\bar y - \\hat y_i)^2\\)\n\nResidual variability:\n\n\\(SSR = \\sum_{i=1}^n (y_i - \\hat y_i)^2\\)\nTo evaluate how good the model is, we again resort to the coefficient of determination:\n\n\\(R^2 = \\frac{SSE}{SST} = 1 - \\frac{SSR}{SST}\\)\n\nThe coefficient of determination can be thought of as the size of the effect – and it is nothing more than the already familiar \\(\\eta^2\\)!\n\n\\(\\eta^2 = \\frac{SSE}{SST}\\)\n\nIn linear regression analysis, the coefficient of determination is also considered as the degree of correlation between the initial values ​​of the variable \\(y\\) and predicted \\(\\hat y\\). And as we remember, it is equal to the square of the correlation between the initial values ​​of the variable \\(y\\) and predicted \\(\\hat y\\):\n\n\\(R^2 = r_{xy}^2 = \\frac{\\text{cov(x, y)}}{sd_x sd_y} = \\frac{\\sum_{i=1}^n (x_i - \\bar x) (y_i - \\bar y )}{(n-1)sd_x sd_y}\\)\n\n\n\n9.2.3 Regression analysis (testing regression coefficients)\nRegression analysis is an interesting thing, as it consists of several layers that take something from ANOVA, and something from correlation analysis. Testing the significance of the coefficients is carried out on the basis of a criterion belonging to the T-distribution family, just like correlation analysis. And testing the entire model is carried out using the F-criterion, just like ANOVA. In regression analysis, we are more interested in testing the significance of the coefficients - since it is by the coefficients with which the factors in the model are taken that we determine whether the influence of these factors is significant.\nThe number of degrees of freedom is calculated using the formula:\n\n\\(df = n - 2\\), n – number of observations\n\nModel equation:\n\n\\(\\hat y = b_o + b_1x\\)\n\nNull and alternative hypotheses:\n\n\\(H_0\\): \\(b_{1_{xy}} = 0\\)\n\n\n\\(H_1\\): \\(b_{1_{xy}} \\neq 0\\)\n\nThe key statistic for the coefficients is the T-value, it is calculated using the formula:\n\n\\(T = \\frac{b_1}{se}\\)\n\n\n\n9.2.4 Multiple Regression Analysis\nMultiple regression analysis – implies the same thing, only new predictors appear (independent variables, also known as factors)\n\n\\(\\hat y = b_o + b_1x_1 + b_2x_2 + ... + b_nx_n\\)\n\n\n\n9.2.5 Assumptions for Regression Analysis\n(Variables are measured on a quantitative or ordinal scale)\n\nThe distribution of variables is linear - there is no picture of non-linear relationships or clusters of data in different places.\nThe residuals vary approximately equally along the entire line – homogeneity (or homoscedasticity) of the residuals. It is most often tested using a diagnostic scatter plot with the distribution of residuals by the predicted values ​​(fitted values)\nThe residuals are normally distributed – the same as here , only for the residuals (probability density plot for residuals or QQ-plot)\nFor multiple linear regression – absence of multicollinearity (strong correlation between independent variables). Checked using the VIF test (“variance inflation index”)\n\nExamples of diagnostic graphs for residues: https://gallery.shinyapps.io/slr_diag/\n\n\n9.2.6 Calculating Regression Analysis\nWhen we run a regression analysis calculation, we end up with a table like this:\n\n\n\n\n\n\n\n\n\n\nDesignation\nCoefficient\nStatistic\nSE\np-value\n\n\n\n\n\\(b_0\\)\nIntercept\n\\(t_{b0}\\)\n\\(SE_{b0}\\)\n\\(p_{b0}\\)\n\n\n\\(b_1\\)\nКоэф для фактора1\n\\(t_{b1}\\)\n\\(SE_{b1}\\)\n\\(p_{b1}\\)\n\n\n(если есть) \\(b_2\\)\n(если есть) Коэф для фактора2\n\\(t_{b2}\\)\n\\(SE_{b2}\\)\n\\(p_{b2}\\)\n\n\n(если есть) \\(b_3\\)\n(если есть) Коэф для фактора3\n\\(t_{b3}\\)\n\\(SE_{b3}\\)\n\\(p_{b3}\\)\n\n\n\nJust like everywhere else, we are primarily interested in the value of the statistics (t-value), the level of significance (p-value) and here we are also interested in the value of the coefficients themselves. In the case of significance (p-value &lt; alpha), that is, obtaining the result that this factor significantly affects the variability of the data, and we can construct a regression line - we will write the equation of the regression line based on these values:\nyy= Intercept + Coefficient for factor1 * Factor1 + Coefficient for factor2 * Factor2 + Coefficient for factor3 * Factor3\n\n\\(\\hat y\\) = Intercept + Coefficient for factor1 * Factor1 + Coefficient for factor2 * Factor2 + Coefficient for factor3 * Factor3\n\nFor example, let’s take another dataset with Udemy course information.\n\n\n\n\n\n\nid\ntitle\nis_paid\nprice\nnum_subscribers\navg_rating\nnum_reviews\nnum_comments\nnum_lectures\ncontent_length_min\npublished_time\nlast_update_date\ncategory\nsubcategory\ntopic\nlanguage\ncourse_url\ninstructor_name\ninstructor_url\nprice_log\nnum_subscribers_log\n\n\n\n\n1039124\nStart Finishing Your Projects\nTRUE\n39.99\n7047\n4.625000\n1151\n220\n41\n129\n2017-04-01 00:41:21\n2019-01-07\nPersonal Development\nPersonal Productivity\nPersonal Productivity\nEnglish\n/course/start-finishing-your-projects/\nCharlie Gilkey\n/user/charlie-gilkey/\n3.688629\n8.860357\n\n\n1128200\nUsing Scrum to Complete Projects in your Client's Budget\nTRUE\n19.99\n11\n3.750000\n2\n0\n26\n149\n2017-05-02 22:06:57\n2017-04-27\nIT & Software\nOther IT & Software\nScrum\nEnglish\n/course/scrum-practices/\nBeverly Reynolds\n/user/beverly-reynolds/\n2.995232\n2.397895\n\n\n760252\nLearn US Politics with Film\nTRUE\n109.99\n245\n3.950000\n50\n10\n44\n271\n2016-02-25 17:31:21\n2017-01-01\nTeaching & Academics\nSocial Science\nPolitical Science\nEnglish\n/course/usapolitics/\nSteven Ward, M.P.A.\n/user/stevenward6/\n4.700389\n5.501258\n\n\n1990786\nStatistics Fundamentals and its Applications\nTRUE\n19.99\n11397\n3.950000\n540\n119\n37\n197\n2018-11-20 19:10:42\n2020-03-01\nTeaching & Academics\nMath\nStatistics\nEnglish\n/course/statistics-fundamentals/\nAmba Kumari\n/user/amba-kumari/\n2.995232\n9.341105\n\n\n3888974\nPhotoshop Elements 2021\nTRUE\n44.99\n24\n4.357143\n7\n2\n117\n547\n2021-03-04 17:12:38\n2021-03-03\nDesign\nGraphic Design & Illustration\nPhotoshop\nDutch\n/course/photoshop-elements-2021/\nMartijn van Weeghel\n/user/martijn-van-weeghel/\n3.806440\n3.178054\n\n\n856424\nSQL DBA For Beginners\nTRUE\n124.99\n302\n3.650000\n70\n16\n49\n264\n2016-05-25 14:46:09\n2021-06-14\nDevelopment\nDatabase Design & Development\nSQL\nEnglish\n/course/sql-dba-for-beginners/\nSkill Tree\n/user/williamumusu/\n4.828234\n5.710427\n\n\n2816955\nMicrosoft Outlook Kurs für Einsteiger\nTRUE\n199.99\n1241\n4.550000\n50\n14\n36\n264\n2020-03-10 17:59:43\n2022-10-07\nOffice Productivity\nMicrosoft\nMicrosoft Outlook\nGerman\n/course/microsoft-outlook-grundkurs/\nBen Polland\n/user/ben-polland/\n5.298267\n7.123673\n\n\n2271686\nUltimate Course for Stock Market Beginners - ZERO to HERO\nTRUE\n19.99\n3259\n4.100000\n87\n22\n56\n402\n2019-09-05 20:09:58\n2019-09-03\nFinance & Accounting\nFinance\nStock Trading\nEnglish\n/course/ultimate-course-for-stock-market-beginners-zero-to-hero/\nChetan Mirani\n/user/chetan-mirani/\n2.995232\n8.089176\n\n\n4142060\n【Google Apps Script 超入門講座】2時間で知識ゼロから基礎をマスター｜様々なアプリと連携して業務効率化\nTRUE\n3.00\n1419\n4.135416\n226\n19\n43\n121\n2021-07-01 05:44:21\n2022-06-19\nDevelopment\nProgramming Languages\nGoogle Apps Script\nJapanese\n/course/googleappsscript/\n高幸 仲条\n/user/zhong-tiao-gao-xing/\n1.098612\n7.257708\n\n\n3427978\nGerenciamento de manutenção Industrial - Parte 2\nTRUE\n99.90\n18\n4.300000\n5\n0\n47\n459\n2020-10-30 16:36:39\n2021-02-24\nBusiness\nIndustry\nMaintenance Management\nPortuguese\n/course/gerenciamento-de-manutencao-industrial-parte-2/\nLucenir Piovesan\n/user/lucenir-piovesan/\n4.604170\n2.890372\n\n\n\n\n\n\n\nAnd we will try to build a model of the cost of the course based on the number of students ( you may not have noticed, but here we are getting very close to the real problems that data analysts solve )\nSpoiler: since this is real data, we had to tinker with its preprocessing, and even after that, the best option for building a model looks like this:\n\n\n\n\n\n\n\n\n\nIt is obvious that the model will not work well here. Let’s take another unsuccessful example for building regression models:\n\n\n\n\n\n\n\n\n\nTherefore, we will discard the idea of ​​predicting the price and rating for now and move on to something more prosaic – we will build a model of the course duration from the number of lectures.\n\nHypothesis: Duration content_length_minis determined by the number of lecturesnum_lectures\n\n\ncontent_length_min ~ num_lectures\n\n\nWe formulate the null hypothesis:\n\nCoefficient \\(b_1\\) for num_lecturesshould not be equal to zero (that is, num_lecturesit affects the variability of the data)\n\n\\(H_0\\): \\(b_{1_{xy}} = 0\\)\n\n\n\\(H_1\\): \\(b_{1_{xy}} \\neq 0\\)\n\n\nLet’s fix that we will test the hypothesis at the level \\(\\alpha = 0.05\\)\nLet’s choose a statistical criterion for testing. Let’s see how linearly distributed the variables and residuals are:\n\n\n\n\n\n\n\n\n\n\nNot the best option, but you can work with it (why? how is it different from the previous picture?)\n\nWe build a regression model and conduct a regression analysis, look at the significance of the coefficients\n\n\n\n\nCall:\nlm(formula = udemy_model$content_length_min ~ udemy_model$num_lectures)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-178.70  -46.37  -10.45   39.27  210.40 \n\nCoefficients:\n                         Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)               76.1720     5.2651   14.47 &lt;0.0000000000000002 ***\nudemy_model$num_lectures   2.2852     0.1913   11.95 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 62.67 on 497 degrees of freedom\nMultiple R-squared:  0.223, Adjusted R-squared:  0.2215 \nF-statistic: 142.7 on 1 and 497 DF,  p-value: &lt; 0.00000000000000022\n\n\n\nLet’s interpret the results: what is the p-value for the coefficient num_lectures? We see that it is very small and clearly less than the declared alpha level - that is, the coefficient is significant, our hypothesis that the number of lessons determines the course length has been confirmed, hurray! What is the value of the coefficient itself? About 2.16. That is, with an increase in the number of lessons by 1, the course length will increase by 2.16 minutes! What is \\(R^2\\)? It is equal to 0.19, which is not very much in general, but it is already a result. That is, 19% of the variability of our data on the course duration is determined by the number of lessons!\n\nI can now write the equation of the regression line as follows:\n\n\\(\\hat content\\_length\\_min = 81.94 + 2.16 \\times num\\_lectures\\)\n\nLet’s do the same analysis, but taking into account several factors (predictors). Let’s assume that the course duration is also explained by the number of subscribersnum_subscribers\n\nmodel_length2 &lt;- lm(udemy_model$content_length_min ~ udemy_model$num_lectures + udemy_model$num_subscribers_log) \nsummary(model_length2)\n\n\nCall:\nlm(formula = udemy_model$content_length_min ~ udemy_model$num_lectures + \n    udemy_model$num_subscribers_log)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-179.07  -46.27  -10.45   39.03  210.61 \n\nCoefficients:\n                                Estimate Std. Error t value            Pr(&gt;|t|)\n(Intercept)                      76.7846     7.7452   9.914 &lt;0.0000000000000002\nudemy_model$num_lectures          2.2895     0.1957  11.700 &lt;0.0000000000000002\nudemy_model$num_subscribers_log  -0.1495     1.3847  -0.108               0.914\n                                   \n(Intercept)                     ***\nudemy_model$num_lectures        ***\nudemy_model$num_subscribers_log    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 62.73 on 496 degrees of freedom\nMultiple R-squared:  0.2231,    Adjusted R-squared:  0.2199 \nF-statistic:  71.2 on 2 and 496 DF,  p-value: &lt; 0.00000000000000022\n\n\nWhat can be said about these results? Are both coefficients significant?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Checking the linear link</span>"
    ]
  },
  {
    "objectID": "r_3.html",
    "href": "r_3.html",
    "title": "8  Visualizations",
    "section": "",
    "text": "8.1 Assignments after the seminar 3\nWe have already built simple visualizations - a simple histogram and a simple probability density graph using basic R.\nThese visualizations are correct, but they are standard and do not look very nice. Meanwhile, visualizations are very important - the clarity of the data presentation depends on their quality, and you can even manipulate data with visualizations. Scripting languages ​​have a huge number of tools for working with visualizations. In Python, these are matplotlib. And now welcome to ggplot2the universe of graphs in R https://ggplot2.tidyverse.org/\nFor example, you can draw a graph like this , or like this . All this is achieved by means of the package ggplot2, which is already integrated into tidyverse.\n“gg” in the package name means Grammar of graphics . You can read more about it, for example, from Ivan Pozdniakov https://pozdniakov.github.io/tidy_stats/230-ggplot2.html\nThe principle consists of drawing the graph in layers. Mandatory layers:\nAuxiliary layers:\nImage from The Grammar of Graphics\nggplot integrates well with tiedivers, but it appeared, took shape and spread earlier, so instead of the pipe icon %&gt;%there is +. Let’s draw the same histogram, but more beautifully:\nWe can change colors of two things: fill and outline, as usual for shapes in PowerPoint or Word. For fill, use the fill argument, for outline – color. Some geoms have only outline – like, for example, scatter plot, since points have no fill.\nLet’s plot a similar probability density graph\nAll of these graphs were descriptive - to study one variable. Now let’s create a scatterplot, which shows the distribution of one variable as a function of the values ​​of another.\nAnd we can even color the points on this graph depending on the values ​​of the third variable! For example, the region. Everything is exactly the same as with colors before: if we want to fill the figure depending on the values ​​of the third variable, then the fill argument is used for coloring, if we color the outline, then color (like the outline and fill in Word or PowerPoint). But with the difference that now the fill color is set inside the aesthetic , since this is still about “pulling an owl onto a globe”, that is, our data on the graph.\nThere are a huge number of additional things for more beautiful visualizations in ggplot to do something truly incredible. Of course, we will not have time to get acquainted with them within the course, but here, for example, is an additional package with palettes made based on Wes Anderson’s films:) If we want to color our graph not just in the default colors, but in some special ones - there is a package in which color schemes from these films are collected! First, you need to put it through the usual way install.packages(), connect through library(), and then add one line with the addition of a scale scale_fill_manual()or scale_color_manual()- again, depending on whether we want to do a fill with these palettes or an outline.\nThe dataset is used for tasks2016-2",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "r_3.html#tasks_sem3",
    "href": "r_3.html#tasks_sem3",
    "title": "8  Visualizations",
    "section": "",
    "text": "Follow all the examples in this section to make sure everything works correctly.\nCalculate how many NAs are in columns Economy (GDP per Capita), Family, Health (Life Expectancy), Freedom, Trust (Government Corruption), Generosity (the task can be done in different ways, any one that gives the correct answer will do)\nCalculate descriptive statistics for the Economy (GDP per Capita), , columns Family: Freedommean, median, mode, standard deviation, variance, and interquartile range, and (for those working in R only) calculate values ​​for quartiles 1 and 3. Answers should be different from NA (one way to find out values ​​by quartile is, for example, using separate common functions for descriptive statistics) .\nUsing the column with values ​​and , created in the previous homework , depending on the values ​​, calculate the mean and standard deviation within these groups ( and ) for the columns , , . (Only for those working in R) Compare the obtained mean values ​​with the values ​​for the 1st and 3rd quartiles from the previous task. What can you say about them?upperlowerHappiness ScoreupperlowerEconomy (GDP per Capita)FamilyFreedom\nPlot probability density plots for variables Familyand Freedom, (only for those working in R) using at least 2 non-default settings (theme, color, line type, transparency, etc.) Examples can be googled or viewed in the cheatsheet\nCreate a scatterplot for variable Familyby variable Freedomand (for those working in R only) color the dots based on the values ​​of the upper and lower variable you created in the last homework.\nCalculate the correlation coefficient between variables Familyand Freedom",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "r_4.html",
    "href": "r_4.html",
    "title": "13  Testing hypotheses",
    "section": "",
    "text": "13.0.1 Correlation test\nIt’s time to move on to what we spent last semester studying statistics for: hypothesis testing.\nI collected all the new columns we created into one dataset so that we could use them if needed.\nBefore applying statistical tests, it is worth reviewing the section on statistical inference in its entirety and the hypothesis testing algorithm in particular . We do not apply tests straight away! First, we must have a meaningful hypothesis, an understanding of what the \\(H_0\\) and \\(H_1\\), the selected significance levelααand calculated based on statistical \\(power\\) the required amount of data. Here we are working with ready-made data, so we cannot influence the amount of data, but we must take into account all other points.\nThe first thing we’ll practice with is a correlation test .\nFirst, we check the assumptions for the applicability of the correlation test\nlibrary(tidyverse)\nwhr &lt;-read_csv(\"2016.csv\")\n\n#Check linearity of relationship\nwhr %&gt;%\n  ggplot(aes(x = Family, y = `Happiness Score`)) +\n  geom_point(color = \"#355C7D\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# check distributioins\n\nwhr %&gt;%\n  ggplot(aes(x = Family, y = `Happiness Score`)) +\n  geom_point(color = \"#355C7D\") +\n  theme_minimal()\n\n\n\n\n\n\n\nwhr %&gt;%\n  ggplot(aes(x =`Happiness Score`)) +\n  geom_density(color = \"#355C7D\") +\n  theme_minimal()\n\n\n\n\n\n\n\nwhr %&gt;%\n  ggplot(aes(x =Family)) +\n  geom_density(color = \"#355C7D\") +\n  theme_minimal()\n\n\n\n\n\n\n\nwhr %&gt;%\n  ggplot(aes(sample = Family)) +\n  stat_qq(color = \"#355C7D\") +\n  geom_qq_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n# We can see that variable Family distributed not exactly normally, so we will use Spearman correlatoin instead of Pearson\n\ncor.test(whr$`Happiness Score`, whr$Family, method = \"spearman\")\n\nWarning in cor.test.default(whr$`Happiness Score`, whr$Family, method =\n\"spearman\"): Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  whr$`Happiness Score` and whr$Family\nS = 153450, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n     rho \n0.762077 \n\nwhr %&gt;%\n  ggplot(aes(x = Freedom, y = `Happiness Score`)) +\n  geom_point(color = \"#355C7D\") +\n  theme_minimal()\n\n\n\n\n\n\n\nwhr %&gt;%\n  ggplot(aes(x = Freedom)) +\n  geom_density(color = \"#355C7D\") +\n  theme_minimal()\n\n\n\n\n\n\n\nwhr %&gt;%\n  ggplot(aes(sample = Freedom)) +\n  stat_qq(color = \"#355C7D\") +\n  geom_qq_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n# But here we can use Pearson correlation\ncor.test(whr$`Happiness Score`, whr$Family, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  whr$`Happiness Score` and whr$Family\nt = 13.667, df = 155, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6589122 0.8029161\nsample estimates:\n      cor \n0.7392516\nHow to interpret this? Let’s recall everything we learned about statistical criteria last semester.\nBy the way, in programming languages, numbers are often written using exponential notation ase-10e-10– this is convenient for the mathematical view, which digit comes first after a set of zeros, but inconvenient for interpretation. To remove it, you can perform the operation anywhere in P\noptions(scipen = 999)\nCorrelation matrices\n# корреляция по простому\nwhr %&gt;%\n  select(c(`Happiness Score`:Generosity, -`Upper Confidence Interval`, -`Lower Confidence Interval`)) %&gt;%\n  cor(method = \"spearman\") %&gt;%\n  round(2) \n\n                              Happiness Score Economy (GDP per Capita) Family\nHappiness Score                          1.00                     0.81   0.76\nEconomy (GDP per Capita)                 0.81                     1.00   0.70\nFamily                                   0.76                     0.70   1.00\nHealth (Life Expectancy)                 0.77                     0.86   0.62\nFreedom                                  0.56                     0.40   0.51\nTrust (Government Corruption)            0.31                     0.22   0.18\nGenerosity                               0.15                     0.00   0.12\n                              Health (Life Expectancy) Freedom\nHappiness Score                                   0.77    0.56\nEconomy (GDP per Capita)                          0.86    0.40\nFamily                                            0.62    0.51\nHealth (Life Expectancy)                          1.00    0.35\nFreedom                                           0.35    1.00\nTrust (Government Corruption)                     0.17    0.47\nGenerosity                                        0.08    0.40\n                              Trust (Government Corruption) Generosity\nHappiness Score                                        0.31       0.15\nEconomy (GDP per Capita)                               0.22       0.00\nFamily                                                 0.18       0.12\nHealth (Life Expectancy)                               0.17       0.08\nFreedom                                                0.47       0.40\nTrust (Government Corruption)                          1.00       0.25\nGenerosity                                             0.25       1.00\n\nlibrary(\"Hmisc\")\n\nwhr %&gt;%\n  select(c(`Happiness Score`:Generosity, -`Upper Confidence Interval`, -`Lower Confidence Interval`)) %&gt;%\n  as.matrix() %&gt;%\n  rcorr() -&gt; whr_cor\n\nwhr_cor$r\n\n                              Happiness Score Economy (GDP per Capita)\nHappiness Score                     1.0000000               0.79032202\nEconomy (GDP per Capita)            0.7903220               1.00000000\nFamily                              0.7392516               0.66953969\nHealth (Life Expectancy)            0.7653843               0.83706723\nFreedom                             0.5668267               0.36228285\nTrust (Government Corruption)       0.4020322               0.29418478\nGenerosity                          0.1568478              -0.02553066\n                                  Family Health (Life Expectancy)   Freedom\nHappiness Score               0.73925158               0.76538433 0.5668267\nEconomy (GDP per Capita)      0.66953969               0.83706723 0.3622828\nFamily                        1.00000000               0.58837678 0.4502082\nHealth (Life Expectancy)      0.58837678               1.00000000 0.3411993\nFreedom                       0.45020820               0.34119929 1.0000000\nTrust (Government Corruption) 0.21356094               0.24958329 0.5020540\nGenerosity                    0.08962885               0.07598731 0.3617513\n                              Trust (Government Corruption)  Generosity\nHappiness Score                                   0.4020322  0.15684780\nEconomy (GDP per Capita)                          0.2941848 -0.02553066\nFamily                                            0.2135609  0.08962885\nHealth (Life Expectancy)                          0.2495833  0.07598731\nFreedom                                           0.5020540  0.36175133\nTrust (Government Corruption)                     1.0000000  0.30592986\nGenerosity                                        0.3059299  1.00000000\n\nlibrary(corrplot)\n\ncorrplot(whr_cor$r, method=\"circle\")\n\n\n\n\n\n\n\n# corrplot(whr_cor$r, p.mat = whr_cor$P, sig.level = 0.05, insig = \"blank\")\nheatmap(whr_cor$r)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Testing hypotheses</span>"
    ]
  },
  {
    "objectID": "r_4.html#assignments-after-the-seminar-4",
    "href": "r_4.html#assignments-after-the-seminar-4",
    "title": "13  Testing hypotheses",
    "section": "13.1 Assignments after the seminar 4",
    "text": "13.1 Assignments after the seminar 4\n\nOn the WHR dataset, conduct at least one correlation test for the entire NHST algorithm: select suitable variables, formulate a hypothesis, formulate H_0 And H_1, select level α, preprocess the data if necessary, and run a correlation test at the chosen significance level. Interpret the results: was the hypothesis confirmed?\nOn the WHR dataset, conduct at least one t-test or its nonparametric analogue using the entire NHST algorithm: select the appropriate variables, formulate a hypothesis, formulate H_0 And H_1, select level α, determine whether the samples are dependent or independent, preprocess the data if necessary, and perform a mean comparison test at the chosen significance level. Interpret the results: was the hypothesis confirmed?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Testing hypotheses</span>"
    ]
  },
  {
    "objectID": "r_4.html#tasks_sem3",
    "href": "r_4.html#tasks_sem3",
    "title": "13  Testing hypotheses",
    "section": "13.1 Assignments after the seminar 3",
    "text": "13.1 Assignments after the seminar 3\n\nOn the WHR dataset, conduct at least one correlation test for the entire NHST algorithm: select suitable variables, formulate a hypothesis, formulate H_0 And H_1, select level α, preprocess the data if necessary, and run a correlation test at the chosen significance level. Interpret the results: was the hypothesis confirmed?\nOn the WHR dataset, conduct at least one t-test or its nonparametric analogue using the entire NHST algorithm: select the appropriate variables, formulate a hypothesis, formulate H_0 And H_1, select level α, determine whether the samples are dependent or independent, preprocess the data if necessary, and perform a mean comparison test at the chosen significance level. Interpret the results: was the hypothesis confirmed?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Testing hypotheses</span>"
    ]
  },
  {
    "objectID": "r_5.html",
    "href": "r_5.html",
    "title": "14  Testing Hypothesis 2: ANOVA and Linear Regression",
    "section": "",
    "text": "14.0.1 ANOVA (analysis of variance)\nI wanted to add some variety to our data, so I found another table related to the World Happiness Report, you can read more about it here (by the way, this doc, apparently, is also formatted in R).\nThis is roughly what preprocessing of real (not the cleanest and crooked) data might look like: I combine two tables (the table for 2016 and the new table for 2019) using the function right_join(this means that I attach the left table to the right one, and everything that is left in the left one that does not fit into the right one is not taken) from the family join, rename columns and replace spaces on them with underscores using the function rename_with, select only the columns that interest me using the function select, create new variables that we have already created earlier in homework, using the mutate function and save the result in a new table in the datasetwhr_tests_hw\nwhr_2019 &lt;- readxl::read_xls(\"Chapter2OnlineData2019.xls\")\nwhr &lt;- read_csv(\"2016.csv\")\n\nwhr_2019 %&gt;%\n  rename_with(~ gsub(\" \", \"_\", .), .cols = everything()) %&gt;% \n  select(Country_name:Negative_affect) %&gt;%\n  filter(Year &lt;= 2016) -&gt; whr_2019\n\nwhr %&gt;%\n  rename_with(~ gsub(\" \", \"_\", .), .cols = everything()) %&gt;% \n  select(Country, Happiness_Rank, Happiness_Score) %&gt;%\n  right_join(whr_2019, by = join_by(Country == Country_name), multiple = \"last\") -&gt; tmp\n\nwhr %&gt;%\n  select(Country:Region) %&gt;%\n  right_join(tmp, by = join_by(Country == Country), multiple = \"all\") %&gt;%\n  mutate(\"top20\" = ifelse(Happiness_Rank&lt;=20, \"hehe\", \"not hehe\"),\n         \"mean_position\" = ifelse(Happiness_Score&gt;= mean(Happiness_Score, na.rm = TRUE), \"upper\", \"lower\")) -&gt; whr_tests_hw\nI also downloaded the result of this preprocessing into a separate file using Rthe function https://raw.githubusercontent.com/elenary/StatsForDA/main/whr_tests_hw.csvwrite_csv()\nPreviously, we tested the hypothesis that the Happiness Score differs in two regions: Western and Eastern and Central Europe. But what if I want to check whether the level of happiness differs statistically significantly in three regions - Western and Eastern and Central Europe and in Latin America? Again, we turn to the statistical criteria we studied last semester , remember the picture with the tree of choosing statistical tests https://miro.com/app/board/uXjVOxmKhr8=/ . If we have a quantitative PP, categorical NP / categorical, and we need to conduct 3 or more comparisons - then we move from the t-test to ANOVA (analysis of variance) .\nI will periodically adhere to the form of writing code when I do everything at once inside one pipe - both filtering and plotting graphs or calculating a test, but do not forget that before writing everything together, you need to make sure that each line works. And it may be more convenient at first to create more variables and write the filtering results there, and then use this variable - this will make it easier to debug the code. For example, you can first filter the data and write it to a new dataset (sometimes it happens that you cannot do without this form of writing)\nRecalling the assumptions for ANOVA\nFirst, wages should be distributed close to a normal distribution.\nLet’s say it looks like a normal distribution.\nThe second assumption is homogeneity (homoscedasticity) of variances. Variances should be the same in our groups. If they are different, this is bad, and we will have to use nonparametric analogs of ANOVA. Homogeneity of variances is checked using Levene’s Test. Note: here we are interested in obtaining a NON-significant result - because if the test yielded a significant result, then the variances in the groups are different, and this is not ok. We will need a function leveneTestfrom the packagecar\nLook at the p-value: it’s not significant, hooray! So we’ve met all the assumptions, and we can safely use anova. We’ll try to build it in two ways: using a standard function aovand using a function ezANOVAfrom a package ezwith a clearer syntax, but more picky and less stable.\nHow to interpret these results? The first thing we look at is the p-value (column Pr(F)). If it is less than the set levelaa, then we say that we reject the null hypothesis and we have obtained statistically significant differences . If the p-value is greater than or equal toaa– we do not have enough evidence to support the alternative hypothesis, and we say that we do not reject the null hypothesis .\nIf we compare the results, they will be exactly the same, only the second function in the ges column also gives the effect size! This is eta squaredor2or2, the effect size metric for ANOVA , which you used anyway in your homework last year. We can see that the effect we got is quite large!\nWe can calculate it separately for the previous table using the package function effectsize\nThe value will be the same as in the ges column of the ezANOVA output.\nVisualize the results. The most common methods for visualizing ANOVA are boxplots or violet plots.\nWe know that the ANOVA is significant, that is, there are statistically significant differences between the three regions. But how do we know which regions are contributing to the significance? Could it be that the significance is being provided by one region that is very different from the others, while the other two are not different? It is. To find out, we need to conduct post-hoc tests .",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Testing Hypothesis 2: ANOVA and Linear Regression</span>"
    ]
  },
  {
    "objectID": "r_5.html#assignments-after-the-seminar-6",
    "href": "r_5.html#assignments-after-the-seminar-6",
    "title": "14  Testing Hypothesis 2: ANOVA and Linear Regression",
    "section": "14.1 Assignments after the seminar 6",
    "text": "14.1 Assignments after the seminar 6\nTest several hypotheses on the dataset whr_tests_hwusing ANOVA and linear regression analysis: select variables suitable for these tests, formulate a hypothesis, formulate \\(H_0\\) and \\(H_1\\), select levelaa, (if necessary, preprocess the data), check the assumptions for the tests.\n\nSelect appropriate variables, formulate a meaningful hypothesis, and test it using one-way ANOVA. Interpret the results: was the hypothesis confirmed?\nSelect appropriate variables, formulate a meaningful hypothesis and test it with a multivariate ANOVA. Interpret the results: was the hypothesis confirmed?\nSelect appropriate variables, formulate a meaningful hypothesis, and test it using repeated measures ANOVA. Interpret the results: was the hypothesis supported?\nConduct the same tests of hypotheses 1-3 using linear regression analysis.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Testing Hypothesis 2: ANOVA and Linear Regression</span>"
    ]
  },
  {
    "objectID": "r_5.html#linear-regression-analysis",
    "href": "r_5.html#linear-regression-analysis",
    "title": "14  Testing Hypothesis 2: ANOVA and Linear Regression",
    "section": "14.2 Linear Regression Analysis",
    "text": "14.2 Linear Regression Analysis\nNow let’s move on to one of the most important methods in data analysis, including psychological ones - linear regression . First of all, we need to remember when this method is used - and it is used when both the PP and the NP are quantitative. When we talked about correlations, we discussed that this is a simplified version of linear regression, when we have only one NP. For one NP, we can do both a correlation test and a regression analysis, but when there are two or more quantitative NPs, we only need regression. We also discussed that ANOVA is actually a subtype of linear regression, if we move from quantitative NPs to categorical ones! But first things first, let’s start with quantitative NPs, a more classic case of linear regression.\nBefore we build a regression model, we need to check the assumptions for linear regression\nNote that we are not so interested in how normally the salaries are distributed! This is the power of linear regression, that it is robust to salaries of different distributions. But another assumption regarding the distribution of data by salaries is the linearity between the NP and the salaries on which we want to build the model, we need to make sure that there is a linear relationship between them (and not a hole in a donut or a gnome-shaped entity). This is the first assumption.\nI chose for analysis the hypothesis that the level of happiness according to the subjective position on the ladder Life_Ladderdepends on the parameters Log_GDP_per_capita(purchasing power, GDP per capita), Social_support, Freedom_to_make_life_choices,Perceptions_of_corruption.\nLet’s test the assumption of a linear relationship between Life_Ladder salary and several potential variables that I have selected for analysis.\nwhr_tests_hw %&gt;% \n  ggplot(aes(x = Log_GDP_per_capita, y = Life_Ladder)) +\n  geom_point(color = \"#355C7D\", size = 1) +\n  theme_minimal()\nwhr_tests_hw %&gt;% \n  ggplot(aes(x = Social_support, y = Life_Ladder)) +\n  geom_point(color = \"#355C7D\", size = 1) +\n  theme_minimal()\nwhr_tests_hw %&gt;% \n  ggplot(aes(x = Freedom_to_make_life_choices, y = Life_Ladder)) +\n  geom_point(color = \"#355C7D\", size = 1) +\n  theme_minimal()\nwhr_tests_hw %&gt;% \n  ggplot(aes(x = Perceptions_of_corruption, y = Life_Ladder)) +\n  geom_point(color = \"#355C7D\", size = 1) +\n  theme_minimal()\nI see that the Log_GDP_per_capita variable shows a very good linear relationship, the Freedom_to_make_life_choices variable is close to this. I don’t really like the Social_support variable, but I can try to transform it in the process, but the Perceptions_of_corruption variable looks frankly bad, I’ll take it as an anti-example.\n\n14.2.1 Linear Regression with One Predictor (Factor)\nI will build a model with one factor to begin with – Log_GDP_per_capita. The model is built using the lm() function, to display the main information about the model you need to display summary(). By the way, you can also display these results immediately as ANOVA – this is all because, in fact, this is a subspecies of the same linear regression, and if we like the ANOVA table, we can get it by simply applying the functions to the anova() model\nwhr_tests_hw %&gt;% \n  lm(Life_Ladder ~ Log_GDP_per_capita, .) -&gt; lm_model1\nsummary(lm_model1)\nanova(lm_model1)\nIn general, we can already begin to interpret the results of the model, but it is still too early - first we need to make sure that the variances of the residuals are equal, that is, that the assumption of homoscedasticity (homogeneity) of the variances is met. This is easiest to check by displaying the first of four graphs that are displayed if you pass the linear model to the basic plot() function.\nplot(lm_model1, 1)\nWe can make the graph prettier using ggplot(). To do this, we will use the fact that the result of executing lm() is a list containing many different data, including both the model residuals and the predicted values ​​separately.\nhead(lm_model1$residuals) \nhead(lm_model1$fitted.values)\nggplot(lm_model1, aes(x = lm_model1$fitted.values, y = lm_model1$residuals)) +\n  geom_point(color = \"#355C7D\", size = 1) +\n  theme_minimal()\nLet’s recall the examples of diagnostic graphs that we discussed last semester Examples of diagnostic graphs for residuals: https://gallery.shinyapps.io/slr_diag/ A graph is considered good if, when moving from left to right, all the points are distributed approximately evenly. But if we have a broom, that on one side the data has shrunk to one point, and then they start to diverge like a broom - this means that heteroscedasticity is observed in the data , they are not homogeneous, and the results of such a model will not be very reliable. In our case, the graph looks good and passes the test.\nNow let’s take an anti-example and see what happens to the distribution of the residuals.\nwhr_tests_hw %&gt;% \n  lm(Life_Ladder ~ Perceptions_of_corruption, .) -&gt; lm_model2_anti\nplot(lm_model2_anti, 1)\nSomething went wrong here, so we wouldn’t rely on such a model.\nThe next assumption is that the residuals must be normally distributed. Yes, we do not check for normality of the PP distribution, but the residuals need to be checked - otherwise we will not be able to draw a line with the least squares method calculation, that is, trying to get closer to all our points at once. Let’s check this assumption by displaying the second graph of the standard plot() function.\nplot(lm_model1, 2)\n Or, as we have already built beautiful graphs to check normality using ggplot\nggplot(data = lm_model1, aes(x = lm_model1$residuals)) +\n  geom_density(color = \"#355C7D\") +\n  theme_minimal()\n\nggplot(data = lm_model1, aes(sample = lm_model1$residuals)) +\n  stat_qq(color = \"#355C7D\") +\n  geom_qq_line() +\n  theme_minimal()\n\nSo, we have met all the assumptions for the single predictor model! We only have the assumption of no multicollinearity left, but it is not applicable to the single predictor model.\nNow let’s look again at the model results and remember how to interpret them . The key thing we are interested in is the linear regression coefficients ,b0b0, intercept, the intersection of the regression line with the y-axis, andb1b1, slope, the angle of inclination of the line - since the null hypothesis for linear regression is that they are equal to zero. Another equally important thing isR2R2, the coefficient of determination and the percentage of explained variance .\nlm_model1\n## \n## Call:\n## lm(formula = Life_Ladder ~ Log_GDP_per_capita, data = .)\n## \n## Coefficients:\n##        (Intercept)  Log_GDP_per_capita  \n##            -1.4066              0.7421\nsummary(lm_model1)\n## \n## Call:\n## lm(formula = Life_Ladder ~ Log_GDP_per_capita, data = .)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -2.26515 -0.47805 -0.04913  0.54993  2.01866 \n## \n## Coefficients:\n##                    Estimate Std. Error t value            Pr(&gt;|t|)    \n## (Intercept)         -1.4066     0.1467  -9.587 &lt;0.0000000000000002 ***\n## Log_GDP_per_capita   0.7421     0.0158  46.973 &lt;0.0000000000000002 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.7016 on 1404 degrees of freedom\n##   (15 observations deleted due to missingness)\n## Multiple R-squared:  0.6111, Adjusted R-squared:  0.6109 \n## F-statistic:  2206 on 1 and 1404 DF,  p-value: &lt; 0.00000000000000022\nAs usual, we look at the p-value of each coefficient (and we can also look at the p-value of the entire model: here the same story as with ANOVA - the entire model is significant when at least one of the coefficients is significant, and is not significant when all coefficients do not differ statistically significantly from zero). If the coefficient is significant, then we can say that it significantly contributes to the explanation of the variance.b0b0, the intercept, is significantly different from zero, so we can say that our line will not start at the origin, but will be shifted by -1.4 along the y-axis.b1b1Log_GDP_per_capita is significantly different from zero, so our line will have a slope - which means that with an increase in Log_GDP_per_capita by 1, the dependent variable Life_Ladder will increase by 0.74!\nWhat is it equal to?R2R2? It is equal to 0.61, which is a very good result. That is, 61% of the variability of our data at the Life_Ladder level is determined by the level of purchasing power Log_GDP_per_capita!\nI can now write the equation of the regression line as follows:\n\n\\(\\hat Life\\_Ladder = -1.4 + 0.74 \\times Log\\_GDP\\_per\\_capita\\)\n\nLet’s now visualize our regression line\nwhr_tests_hw %&gt;% \n  ggplot(aes(x = Log_GDP_per_capita, y = Life_Ladder)) +\n  geom_point(color = \"#355C7D\", size = 1, alpha = 0.5) +\n  geom_smooth(method = 'lm', color = \"violet\") +\n  theme_minimal()\n\n\n14.2.2 Linear Regression with Multiple Predictors (Factors)\nEverything is the same, but now we will take the second predictor into the model – Freedom_to_make_life_choices.\nwhr_tests_hw %&gt;%\n  lm(Life_Ladder ~ Log_GDP_per_capita + Freedom_to_make_life_choices, .) -&gt; lm_model3_multy\nlm_model3_multy\nsummary(lm_model3_multy)\nChecking is allowed\nplot(lm_model3_multy, 1)\nplot(lm_model3_multy, 2)\nNow we need to check the assumption about the absence of multicollinearity - that there is no strong correlation between the predictors (NP), otherwise our model will be a bit meaningless. We will check using the vif() function, the dispersion inflation coefficient. The indicators are quite arbitrary, but it is considered that if the values ​​are greater than 5, then this is a very strong correlation between the variables, and the strongly correlated variable should be removed.\ncar::vif(lm_model3_multy)\nOur vif readings are quite low, so we won’t change anything.\nNext we can interpret the results (all coefficients are significant) and write down the equation of the regression line.\nLifand_Laddandr=−2.09+0.64×Ltheg_GDP_pandr_capita+2.15×Frandanddthem_tthe_makand_lifand_chtheicandsLifand_Laddandr=−2.09+0.64×Ltheg_GDP_pandr_capita+2.15×Frandanddthem_tthe_makand_lifand_chtheicands",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Testing Hypothesis 2: ANOVA and Linear Regression</span>"
    ]
  },
  {
    "objectID": "r_5.html#assignments-after-the-seminar-7",
    "href": "r_5.html#assignments-after-the-seminar-7",
    "title": "14  Testing Hypothesis 2: ANOVA and Linear Regression",
    "section": "14.3 Assignments after the seminar 7",
    "text": "14.3 Assignments after the seminar 7\n\nSelect variables suitable for linear regression in the datawhr_tests_hw.csv\nConstruct a linear regression on these variables, with one factor (LF)\nConstruct a linear regression on these variables, but with at least two factors (NP)\nPlot diagnostic plots for each model: how equal is the variance of the residuals for these models? Can we trust the results of these models?\nDetermine which model best explains the data (i.e. wage variability)?\nWrite the linear regression equation for both models.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Testing Hypothesis 2: ANOVA and Linear Regression</span>"
    ]
  },
  {
    "objectID": "r_5.html#logistic-regression",
    "href": "r_5.html#logistic-regression",
    "title": "14  Testing Hypothesis 2: ANOVA and Linear Regression",
    "section": "14.4 Logistic Regression",
    "text": "14.4 Logistic Regression\nLet’s move from linear regression to logistic regression. First, let’s remember what this method is and when it is used - it is used when the salary is not quantitative, but categorical, and, as a rule, has two gradations. This is the most classic case of logistic regression, although the salary in it can have more gradations than two - then it will be a multinomial logistic regression . All machine learning is built on log regression - tasks in which it is necessary to determine whether an image belongs to a certain pattern (for example, text recognition), whether a credit borrower is reliable for issuing a loan (scoring models in banks), etc. use this method.\nWe will consider it on the same example of our data from the World Happiness Report. They are not very suitable for this method, so the task will be a bit artificial - we will try to predict whether a country will be in the first or second half of the rating by level of happiness (we created a special variable for this). First, I will recode this variable into 1 and 0, since log regression works with numerical values ​​0 and 1.\nwhr_tests_hw %&gt;%\n  mutate(mean_position_bi = ifelse(mean_position == \"upper\", 1, 0)) -&gt; whr_tests_hw\nFirst, let’s build the simplest model, which will include only the intercept. This model is essentially an analogue of chi-square - without any predictors, estimate whether a country will be in the first or second half of the rating, purely based on the frequency of occurrence of countries from the first and second halves? Specifically, in this example, the frequencies will be approximately the same, but the principle of operation of this model is approximately the same.\ntable(whr_tests_hw$mean_position)\nwhr_tests_hw %&gt;%\n  glm(mean_position_bi ~ 1, data = ., family = binomial) -&gt; lm_model_log1\nsummary(lm_model_log1)\nlog(75/78)\n# log(p/1-p) = intercept\nLet’s complicate the model and add GDP per capita as a predictor\nwhr_tests_hw %&gt;%\n  glm(mean_position_bi ~ Log_GDP_per_capita, ., family = binomial) -&gt; lm_model_log2\nsummary(lm_model_log2)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Testing Hypothesis 2: ANOVA and Linear Regression</span>"
    ]
  }
]